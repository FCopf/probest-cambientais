[["index.html", "Estatística nas Ciências Ambientais Índice", " Estatística nas Ciências Ambientais Fabio Cop Ferreira fabiocopf@gmail.com; fcferreira@unifesp.br Instituto do Mar, Universidade Federal de São Paulo Última atualização em 17/03/2022 Índice I - Estatística descritiva (capítulos 1 e 11) II - Amostragem e Delineamento (capítulos 12 a 13) III - Inferência e Teste de Hipóteses (capítulos 14 a 18) IV - Modelos Lineares Clássicos (capítulos 22 a 25) V - Fundamentos de probabilidade (capítulos 26 a 29) VI - Modelos Probabilísticos, verossimilhança e inferência bayesiana (capítulos 30 a 38) "],["estrdados.html", "Capítulo 1 Estrutura e tipo de dados 1.1 Unidades amostrais e descritores 1.2 Tipos de dados 1.3 Níveis de mensuração", " Capítulo 1 Estrutura e tipo de dados A estatística descritiva se utiliza de métodos para resumir e evidenciar as informações relevantes de um conjunto de dados. Em grande parte, a apresentação destas informações passa pela construção de gráficos e tabelas apropriados a diferentes tipos de dados, além do cálculo de descritores que resumem algumas características das variáveis envolvidas (ex. média aritmética, desvio padrão, frequência relativa, padrões de correlação). Iremos discutir cada um destes tópicos nesta seção e veremos que de modo geral, a forma de apresentação depende da natureza dos dados envolvidos e da relação que estabelecemos entre eles. Neste capítulo iremos tratar da estrutura de um conjunto de dados e dos tipos de variáveis mais comuns. Considere a tabela abaixo, construída a partir do livro Biocenoses em Reservatórios: padrões espaciais e temporais (Rodrigues et al. 2005) que apresenta informações sobre 31 reservatórios do estado do Paraná. Reservatorio Bacia Fechamento Area Trofia pH Condutividade Alcalinidade P.total Riqueza CPUE Cavernoso Iguacu 1965 2.90 Oligotrófico 7.4 33.1 139.80 7.8 18 9.22 Curucaca Iguacu 1982 2.00 Oligotrófico 7.0 32.4 125.70 4.7 16 28.73 Foz do Areia Iguacu 1980 139.00 Oligotrófico 7.3 35.5 97.00 14.3 19 11.59 Irai Iguacu 2000 15.00 Eutrófico 6.9 50.2 3.30 53.4 12 30.76 JMF Iguacu 1970 0.45 Mesotrófico 7.3 40.2 3.70 41.2 18 5.95 Jordao Iguacu 1996 3.40 Oligotrófico 7.1 23.7 152.70 3.3 17 7.75 Passauna Iguacu 1978 14.00 Oligotrófico 8.8 125.6 526.00 15.2 11 7.51 Piraquara Iguacu 1979 3.30 Oligotrófico 7.1 22.8 50.67 4.5 8 4.01 Salto Caxias Iguacu 1998 124.00 Oligotrófico 7.3 39.6 106.00 12.1 21 20.83 Salto do Vau Iguacu 1959 2.90 Oligotrófico 6.5 23.2 279.00 11.0 8 2.43 Salto Osorio Iguacu 1975 51.00 Oligotrófico 8.6 38.9 233.30 3.4 24 12.55 Salto Santiago Iguacu 1979 208.00 Oligotrófico 9.2 39.5 117.60 13.1 21 11.73 Segredo Iguacu 1992 82.50 Oligotrófico 7.0 34.5 165.20 6.4 22 13.72 Mourao Ivai 1964 11.30 Oligotrófico 8.1 23.3 56.55 7.1 15 16.50 Patos Ivai NA 1.30 Mesotrófico 6.9 46.0 180.10 39.2 10 4.71 Guaricana Litoranea 1957 7.00 Oligotrófico 7.4 27.9 83.72 12.4 12 7.95 Parigot Souza Litoranea 1970 12.00 Oligotrófico 7.7 63.6 259.20 16.9 12 13.12 Salto do Meio Litoranea NA 0.10 Oligotrófico 6.9 37.4 147.10 17.1 11 16.10 Vossoroca Litoranea 1949 5.10 Mesotrófico 7.3 39.8 156.00 21.9 14 11.74 Canoas I Paranapanema 1999 30.85 Oligotrófico 7.4 63.3 234.90 9.9 35 17.95 Canoas II Paranapanema 1992 22.50 Oligotrófico 7.8 61.2 NA 9.0 40 13.86 Capivara Paranapanema 1975 419.30 Oligotrófico 7.5 58.6 196.00 5.5 34 13.04 Chavantes Paranapanema 1970 400.00 Oligotrófico 7.6 57.8 211.80 7.8 23 7.35 Rosana Paranapanema 1986 220.00 NA 7.7 58.2 202.40 NA 30 20.92 Salto Grande Paranapanema 1958 12.00 Oligotrófico 7.1 62.3 230.10 10.3 24 13.67 Taquarucu Paranapanema 1989 80.10 Oligotrófico 7.9 57.0 191.80 4.5 33 21.82 Melissa Piriqui 1962 0.10 Eutrófico 6.8 34.0 68.37 66.9 12 6.29 Santa Maria Piriqui NA 0.07 Oligotrófico 6.8 41.7 480.10 14.9 7 9.40 Alagados Tibagi 1909 7.20 Oligotrófico 7.6 41.7 172.20 19.9 7 5.60 Apucaraninha Tibagi 1958 NA NA NA NA NA NA 10 2.05 Harmonia Tibagi NA NA Oligotrófico 8.3 31.0 113.30 8.6 7 24.88 A tabela é formada por 31 linhas referentes a cada reservatório e 11 colunas em que constam informações sobre cada reservatório, sendo elas: Reservatorio: nome do reservatório; Bacia: bacia hidrográfica (Iguacu, Ivai, Litoranea, Paranapanema, Piriqui, Tibagi); Fechamento: ano de formação do reservatório; Area: área em \\(km^2\\); Trofia: grau de trofia (Eutrófico, Mesotrófico, Oligotrófico); pH: pH; Condutividade: condutividade; Alcalinidade: alcalinidade; P.total: fósforo total; Riqueza: número de espécies de peixes encontrada; CPUE: captura (kg) por unidade de esforço; 1.1 Unidades amostrais e descritores Esta tabela está organizada em um formato muito específico em que cada linha representa uma unidade amostral (UA) e cada coluna representa uma variável (VA) que descreve determinada característica desta observação. Ao longo desta apostila veremos diversos conjuntos de dados, todos eles organizados neste formato. ID VA 1 VA 2 VA 3 VA 4 VA 5 VA 6 VA 7 UA 1 UA 2 UA 3 UA 4 UA 5 UA 6 UA 7 UA 8 UA 9 UA 10 Em nosso exemplo, cada unidade amostral é um reservatório que é descrito pelas variáveis dispostas nas colunas. O reservatório de Cavernoso por exemplo faz parte da bacia do rio Iguacu, foi formado no ano de 1965, tem área de 2.9 \\(km^2\\), pH igual a 7.4 e assim por diante. Valores faltantes: algumas células da tabela estão preenchidas por NA. Isto significa que a informação naquela célula não foi mensurada e que temos um dado faltante. Você deve ter muito cuidado ao lidar com este tipo de situação. Se uma linha contém muitas células sem informação, é prudente excluir esta observação das análises. Se por outro lado, uma coluna apresenta muitos valores faltantes, talvez seja prudente excluir a variável das análises. Se você não deseja ou não pode excluir a linha ou a coluna existem métodos de preenchimento de dados faltantes. No entanto, ao optar por algum destes métodos, você deve ter ter claro quais serão os efeitos de inserir uma informação à tabela de dados que efetivamente não foi mensurada. 1.2 Tipos de dados Uma tabela de dados pode ser composta por variáveis quantitativas ou qualitativas. 1.2.1 Variáveis qualitativas São variáveis não-numéricas como categorias ou rótulos. Dentre as variáveis qualitativas temos aquelas do tipo categóricas não-ordenadas e do tipo categóricas ordenadas. Variável categórica não-ordenada: Em nossa tabela, a variável Bacia classifica um reservatório como pertencente a uma determinada bacia hidrográfica. Os níveis da variável Bacia são: Iguacu, Ivai, Litoranea, Paranapanema, Piriqui, Tibagi. A variável é do tipo categórica não-ordenada pois estes níveis não possuem qualquer relação de ordenação natural entre si. Variável categórica ordenada: a variável Trofia ordena os reserrvatórios como função da quantidade de nutrientes em Oligotrófico &lt; Mesotrófico &lt; Eutrófico. Ainda que os níveis possam ser ordenados, não é possível atribuir diferenças numéricas entre eles, fazendo desta uma variável qualotativa. 1.2.2 Variáveis quantitativas São variáveis numéricas que também podem ser sub-divididas em dois grupos: discretas e contínuas. Variáveis quantitativas discretas: envolvem quantias enumeráveis como a contagem de barcos que saem para pescar em um determinado dia, o número de peixes de um cardume. Em nosso exemplo, a variável Riqueza é quantitativa discreta pois expressa o número de espécies de peixes encontradas em cada reservatório. Este é um número inteiro que pode assumir valor mínimo igual a 0 (nenhuma espécie) e em teoria, não tem limite superior (ainda que neste exemplo, o número máximo encontrados seja de 40 espécies). Variáveis quantitativas contínuas: envolvem quantias não-enumeráveis como a vazão em \\(m^3/seg\\) que verte de uma cachoeira, o volume de chuva em um determinado dia, altura da maré ou a velocidade do vento. O limite de precisão que utilizamos para representá-las depende basicamente da capacidade de mensuração dos aparelhos disponíveis. Em nosso exemplo, temos diversas variáveis deste tipo como pH, Condutividade, Fosforo_total. Sempre é possível transformar variáveis quantitativas em qualitativas. Se temos a variável comprimento de peixes desembarcados dada em centímetros (variável quantitativa), é possível expressá-la de forma categórica em peixes grandes e pequenos (variável qualitativa). Por outro lado, se tivermos somente a informação de que um peixe é grande ou pequeno, não podemos recuperar as quantias numéricas originais. 1.3 Níveis de mensuração Uma outra forma de organizar variáveis pode ser em função dos níveis de mensuração nominal, ordinal, intervalar e razão. Nível nominal: é característico de variáveis que possuem níveis não ordenaveis. Ex. cor, grupo taxonômico, nomes de cidades, etc. Nível ordinal: é aquele em que os níveis podem ser ordenados, embora não seja possível quantificar as diferenças entre dois níveis. Ex. i - Ordem de chegada de maratonistas em uma competição (\\(1^o\\), \\(2^o\\), \\(3^o\\),…). ii - Condição de saneamento das cidades (ótimo, bom, ruim, péssimo). iii - Condição de saneamento das praias da baixada santista (próprio, imprórpio). No nível ordinal podemos ordenar os elementos porém não podemos quantificar as diferenças entre eles. Nível intervalar: é aquele em que além ser possível ordenar, é possível quantificar as diferenças entre duas observações. No entanto, não há um ponto inicial natural, ou seja, um ponto zero que indique ausência da quantia. Ex. i – Temperatura: \\(0^oC\\) não indica ausência de temperatura, assim como \\(10^oC\\) não é duas vezes mais quente que \\(5^oC\\). Essas características são somente uma convenção relacionada à escala de mensuração da temperatura. ii - Ano do calendário: o ano zero é uma convenção do calendário, não significa ausência de tempo. Nível de razão: é como o intervalar, porém existe um ponto zero natural. Peso igual a 0 kg indica ausência de peso e dez quilogramas é duas vezes mais pesado que 5 kg. O mesmo vale para comprimento, distância, velocidade, número de ovos. A depender do nível de mensuração, algumas operações matemáticas podem ou não fazer sentido. Por exemplo, se uma espécie tem \\(N_A = 100\\) indivíduos na região A e \\(N_B = 200\\) na região B, a segunda região é duas vezes mais populosa pois \\(\\frac{N_B}{N_A} = 2\\). Por outro lado, se a temperatura na região A é de \\(T_A = 10^oC\\) enquanto na B é de \\(T_B = 20^oC\\) não faz sentido fazer \\(\\frac{T_B}{T_A} = 2\\) e dizer que B seja duas vezes mais quente que A. Ainda que matematicamente a operação seja possível nos dois exemplos, no último sua interpretação física não tem sentido. Tipos de dados vs níveis de mensuração: existe uma relação entre tipo de dados e nível de mensuração. Os níveis nominal e ordinal de mensuração se referem a variáveis qualitativas não-ordenadas e qualitativas ordenadas respectivamente. Já os níveis intervalar e razão se referem a variáveis quantitativos, podendo ser discretas ou contínuas. References "],["procdados.html", "Capítulo 2 Processamento de dados 2.1 Criando o diretório de trabalho 2.2 Iniciando o projeto Intro_estatistica 2.3 Instalação de pacotes 2.4 Carregando os pacotes 2.5 Importanto a base de dados 2.6 Verificando a base de dados 2.7 Reorganizando a base de dados 2.8 Selecionando colunas da tabela 2.9 Filtrando linhas da tabela 2.10 Adicionando ou modificando colunas 2.11 Renomeando colunas 2.12 Outras operações para processamento e transformação de dados", " Capítulo 2 Processamento de dados Tabelas de dados como a apresentada no capítulo 1 são normalmente armazenadas em planilhas eletrônicas. Os formatos mais comuns para armazenamento são arquivos do tipo .xlsx, .csv, .txt e mais recentemente planilhas em nuvem (ex. google sheets). Os programas para visualização e destes tipos de arquivos são apropriados para inserção e armazenamendo de dados, mas apresentam limitações para o processamento, descrição e visualização. Neste capítulo iremos utilizar a linguagem estatística R e o ambiente de trabalho RStudio para iniciar um projeto de análise de dados. Veremos como preparar um novo projeto na linguagem R, organizar as base de dados no diretório de trabalho, importá-las para o ambiente R e aplicar algumas ações comuns ao processamento de dados inicial. O objetivo é a base de dados original permaneça inalterada no diretório de trabalho e que todo o processamento seja feito em uma versão da base de dados que será importada para o ambiente R. Isto evita que sejam feitas alterações equivocadas diretamente na base de dados, resultando na perda da informação original. Para mais detalhes sobre programação e processamento de dados no R veja: Introdução ao Ambiente R de Programação. 2.1 Criando o diretório de trabalho Inicialmente, vamos criar um diretório de trabalho, um local em nosso computador onde iremos colocar todos os arquivos necessários ao projeto (ex. as tabelas de dados), bem como os arquivos gerados durante o projeto (planilhas, gráficos, figuras, slides, arquivos .pdf, etc.). 2.2 Iniciando o projeto Intro_estatistica Na pasta Documentos crie um diretório denominado Intro_estatistica. Após criar o diretório copie para dentro dele a base de dados Reservatorios_Parana_parcial.csv. A base está disponível no link do GitHub. Após entra no link, a janela de seu navegador irá mostrar um arquivo texto com os dados da planilha. Estes são os mesmo dados que apresentamod no capítulo 1. Abra o R studio e em seguida crie um novo Script em: File --&gt; New File --&gt; RScript. Antes de qualquer ação, salve o novo arquivo como: aula_01.R. A partir de agora, todos os comandos desta aula deverão ser digitados dentro deste arquivo. Para rodar cada comando, pressione o botão Run no RStudio ou digite as teclas de atalho Ctrl + Enter. Não se esqueça de salvar o arquivo texto à medida que adiciona novos comandos. Vamos dizer ao R onde procurar os arquivos relacionados a este projeto. Na primeira linha de seu arquivo novo digite o comando abaixo e depois pressione Run: # Dewfinindo a localização do projeto em seu cmputador setwd(&quot;C:/Users/f_cop/Documents/Intro_estatistica&quot;) O caminho digitado acima é o caminho do meu diretório de trabalho. Você deve substituir pelo caminho do projeto em seu computador. Se você utiliza o Windows, note que o R utiliza a barra invertida (/) para separação dos pastas. Para verificar se o comando foi bem sucedido, digite o seguinte comando no Console do RStudio: getwd() ## [1] &quot;C:/Users/f_cop/Documents/Intro_estatistica&quot; Como resultado, você deverá obter uma saída parecida com a apresentada acima, contendo o caminho do projeto em seu computador. Finalmente, para verificar se o R está lendo corretamente os arquivos em seu diretório, digite o comando dir() no console do RStudio: dir() ## [1] &quot;Intro_estatistica.Rproj&quot; ## [2] &quot;Reservatorios_Parana_parcial.csv&quot; Se tudo estiver correto, o resultado deste comando deve listar os dois arquivos presentes em sua pasta de trabalho: Intro_estatistica.Rproj e Reservatorios_Parana_parcial.csv. 2.3 Instalação de pacotes O R possui vários pacotes (ou bibliotecas) pré-instalados, porém é comum que tenhamos que utilizar alguns que não venham na instalação padrão. Neste projeto utilizaremos o pacote tidyverse. Para instalar o pacote, digite o seguinte comando no Console do RStudio: install.packages(&quot;tidyverse&quot;) Para mais detalhes sobre o pacote, acesse a apostila do R e veja o site oficial. Um pacote necessita ser instalado somente uma vez em seu computador. Feito isto ele estará sempre disponível, bastando carregá-lo a cada vez que você abrir uma seção do R. 2.4 Carregando os pacotes Antes de iniciar as análises, insira este comando em seu arquivo texto: # Carrega pacotes library(tidyverse) Ao rodar este comando você habilita as funções do pacote para serem utilizados em sua seção do R. Sempre que você iniciar uma nova seção do R você deverá carregar os pacotes necessários com o comando library. 2.5 Importanto a base de dados A base de dados está disponível em nosso diretório, porém ainda não a importamos para o ambiente R. Para isto, utilize o comando read_delim: # Importa base de dados res = read_delim(&#39;Reservatorios_Parana_parcial.csv&#39;, delim = &#39;,&#39;, locale = locale(decimal_mark = &#39;.&#39;, encoding = &#39;latin1&#39;)) O comando acima importa para o R os dados da tabela denominada Reservatorios_Parana_parcial.csv e cria um objeto denominado res contendo estas informações. A partir de agora, qualquer modificação que fizermos no objeto res, não irá alterar o arquivo .csv original. Vamos entender alguns argumentos do comando read_delim: delim = ',': utilizado para dizer ao R que cada coluna na base de dados está separada das demais por uma vírgula. Se no arquivo .csv as colunas estivessem seperadas por ponto-e-vígula, deveríamos escrever delim = ';'. decimal_mark = '.': informa ao R que o símbolo ponto é utilizado como um separador decimal. Se no arquivo .csv os números decimais utilizassem a vírgula, o argumento deveria ser decimal_mark = ','. encoding = 'latin1': indica a forma de codificação. A necessidade de seu uso depende do sistema operacional de seu computador. Outras formas de codificação além de são por exemplo UTF-8, ISO-8859-1. Geralmente, o argumento é necessário quando existem caracteres especiais na base de dados como acentos e cedilhas. Podemos visualizar a base de dados digitando: res ## # A tibble: 31 × 11 ## Reservatorio Bacia Fechamento Area Trofia pH Condutividade Alcalinidade ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Cavernoso Iguacu 1965 2.9 Oligo… 7.4 33.1 140. ## 2 Curucaca Iguacu 1982 2 Oligo… 7 32.4 126. ## 3 Foz do Areia Iguacu 1980 139 Oligo… 7.3 35.5 97 ## 4 Irai Iguacu 2000 15 Eutró… 6.9 50.2 3.3 ## 5 JMF Iguacu 1970 0.45 Mesot… 7.3 40.2 3.7 ## 6 Jordao Iguacu 1996 3.4 Oligo… 7.1 23.7 153. ## 7 Passauna Iguacu 1978 14 Oligo… 8.8 126. 526 ## 8 Piraquara Iguacu 1979 3.3 Oligo… 7.1 22.8 50.7 ## 9 Salto Caxias Iguacu 1998 124 Oligo… 7.3 39.6 106 ## 10 Salto do Vau Iguacu 1959 2.9 Oligo… 6.5 23.2 279 ## # … with 21 more rows, and 3 more variables: P.total &lt;dbl&gt;, Riqueza &lt;dbl&gt;, ## # CPUE &lt;dbl&gt; Fazendo isso você verá somente parte da base de dados, a depender do tamanho de sua janela do RStudio. Caso queira ver a tabela completa digite: View(res) A base de dados é simplesmente uma tabela, em que cada linha trás uma observação e cada coluna uma variável. Vamos realizar algumas operações para entender como explorar as informações na tabela. 2.6 Verificando a base de dados O comando abaixo informa alguns detalhes sobre cada coluna da tabela e dos tipos de dados: glimpse(res) ## Rows: 31 ## Columns: 11 ## $ Reservatorio &lt;chr&gt; &quot;Cavernoso&quot;, &quot;Curucaca&quot;, &quot;Foz do Areia&quot;, &quot;Irai&quot;, &quot;JMF&quot;, … ## $ Bacia &lt;chr&gt; &quot;Iguacu&quot;, &quot;Iguacu&quot;, &quot;Iguacu&quot;, &quot;Iguacu&quot;, &quot;Iguacu&quot;, &quot;Iguac… ## $ Fechamento &lt;dbl&gt; 1965, 1982, 1980, 2000, 1970, 1996, 1978, 1979, 1998, 19… ## $ Area &lt;dbl&gt; 2.90, 2.00, 139.00, 15.00, 0.45, 3.40, 14.00, 3.30, 124.… ## $ Trofia &lt;chr&gt; &quot;Oligotrófico&quot;, &quot;Oligotrófico&quot;, &quot;Oligotrófico&quot;, &quot;Eutrófi… ## $ pH &lt;dbl&gt; 7.4, 7.0, 7.3, 6.9, 7.3, 7.1, 8.8, 7.1, 7.3, 6.5, 8.6, 9… ## $ Condutividade &lt;dbl&gt; 33.1, 32.4, 35.5, 50.2, 40.2, 23.7, 125.6, 22.8, 39.6, 2… ## $ Alcalinidade &lt;dbl&gt; 139.80, 125.70, 97.00, 3.30, 3.70, 152.70, 526.00, 50.67… ## $ P.total &lt;dbl&gt; 7.8, 4.7, 14.3, 53.4, 41.2, 3.3, 15.2, 4.5, 12.1, 11.0, … ## $ Riqueza &lt;dbl&gt; 18, 16, 19, 12, 18, 17, 11, 8, 21, 8, 24, 21, 22, 15, 10… ## $ CPUE &lt;dbl&gt; 9.22, 28.73, 11.59, 30.76, 5.95, 7.75, 7.51, 4.01, 20.83… Vemos dois tipos de dados: chr identificando que a coluna contém uma variável qualitativa e dbl, identificando que a coluna contém uma variável quantitativa. 2.7 Reorganizando a base de dados A tabela contém uma coluna denominada Fechamento, que mostra o ano em que o reservatório foi formado. Vamos visualizar em ordem crescente do ano de formação, do mais antigo para o mais novo: res %&gt;% arrange(Fechamento) %&gt;% View() O reservatório mais antigo é de 1909 e o mais novo de 2000. Três reservatórios estão ao final da tabela pois para estes a variável Fechamento consta como faltante. Ao rodar esta função não alteramos o objeto res, apenas o visualizamos em uma ordem diferente. Digite o nome do objeto e verá que a ordem permanece inalterada. res ## # A tibble: 31 × 11 ## Reservatorio Bacia Fechamento Area Trofia pH Condutividade Alcalinidade ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Cavernoso Iguacu 1965 2.9 Oligo… 7.4 33.1 140. ## 2 Curucaca Iguacu 1982 2 Oligo… 7 32.4 126. ## 3 Foz do Areia Iguacu 1980 139 Oligo… 7.3 35.5 97 ## 4 Irai Iguacu 2000 15 Eutró… 6.9 50.2 3.3 ## 5 JMF Iguacu 1970 0.45 Mesot… 7.3 40.2 3.7 ## 6 Jordao Iguacu 1996 3.4 Oligo… 7.1 23.7 153. ## 7 Passauna Iguacu 1978 14 Oligo… 8.8 126. 526 ## 8 Piraquara Iguacu 1979 3.3 Oligo… 7.1 22.8 50.7 ## 9 Salto Caxias Iguacu 1998 124 Oligo… 7.3 39.6 106 ## 10 Salto do Vau Iguacu 1959 2.9 Oligo… 6.5 23.2 279 ## # … with 21 more rows, and 3 more variables: P.total &lt;dbl&gt;, Riqueza &lt;dbl&gt;, ## # CPUE &lt;dbl&gt; Se desejarmos visualizar a tabela em ordem decresente, basta fazermos: res %&gt;% arrange(desc(Fechamento)) %&gt;% View() Se utilizarmos as função arrange() com uma variável nominal, a tabela será organizada em ordem alfabética para esta variável. res %&gt;% arrange(Bacia) %&gt;% View() 2.8 Selecionando colunas da tabela É interessante, sobretudo para grandes bases de dados, se pudermos selecionar sub-grupos de colunas, excluindo outras que não nos interessam. Podemos fazer isto facilmente utilizando a função select(). Por exemplo, para selecionar somente as colunas Reservatório, Area e Riqueza: res_new = res %&gt;% select(Reservatorio, Area, Riqueza) rew_new Reservatorio Area Riqueza Cavernoso 2.90 18 Curucaca 2.00 16 Foz do Areia 139.00 19 Irai 15.00 12 JMF 0.45 18 Jordao 3.40 17 Passauna 14.00 11 Piraquara 3.30 8 Salto Caxias 124.00 21 Salto do Vau 2.90 8 Salto Osorio 51.00 24 Salto Santiago 208.00 21 Segredo 82.50 22 Mourao 11.30 15 Patos 1.30 10 Guaricana 7.00 12 Parigot Souza 12.00 12 Salto do Meio 0.10 11 Vossoroca 5.10 14 Canoas I 30.85 35 Canoas II 22.50 40 Capivara 419.30 34 Chavantes 400.00 23 Rosana 220.00 30 Salto Grande 12.00 24 Taquarucu 80.10 33 Melissa 0.10 12 Santa Maria 0.07 7 Alagados 7.20 7 Apucaraninha NA 10 Harmonia NA 7 O que fizemos aqui foi cirar um novo objeto (res_new), contendo somente as colunas selecionadas. Se quisermos selecionar todas as variáveis exceto Reservatório fazemos: rew_new = res %&gt;% select(-Reservatorio) rew_new Bacia Fechamento Area Trofia pH Condutividade Alcalinidade P.total Riqueza CPUE Iguacu 1965 2.90 Oligotrófico 7.4 33.1 139.80 7.8 18 9.22 Iguacu 1982 2.00 Oligotrófico 7.0 32.4 125.70 4.7 16 28.73 Iguacu 1980 139.00 Oligotrófico 7.3 35.5 97.00 14.3 19 11.59 Iguacu 2000 15.00 Eutrófico 6.9 50.2 3.30 53.4 12 30.76 Iguacu 1970 0.45 Mesotrófico 7.3 40.2 3.70 41.2 18 5.95 Iguacu 1996 3.40 Oligotrófico 7.1 23.7 152.70 3.3 17 7.75 Iguacu 1978 14.00 Oligotrófico 8.8 125.6 526.00 15.2 11 7.51 Iguacu 1979 3.30 Oligotrófico 7.1 22.8 50.67 4.5 8 4.01 Iguacu 1998 124.00 Oligotrófico 7.3 39.6 106.00 12.1 21 20.83 Iguacu 1959 2.90 Oligotrófico 6.5 23.2 279.00 11.0 8 2.43 Iguacu 1975 51.00 Oligotrófico 8.6 38.9 233.30 3.4 24 12.55 Iguacu 1979 208.00 Oligotrófico 9.2 39.5 117.60 13.1 21 11.73 Iguacu 1992 82.50 Oligotrófico 7.0 34.5 165.20 6.4 22 13.72 Ivai 1964 11.30 Oligotrófico 8.1 23.3 56.55 7.1 15 16.50 Ivai NA 1.30 Mesotrófico 6.9 46.0 180.10 39.2 10 4.71 Litoranea 1957 7.00 Oligotrófico 7.4 27.9 83.72 12.4 12 7.95 Litoranea 1970 12.00 Oligotrófico 7.7 63.6 259.20 16.9 12 13.12 Litoranea NA 0.10 Oligotrófico 6.9 37.4 147.10 17.1 11 16.10 Litoranea 1949 5.10 Mesotrófico 7.3 39.8 156.00 21.9 14 11.74 Paranapanema 1999 30.85 Oligotrófico 7.4 63.3 234.90 9.9 35 17.95 Paranapanema 1992 22.50 Oligotrófico 7.8 61.2 NA 9.0 40 13.86 Paranapanema 1975 419.30 Oligotrófico 7.5 58.6 196.00 5.5 34 13.04 Paranapanema 1970 400.00 Oligotrófico 7.6 57.8 211.80 7.8 23 7.35 Paranapanema 1986 220.00 NA 7.7 58.2 202.40 NA 30 20.92 Paranapanema 1958 12.00 Oligotrófico 7.1 62.3 230.10 10.3 24 13.67 Paranapanema 1989 80.10 Oligotrófico 7.9 57.0 191.80 4.5 33 21.82 Piriqui 1962 0.10 Eutrófico 6.8 34.0 68.37 66.9 12 6.29 Piriqui NA 0.07 Oligotrófico 6.8 41.7 480.10 14.9 7 9.40 Tibagi 1909 7.20 Oligotrófico 7.6 41.7 172.20 19.9 7 5.60 Tibagi 1958 NA NA NA NA NA NA 10 2.05 Tibagi NA NA Oligotrófico 8.3 31.0 113.30 8.6 7 24.88 Podemos fazer também uma seleção que vá desde uma coluna inicial até uma coluna final. Podemos selecionar, por exemplo, todas as colunas desde pH até P.total. Para isto fazemos: res_new = res %&gt;% select(pH:P.total) res_new pH Condutividade Alcalinidade P.total 7.4 33.1 139.80 7.8 7.0 32.4 125.70 4.7 7.3 35.5 97.00 14.3 6.9 50.2 3.30 53.4 7.3 40.2 3.70 41.2 7.1 23.7 152.70 3.3 8.8 125.6 526.00 15.2 7.1 22.8 50.67 4.5 7.3 39.6 106.00 12.1 6.5 23.2 279.00 11.0 8.6 38.9 233.30 3.4 9.2 39.5 117.60 13.1 7.0 34.5 165.20 6.4 8.1 23.3 56.55 7.1 6.9 46.0 180.10 39.2 7.4 27.9 83.72 12.4 7.7 63.6 259.20 16.9 6.9 37.4 147.10 17.1 7.3 39.8 156.00 21.9 7.4 63.3 234.90 9.9 7.8 61.2 NA 9.0 7.5 58.6 196.00 5.5 7.6 57.8 211.80 7.8 7.7 58.2 202.40 NA 7.1 62.3 230.10 10.3 7.9 57.0 191.80 4.5 6.8 34.0 68.37 66.9 6.8 41.7 480.10 14.9 7.6 41.7 172.20 19.9 NA NA NA NA 8.3 31.0 113.30 8.6 Podemos ainda selecionar todas as variáveis categóricas: rew_new = res %&gt;% select_if(is.character) rew_new Reservatorio Bacia Trofia Cavernoso Iguacu Oligotrófico Curucaca Iguacu Oligotrófico Foz do Areia Iguacu Oligotrófico Irai Iguacu Eutrófico JMF Iguacu Mesotrófico Jordao Iguacu Oligotrófico Passauna Iguacu Oligotrófico Piraquara Iguacu Oligotrófico Salto Caxias Iguacu Oligotrófico Salto do Vau Iguacu Oligotrófico Salto Osorio Iguacu Oligotrófico Salto Santiago Iguacu Oligotrófico Segredo Iguacu Oligotrófico Mourao Ivai Oligotrófico Patos Ivai Mesotrófico Guaricana Litoranea Oligotrófico Parigot Souza Litoranea Oligotrófico Salto do Meio Litoranea Oligotrófico Vossoroca Litoranea Mesotrófico Canoas I Paranapanema Oligotrófico Canoas II Paranapanema Oligotrófico Capivara Paranapanema Oligotrófico Chavantes Paranapanema Oligotrófico Rosana Paranapanema NA Salto Grande Paranapanema Oligotrófico Taquarucu Paranapanema Oligotrófico Melissa Piriqui Eutrófico Santa Maria Piriqui Oligotrófico Alagados Tibagi Oligotrófico Apucaraninha Tibagi NA Harmonia Tibagi Oligotrófico Ou todas as variáveis numéricas: rew_new = res %&gt;% select_if(is.numeric) rew_new Fechamento Area pH Condutividade Alcalinidade P.total Riqueza CPUE 1965 2.90 7.4 33.1 139.80 7.8 18 9.22 1982 2.00 7.0 32.4 125.70 4.7 16 28.73 1980 139.00 7.3 35.5 97.00 14.3 19 11.59 2000 15.00 6.9 50.2 3.30 53.4 12 30.76 1970 0.45 7.3 40.2 3.70 41.2 18 5.95 1996 3.40 7.1 23.7 152.70 3.3 17 7.75 1978 14.00 8.8 125.6 526.00 15.2 11 7.51 1979 3.30 7.1 22.8 50.67 4.5 8 4.01 1998 124.00 7.3 39.6 106.00 12.1 21 20.83 1959 2.90 6.5 23.2 279.00 11.0 8 2.43 1975 51.00 8.6 38.9 233.30 3.4 24 12.55 1979 208.00 9.2 39.5 117.60 13.1 21 11.73 1992 82.50 7.0 34.5 165.20 6.4 22 13.72 1964 11.30 8.1 23.3 56.55 7.1 15 16.50 NA 1.30 6.9 46.0 180.10 39.2 10 4.71 1957 7.00 7.4 27.9 83.72 12.4 12 7.95 1970 12.00 7.7 63.6 259.20 16.9 12 13.12 NA 0.10 6.9 37.4 147.10 17.1 11 16.10 1949 5.10 7.3 39.8 156.00 21.9 14 11.74 1999 30.85 7.4 63.3 234.90 9.9 35 17.95 1992 22.50 7.8 61.2 NA 9.0 40 13.86 1975 419.30 7.5 58.6 196.00 5.5 34 13.04 1970 400.00 7.6 57.8 211.80 7.8 23 7.35 1986 220.00 7.7 58.2 202.40 NA 30 20.92 1958 12.00 7.1 62.3 230.10 10.3 24 13.67 1989 80.10 7.9 57.0 191.80 4.5 33 21.82 1962 0.10 6.8 34.0 68.37 66.9 12 6.29 NA 0.07 6.8 41.7 480.10 14.9 7 9.40 1909 7.20 7.6 41.7 172.20 19.9 7 5.60 1958 NA NA NA NA NA 10 2.05 NA NA 8.3 31.0 113.30 8.6 7 24.88 Finalmente, podemos alterar a ordem das colunas: nova_ordem = c(&#39;Fechamento&#39;, &#39;Area&#39;, &#39;Bacia&#39;, &#39;Reservatorio&#39;, &#39;CPUE&#39;, &#39;Riqueza&#39;, &#39;Trofia&#39;, &#39;Condutividade&#39;, &#39;pH&#39;, &#39;P.total&#39;, &#39;Alcalinidade&#39;) res_new = res %&gt;% select(nova_ordem) res_new Fechamento Area Bacia Reservatorio CPUE Riqueza Trofia Condutividade pH P.total Alcalinidade 1965 2.90 Iguacu Cavernoso 9.22 18 Oligotrófico 33.1 7.4 7.8 139.80 1982 2.00 Iguacu Curucaca 28.73 16 Oligotrófico 32.4 7.0 4.7 125.70 1980 139.00 Iguacu Foz do Areia 11.59 19 Oligotrófico 35.5 7.3 14.3 97.00 2000 15.00 Iguacu Irai 30.76 12 Eutrófico 50.2 6.9 53.4 3.30 1970 0.45 Iguacu JMF 5.95 18 Mesotrófico 40.2 7.3 41.2 3.70 1996 3.40 Iguacu Jordao 7.75 17 Oligotrófico 23.7 7.1 3.3 152.70 1978 14.00 Iguacu Passauna 7.51 11 Oligotrófico 125.6 8.8 15.2 526.00 1979 3.30 Iguacu Piraquara 4.01 8 Oligotrófico 22.8 7.1 4.5 50.67 1998 124.00 Iguacu Salto Caxias 20.83 21 Oligotrófico 39.6 7.3 12.1 106.00 1959 2.90 Iguacu Salto do Vau 2.43 8 Oligotrófico 23.2 6.5 11.0 279.00 1975 51.00 Iguacu Salto Osorio 12.55 24 Oligotrófico 38.9 8.6 3.4 233.30 1979 208.00 Iguacu Salto Santiago 11.73 21 Oligotrófico 39.5 9.2 13.1 117.60 1992 82.50 Iguacu Segredo 13.72 22 Oligotrófico 34.5 7.0 6.4 165.20 1964 11.30 Ivai Mourao 16.50 15 Oligotrófico 23.3 8.1 7.1 56.55 NA 1.30 Ivai Patos 4.71 10 Mesotrófico 46.0 6.9 39.2 180.10 1957 7.00 Litoranea Guaricana 7.95 12 Oligotrófico 27.9 7.4 12.4 83.72 1970 12.00 Litoranea Parigot Souza 13.12 12 Oligotrófico 63.6 7.7 16.9 259.20 NA 0.10 Litoranea Salto do Meio 16.10 11 Oligotrófico 37.4 6.9 17.1 147.10 1949 5.10 Litoranea Vossoroca 11.74 14 Mesotrófico 39.8 7.3 21.9 156.00 1999 30.85 Paranapanema Canoas I 17.95 35 Oligotrófico 63.3 7.4 9.9 234.90 1992 22.50 Paranapanema Canoas II 13.86 40 Oligotrófico 61.2 7.8 9.0 NA 1975 419.30 Paranapanema Capivara 13.04 34 Oligotrófico 58.6 7.5 5.5 196.00 1970 400.00 Paranapanema Chavantes 7.35 23 Oligotrófico 57.8 7.6 7.8 211.80 1986 220.00 Paranapanema Rosana 20.92 30 NA 58.2 7.7 NA 202.40 1958 12.00 Paranapanema Salto Grande 13.67 24 Oligotrófico 62.3 7.1 10.3 230.10 1989 80.10 Paranapanema Taquarucu 21.82 33 Oligotrófico 57.0 7.9 4.5 191.80 1962 0.10 Piriqui Melissa 6.29 12 Eutrófico 34.0 6.8 66.9 68.37 NA 0.07 Piriqui Santa Maria 9.40 7 Oligotrófico 41.7 6.8 14.9 480.10 1909 7.20 Tibagi Alagados 5.60 7 Oligotrófico 41.7 7.6 19.9 172.20 1958 NA Tibagi Apucaraninha 2.05 10 NA NA NA NA NA NA NA Tibagi Harmonia 24.88 7 Oligotrófico 31.0 8.3 8.6 113.30 2.9 Filtrando linhas da tabela Podemos fazer algo similar com as linhas utilizando a função filter(). Se quisermos selecionar somente os reservatórios da bacia do rio Paranapanema podemos fazer: res_paranapanema = res %&gt;% filter(Bacia == &#39;Paranapanema&#39;) res_paranapanema Reservatorio Bacia Fechamento Area Trofia pH Condutividade Alcalinidade P.total Riqueza CPUE Canoas I Paranapanema 1999 30.85 Oligotrófico 7.4 63.3 234.9 9.9 35 17.95 Canoas II Paranapanema 1992 22.50 Oligotrófico 7.8 61.2 NA 9.0 40 13.86 Capivara Paranapanema 1975 419.30 Oligotrófico 7.5 58.6 196.0 5.5 34 13.04 Chavantes Paranapanema 1970 400.00 Oligotrófico 7.6 57.8 211.8 7.8 23 7.35 Rosana Paranapanema 1986 220.00 NA 7.7 58.2 202.4 NA 30 20.92 Salto Grande Paranapanema 1958 12.00 Oligotrófico 7.1 62.3 230.1 10.3 24 13.67 Taquarucu Paranapanema 1989 80.10 Oligotrófico 7.9 57.0 191.8 4.5 33 21.82 Ou todos os reservatório exceto os da bacia do rio Paranapanema. res_outros = res %&gt;% filter(Bacia != &#39;Paranapanema&#39;) res_outros Reservatorio Bacia Fechamento Area Trofia pH Condutividade Alcalinidade P.total Riqueza CPUE Cavernoso Iguacu 1965 2.90 Oligotrófico 7.4 33.1 139.80 7.8 18 9.22 Curucaca Iguacu 1982 2.00 Oligotrófico 7.0 32.4 125.70 4.7 16 28.73 Foz do Areia Iguacu 1980 139.00 Oligotrófico 7.3 35.5 97.00 14.3 19 11.59 Irai Iguacu 2000 15.00 Eutrófico 6.9 50.2 3.30 53.4 12 30.76 JMF Iguacu 1970 0.45 Mesotrófico 7.3 40.2 3.70 41.2 18 5.95 Jordao Iguacu 1996 3.40 Oligotrófico 7.1 23.7 152.70 3.3 17 7.75 Passauna Iguacu 1978 14.00 Oligotrófico 8.8 125.6 526.00 15.2 11 7.51 Piraquara Iguacu 1979 3.30 Oligotrófico 7.1 22.8 50.67 4.5 8 4.01 Salto Caxias Iguacu 1998 124.00 Oligotrófico 7.3 39.6 106.00 12.1 21 20.83 Salto do Vau Iguacu 1959 2.90 Oligotrófico 6.5 23.2 279.00 11.0 8 2.43 Salto Osorio Iguacu 1975 51.00 Oligotrófico 8.6 38.9 233.30 3.4 24 12.55 Salto Santiago Iguacu 1979 208.00 Oligotrófico 9.2 39.5 117.60 13.1 21 11.73 Segredo Iguacu 1992 82.50 Oligotrófico 7.0 34.5 165.20 6.4 22 13.72 Mourao Ivai 1964 11.30 Oligotrófico 8.1 23.3 56.55 7.1 15 16.50 Patos Ivai NA 1.30 Mesotrófico 6.9 46.0 180.10 39.2 10 4.71 Guaricana Litoranea 1957 7.00 Oligotrófico 7.4 27.9 83.72 12.4 12 7.95 Parigot Souza Litoranea 1970 12.00 Oligotrófico 7.7 63.6 259.20 16.9 12 13.12 Salto do Meio Litoranea NA 0.10 Oligotrófico 6.9 37.4 147.10 17.1 11 16.10 Vossoroca Litoranea 1949 5.10 Mesotrófico 7.3 39.8 156.00 21.9 14 11.74 Melissa Piriqui 1962 0.10 Eutrófico 6.8 34.0 68.37 66.9 12 6.29 Santa Maria Piriqui NA 0.07 Oligotrófico 6.8 41.7 480.10 14.9 7 9.40 Alagados Tibagi 1909 7.20 Oligotrófico 7.6 41.7 172.20 19.9 7 5.60 Apucaraninha Tibagi 1958 NA NA NA NA NA NA 10 2.05 Harmonia Tibagi NA NA Oligotrófico 8.3 31.0 113.30 8.6 7 24.88 A função filter() utiliza operadores lógicos (retornam VERDADEIRO ou FALSO). Para aprender sobre este operadores no R veja Operadores lógicos Podemos realizar operações análogas para variáveis numéricas. Vamos selecionar somente os reservatórios com pH menor que \\(7.0\\). res_acidos = res %&gt;% filter(pH &lt; 7) res_acidos Reservatorio Bacia Fechamento Area Trofia pH Condutividade Alcalinidade P.total Riqueza CPUE Irai Iguacu 2000 15.00 Eutrófico 6.9 50.2 3.30 53.4 12 30.76 Salto do Vau Iguacu 1959 2.90 Oligotrófico 6.5 23.2 279.00 11.0 8 2.43 Patos Ivai NA 1.30 Mesotrófico 6.9 46.0 180.10 39.2 10 4.71 Salto do Meio Litoranea NA 0.10 Oligotrófico 6.9 37.4 147.10 17.1 11 16.10 Melissa Piriqui 1962 0.10 Eutrófico 6.8 34.0 68.37 66.9 12 6.29 Santa Maria Piriqui NA 0.07 Oligotrófico 6.8 41.7 480.10 14.9 7 9.40 Se quisermos pH’s menores ou iguais a \\(7.0\\) fazemos: res_acidos = res %&gt;% filter(pH &lt;= 7) res_acidos Reservatorio Bacia Fechamento Area Trofia pH Condutividade Alcalinidade P.total Riqueza CPUE Curucaca Iguacu 1982 2.00 Oligotrófico 7.0 32.4 125.70 4.7 16 28.73 Irai Iguacu 2000 15.00 Eutrófico 6.9 50.2 3.30 53.4 12 30.76 Salto do Vau Iguacu 1959 2.90 Oligotrófico 6.5 23.2 279.00 11.0 8 2.43 Segredo Iguacu 1992 82.50 Oligotrófico 7.0 34.5 165.20 6.4 22 13.72 Patos Ivai NA 1.30 Mesotrófico 6.9 46.0 180.10 39.2 10 4.71 Salto do Meio Litoranea NA 0.10 Oligotrófico 6.9 37.4 147.10 17.1 11 16.10 Melissa Piriqui 1962 0.10 Eutrófico 6.8 34.0 68.37 66.9 12 6.29 Santa Maria Piriqui NA 0.07 Oligotrófico 6.8 41.7 480.10 14.9 7 9.40 2.10 Adicionando ou modificando colunas Podemos adicionar novas colunas, assim como fazemos em uma planilha excel. Temos por exemplo, o ano de formação do reservatório. Assumindo que estes dados são de 2005, podemos criar uma coluna indicando a idade do reservatório no momento da tomada de dados. Para criar uma nova coluna usamos a função mutate(). res = res %&gt;% mutate(Idade = 2005 - Fechamento) res Reservatorio Bacia Fechamento Area Trofia pH Condutividade Alcalinidade P.total Riqueza CPUE Idade Cavernoso Iguacu 1965 2.90 Oligotrófico 7.4 33.1 139.80 7.8 18 9.22 40 Curucaca Iguacu 1982 2.00 Oligotrófico 7.0 32.4 125.70 4.7 16 28.73 23 Foz do Areia Iguacu 1980 139.00 Oligotrófico 7.3 35.5 97.00 14.3 19 11.59 25 Irai Iguacu 2000 15.00 Eutrófico 6.9 50.2 3.30 53.4 12 30.76 5 JMF Iguacu 1970 0.45 Mesotrófico 7.3 40.2 3.70 41.2 18 5.95 35 Jordao Iguacu 1996 3.40 Oligotrófico 7.1 23.7 152.70 3.3 17 7.75 9 Passauna Iguacu 1978 14.00 Oligotrófico 8.8 125.6 526.00 15.2 11 7.51 27 Piraquara Iguacu 1979 3.30 Oligotrófico 7.1 22.8 50.67 4.5 8 4.01 26 Salto Caxias Iguacu 1998 124.00 Oligotrófico 7.3 39.6 106.00 12.1 21 20.83 7 Salto do Vau Iguacu 1959 2.90 Oligotrófico 6.5 23.2 279.00 11.0 8 2.43 46 Salto Osorio Iguacu 1975 51.00 Oligotrófico 8.6 38.9 233.30 3.4 24 12.55 30 Salto Santiago Iguacu 1979 208.00 Oligotrófico 9.2 39.5 117.60 13.1 21 11.73 26 Segredo Iguacu 1992 82.50 Oligotrófico 7.0 34.5 165.20 6.4 22 13.72 13 Mourao Ivai 1964 11.30 Oligotrófico 8.1 23.3 56.55 7.1 15 16.50 41 Patos Ivai NA 1.30 Mesotrófico 6.9 46.0 180.10 39.2 10 4.71 NA Guaricana Litoranea 1957 7.00 Oligotrófico 7.4 27.9 83.72 12.4 12 7.95 48 Parigot Souza Litoranea 1970 12.00 Oligotrófico 7.7 63.6 259.20 16.9 12 13.12 35 Salto do Meio Litoranea NA 0.10 Oligotrófico 6.9 37.4 147.10 17.1 11 16.10 NA Vossoroca Litoranea 1949 5.10 Mesotrófico 7.3 39.8 156.00 21.9 14 11.74 56 Canoas I Paranapanema 1999 30.85 Oligotrófico 7.4 63.3 234.90 9.9 35 17.95 6 Canoas II Paranapanema 1992 22.50 Oligotrófico 7.8 61.2 NA 9.0 40 13.86 13 Capivara Paranapanema 1975 419.30 Oligotrófico 7.5 58.6 196.00 5.5 34 13.04 30 Chavantes Paranapanema 1970 400.00 Oligotrófico 7.6 57.8 211.80 7.8 23 7.35 35 Rosana Paranapanema 1986 220.00 NA 7.7 58.2 202.40 NA 30 20.92 19 Salto Grande Paranapanema 1958 12.00 Oligotrófico 7.1 62.3 230.10 10.3 24 13.67 47 Taquarucu Paranapanema 1989 80.10 Oligotrófico 7.9 57.0 191.80 4.5 33 21.82 16 Melissa Piriqui 1962 0.10 Eutrófico 6.8 34.0 68.37 66.9 12 6.29 43 Santa Maria Piriqui NA 0.07 Oligotrófico 6.8 41.7 480.10 14.9 7 9.40 NA Alagados Tibagi 1909 7.20 Oligotrófico 7.6 41.7 172.20 19.9 7 5.60 96 Apucaraninha Tibagi 1958 NA NA NA NA NA NA 10 2.05 47 Harmonia Tibagi NA NA Oligotrófico 8.3 31.0 113.30 8.6 7 24.88 NA Note que aqui, nós re-escrevemos o objeto res adicionando uma coluna Idade. A partir de agora, o objeto res tem uma coluna a mais. Mas lembre-se que nossa base de dados original .csv permanece inalterada. Podemos também fazer alterações em uma coluna existente usando a mesma função mutate. Por exemplo, a variável Area é dada em \\(km^2\\). Podemos transformá-la em Hectares multiplicando os valores por \\(100\\). res = res %&gt;% mutate(Area = Area * 100) res Reservatorio Bacia Fechamento Area Trofia pH Condutividade Alcalinidade P.total Riqueza CPUE Idade Cavernoso Iguacu 1965 290 Oligotrófico 7.4 33.1 139.80 7.8 18 9.22 40 Curucaca Iguacu 1982 200 Oligotrófico 7.0 32.4 125.70 4.7 16 28.73 23 Foz do Areia Iguacu 1980 13900 Oligotrófico 7.3 35.5 97.00 14.3 19 11.59 25 Irai Iguacu 2000 1500 Eutrófico 6.9 50.2 3.30 53.4 12 30.76 5 JMF Iguacu 1970 45 Mesotrófico 7.3 40.2 3.70 41.2 18 5.95 35 Jordao Iguacu 1996 340 Oligotrófico 7.1 23.7 152.70 3.3 17 7.75 9 Passauna Iguacu 1978 1400 Oligotrófico 8.8 125.6 526.00 15.2 11 7.51 27 Piraquara Iguacu 1979 330 Oligotrófico 7.1 22.8 50.67 4.5 8 4.01 26 Salto Caxias Iguacu 1998 12400 Oligotrófico 7.3 39.6 106.00 12.1 21 20.83 7 Salto do Vau Iguacu 1959 290 Oligotrófico 6.5 23.2 279.00 11.0 8 2.43 46 Salto Osorio Iguacu 1975 5100 Oligotrófico 8.6 38.9 233.30 3.4 24 12.55 30 Salto Santiago Iguacu 1979 20800 Oligotrófico 9.2 39.5 117.60 13.1 21 11.73 26 Segredo Iguacu 1992 8250 Oligotrófico 7.0 34.5 165.20 6.4 22 13.72 13 Mourao Ivai 1964 1130 Oligotrófico 8.1 23.3 56.55 7.1 15 16.50 41 Patos Ivai NA 130 Mesotrófico 6.9 46.0 180.10 39.2 10 4.71 NA Guaricana Litoranea 1957 700 Oligotrófico 7.4 27.9 83.72 12.4 12 7.95 48 Parigot Souza Litoranea 1970 1200 Oligotrófico 7.7 63.6 259.20 16.9 12 13.12 35 Salto do Meio Litoranea NA 10 Oligotrófico 6.9 37.4 147.10 17.1 11 16.10 NA Vossoroca Litoranea 1949 510 Mesotrófico 7.3 39.8 156.00 21.9 14 11.74 56 Canoas I Paranapanema 1999 3085 Oligotrófico 7.4 63.3 234.90 9.9 35 17.95 6 Canoas II Paranapanema 1992 2250 Oligotrófico 7.8 61.2 NA 9.0 40 13.86 13 Capivara Paranapanema 1975 41930 Oligotrófico 7.5 58.6 196.00 5.5 34 13.04 30 Chavantes Paranapanema 1970 40000 Oligotrófico 7.6 57.8 211.80 7.8 23 7.35 35 Rosana Paranapanema 1986 22000 NA 7.7 58.2 202.40 NA 30 20.92 19 Salto Grande Paranapanema 1958 1200 Oligotrófico 7.1 62.3 230.10 10.3 24 13.67 47 Taquarucu Paranapanema 1989 8010 Oligotrófico 7.9 57.0 191.80 4.5 33 21.82 16 Melissa Piriqui 1962 10 Eutrófico 6.8 34.0 68.37 66.9 12 6.29 43 Santa Maria Piriqui NA 7 Oligotrófico 6.8 41.7 480.10 14.9 7 9.40 NA Alagados Tibagi 1909 720 Oligotrófico 7.6 41.7 172.20 19.9 7 5.60 96 Apucaraninha Tibagi 1958 NA NA NA NA NA NA 10 2.05 47 Harmonia Tibagi NA NA Oligotrófico 8.3 31.0 113.30 8.6 7 24.88 NA 2.11 Renomeando colunas Finalmente, podemos renomear colunas para facilitar a leitura dos dados. A coluna P.total, por exemplo, indica a quantidade de fósforo total, enquanto a variável CPUE é a captura em kg. Podemos renomear estas colunas usando a função rename: res = res %&gt;% rename(Fosforo_total = P.total, Captura_kg = CPUE) res Reservatorio Bacia Fechamento Area Trofia pH Condutividade Alcalinidade Fosforo_total Riqueza Captura_kg Idade Cavernoso Iguacu 1965 290 Oligotrófico 7.4 33.1 139.80 7.8 18 9.22 40 Curucaca Iguacu 1982 200 Oligotrófico 7.0 32.4 125.70 4.7 16 28.73 23 Foz do Areia Iguacu 1980 13900 Oligotrófico 7.3 35.5 97.00 14.3 19 11.59 25 Irai Iguacu 2000 1500 Eutrófico 6.9 50.2 3.30 53.4 12 30.76 5 JMF Iguacu 1970 45 Mesotrófico 7.3 40.2 3.70 41.2 18 5.95 35 Jordao Iguacu 1996 340 Oligotrófico 7.1 23.7 152.70 3.3 17 7.75 9 Passauna Iguacu 1978 1400 Oligotrófico 8.8 125.6 526.00 15.2 11 7.51 27 Piraquara Iguacu 1979 330 Oligotrófico 7.1 22.8 50.67 4.5 8 4.01 26 Salto Caxias Iguacu 1998 12400 Oligotrófico 7.3 39.6 106.00 12.1 21 20.83 7 Salto do Vau Iguacu 1959 290 Oligotrófico 6.5 23.2 279.00 11.0 8 2.43 46 Salto Osorio Iguacu 1975 5100 Oligotrófico 8.6 38.9 233.30 3.4 24 12.55 30 Salto Santiago Iguacu 1979 20800 Oligotrófico 9.2 39.5 117.60 13.1 21 11.73 26 Segredo Iguacu 1992 8250 Oligotrófico 7.0 34.5 165.20 6.4 22 13.72 13 Mourao Ivai 1964 1130 Oligotrófico 8.1 23.3 56.55 7.1 15 16.50 41 Patos Ivai NA 130 Mesotrófico 6.9 46.0 180.10 39.2 10 4.71 NA Guaricana Litoranea 1957 700 Oligotrófico 7.4 27.9 83.72 12.4 12 7.95 48 Parigot Souza Litoranea 1970 1200 Oligotrófico 7.7 63.6 259.20 16.9 12 13.12 35 Salto do Meio Litoranea NA 10 Oligotrófico 6.9 37.4 147.10 17.1 11 16.10 NA Vossoroca Litoranea 1949 510 Mesotrófico 7.3 39.8 156.00 21.9 14 11.74 56 Canoas I Paranapanema 1999 3085 Oligotrófico 7.4 63.3 234.90 9.9 35 17.95 6 Canoas II Paranapanema 1992 2250 Oligotrófico 7.8 61.2 NA 9.0 40 13.86 13 Capivara Paranapanema 1975 41930 Oligotrófico 7.5 58.6 196.00 5.5 34 13.04 30 Chavantes Paranapanema 1970 40000 Oligotrófico 7.6 57.8 211.80 7.8 23 7.35 35 Rosana Paranapanema 1986 22000 NA 7.7 58.2 202.40 NA 30 20.92 19 Salto Grande Paranapanema 1958 1200 Oligotrófico 7.1 62.3 230.10 10.3 24 13.67 47 Taquarucu Paranapanema 1989 8010 Oligotrófico 7.9 57.0 191.80 4.5 33 21.82 16 Melissa Piriqui 1962 10 Eutrófico 6.8 34.0 68.37 66.9 12 6.29 43 Santa Maria Piriqui NA 7 Oligotrófico 6.8 41.7 480.10 14.9 7 9.40 NA Alagados Tibagi 1909 720 Oligotrófico 7.6 41.7 172.20 19.9 7 5.60 96 Apucaraninha Tibagi 1958 NA NA NA NA NA NA 10 2.05 47 Harmonia Tibagi NA NA Oligotrófico 8.3 31.0 113.30 8.6 7 24.88 NA 2.12 Outras operações para processamento e transformação de dados Existem muitas outras operações possíveis para processar conjuntos de dados. A maioria faz parte do conjunto de pacotes tidyverse (veja aqui). Neste capítulo vimos funções para: Importar arquivos .csv: read_delim; Verificar os tipos de dados de uma tabela: glimpse; Reorganizar informações: arrange; Selecionar colunas: select; Filtrar linhas: filter; Adicionar/modificar colunas: mutate; Renomear colunas: rename. A partir dos próximos capítulos iremos nos dedicar a aspectos da estatística descritiva, apresentando novas funções à medida que forem necessárias. "],["varqualit.html", "Capítulo 3 Descrevendo variáveis qualitativas 3.1 Representação em tabelas de frequência 3.2 Tabelas de frequência para variáveis categóricas ordenadas 3.3 Representação gráfica", " Capítulo 3 Descrevendo variáveis qualitativas Variáveis qualitativas podem ser categóricas não-ordenadas ou categóricas ordenadas. A descrição de variáveis desta natureza se dá por meio da contagem e da representação dos níveis destas variáveis por meio da contagem total, pelos valores relativos ou percentuais. Carregue o pacote tidyverse e importe novamente a base de dados Reservatorios_Parana_parcial.csv. # Carrega pacotes library(tidyverse) # Importa base de dados res = read_delim(&#39;Reservatorios_Parana_parcial.csv&#39;, delim = &#39;,&#39;, locale = locale(decimal_mark = &#39;.&#39;, encoding = &#39;latin1&#39;)) Na tabela, temos 3 variáveis categóricas: Reservatorio, Bacia e Trofia. A primeira identifica cada reservatório pelo seu nome. A segunda é uma variável categórica não-ordenada (nível de mensuração nominal) e a terceira uma variável categórica ordenada (nível de mensuração ordinal). 3.1 Representação em tabelas de frequência Como discutimos no capítulo 1, se uma variável é descrita no nível de mensuração nominal, como é o caso de Bacia, nossa única ação possível é a contagem da frequência com que cada um dos níveis aparece na variável. Essa contagem pode ser obtda por meio de uma tabela de frequências. fbacia = res %&gt;% group_by(Bacia) %&gt;% summarise(Frequencia = n()) fbacia Bacia Frequencia Iguacu 13 Ivai 2 Litoranea 4 Paranapanema 7 Piriqui 2 Tibagi 3 O resultado mostra que existem 13 reservatórios na tabela pertancentes à bacia do rio 13, 2 à bacia do rio 2 e assim por diante. As linhas da tabela estão organizadas em ordem alfabética. Como esata é uma variável não-ordenada, isto em sí não é um problerma. No entanto, para facilitar a visualização, podemos ordená-las de modo decrescente como função do número de reservatórios por bacia: fbacia = fbacia %&gt;% arrange(desc(Frequencia)) fbacia Bacia Frequencia Iguacu 13 Paranapanema 7 Litoranea 4 Tibagi 3 Ivai 2 Piriqui 2 Podemos olhar também para a frequência relativa do número de reservatórios por bacia. fbacia_rel = fbacia %&gt;% mutate(Freq_relativa = Frequencia / sum(Frequencia)) fbacia_rel Bacia Frequencia Freq_relativa Iguacu 13 0.4193548 Paranapanema 7 0.2258065 Litoranea 4 0.1290323 Tibagi 3 0.0967742 Ivai 2 0.0645161 Piriqui 2 0.0645161 A característica da frequência relativa, é que o somatório da coluna deve ser igual a \\(1\\), enquanto a frequência numérica tem o somatório igual ao número de linhas na tabela. ## # A tibble: 1 × 2 ## Frequencia Freq_relativa ## &lt;int&gt; &lt;dbl&gt; ## 1 31 1 3.2 Tabelas de frequência para variáveis categóricas ordenadas A característica da variável Trofia difere da anterior unicamente por ser uma variável categórica ordenada que no caso, expressa o grau de eutrofização dos reservatórios. Neste sentido, a única mudança na representação da variável se deve ao fato de que existe uma sequência natural para representar os níveis. Podemos indicar que uma determinada variável é categórica ordenada fazendo uma pequena alteração na base de dados. Se montarmos uma tabela de frequência da variável trofia, teremos as linhas organizadas em ordem alfabética: ftrofia = res %&gt;% group_by(Trofia) %&gt;% summarise(Frequencia = n()) ftrofia Trofia Frequencia Eutrófico 2 Mesotrófico 3 Oligotrófico 24 NA 2 Se desejarmos que as colunas apareçam como função do nível de Eutrofização, devemos primeiro transformar a variável bacia em um fator ordenado que é o modo como o R interpreta uma variável categórica ordenada. Use o comando: glimpse(res) ## Rows: 31 ## Columns: 11 ## $ Reservatorio &lt;chr&gt; &quot;Cavernoso&quot;, &quot;Curucaca&quot;, &quot;Foz do Areia&quot;, &quot;Irai&quot;, &quot;JMF&quot;, … ## $ Bacia &lt;chr&gt; &quot;Iguacu&quot;, &quot;Iguacu&quot;, &quot;Iguacu&quot;, &quot;Iguacu&quot;, &quot;Iguacu&quot;, &quot;Iguac… ## $ Fechamento &lt;dbl&gt; 1965, 1982, 1980, 2000, 1970, 1996, 1978, 1979, 1998, 19… ## $ Area &lt;dbl&gt; 2.90, 2.00, 139.00, 15.00, 0.45, 3.40, 14.00, 3.30, 124.… ## $ Trofia &lt;chr&gt; &quot;Oligotrófico&quot;, &quot;Oligotrófico&quot;, &quot;Oligotrófico&quot;, &quot;Eutrófi… ## $ pH &lt;dbl&gt; 7.4, 7.0, 7.3, 6.9, 7.3, 7.1, 8.8, 7.1, 7.3, 6.5, 8.6, 9… ## $ Condutividade &lt;dbl&gt; 33.1, 32.4, 35.5, 50.2, 40.2, 23.7, 125.6, 22.8, 39.6, 2… ## $ Alcalinidade &lt;dbl&gt; 139.80, 125.70, 97.00, 3.30, 3.70, 152.70, 526.00, 50.67… ## $ P.total &lt;dbl&gt; 7.8, 4.7, 14.3, 53.4, 41.2, 3.3, 15.2, 4.5, 12.1, 11.0, … ## $ Riqueza &lt;dbl&gt; 18, 16, 19, 12, 18, 17, 11, 8, 21, 8, 24, 21, 22, 15, 10… ## $ CPUE &lt;dbl&gt; 9.22, 28.73, 11.59, 30.76, 5.95, 7.75, 7.51, 4.01, 20.83… para verificar que o R entende a variável Trofia como um character (&lt;chr&gt;). Iremos transformar esta variável para que o R a interprete como uma variável categórica ordenada fazendo: res = res %&gt;% mutate(Trofia = factor(Trofia, ordered = TRUE, levels = c(&#39;Oligotrófico&#39;, &#39;Mesotrófico&#39;, &#39;Eutrófico&#39;))) Após aplicarmos este comando, vemos que agora o R reconhece esta variável como do tipo &lt;ord&gt;: glimpse(res) ## Rows: 31 ## Columns: 11 ## $ Reservatorio &lt;chr&gt; &quot;Cavernoso&quot;, &quot;Curucaca&quot;, &quot;Foz do Areia&quot;, &quot;Irai&quot;, &quot;JMF&quot;, … ## $ Bacia &lt;chr&gt; &quot;Iguacu&quot;, &quot;Iguacu&quot;, &quot;Iguacu&quot;, &quot;Iguacu&quot;, &quot;Iguacu&quot;, &quot;Iguac… ## $ Fechamento &lt;dbl&gt; 1965, 1982, 1980, 2000, 1970, 1996, 1978, 1979, 1998, 19… ## $ Area &lt;dbl&gt; 2.90, 2.00, 139.00, 15.00, 0.45, 3.40, 14.00, 3.30, 124.… ## $ Trofia &lt;ord&gt; Oligotrófico, Oligotrófico, Oligotrófico, Eutrófico, Mes… ## $ pH &lt;dbl&gt; 7.4, 7.0, 7.3, 6.9, 7.3, 7.1, 8.8, 7.1, 7.3, 6.5, 8.6, 9… ## $ Condutividade &lt;dbl&gt; 33.1, 32.4, 35.5, 50.2, 40.2, 23.7, 125.6, 22.8, 39.6, 2… ## $ Alcalinidade &lt;dbl&gt; 139.80, 125.70, 97.00, 3.30, 3.70, 152.70, 526.00, 50.67… ## $ P.total &lt;dbl&gt; 7.8, 4.7, 14.3, 53.4, 41.2, 3.3, 15.2, 4.5, 12.1, 11.0, … ## $ Riqueza &lt;dbl&gt; 18, 16, 19, 12, 18, 17, 11, 8, 21, 8, 24, 21, 22, 15, 10… ## $ CPUE &lt;dbl&gt; 9.22, 28.73, 11.59, 30.76, 5.95, 7.75, 7.51, 4.01, 20.83… E se fizermos: res$Trofia ## [1] Oligotrófico Oligotrófico Oligotrófico Eutrófico Mesotrófico ## [6] Oligotrófico Oligotrófico Oligotrófico Oligotrófico Oligotrófico ## [11] Oligotrófico Oligotrófico Oligotrófico Oligotrófico Mesotrófico ## [16] Oligotrófico Oligotrófico Oligotrófico Mesotrófico Oligotrófico ## [21] Oligotrófico Oligotrófico Oligotrófico &lt;NA&gt; Oligotrófico ## [26] Oligotrófico Eutrófico Oligotrófico Oligotrófico &lt;NA&gt; ## [31] Oligotrófico ## Levels: Oligotrófico &lt; Mesotrófico &lt; Eutrófico Temos agora a indicação de que: Oligotrófico &lt; Mesotrófico &lt; Eutrófico, expressando a sequência que definimos para esta variável ordinal. A partir de agora, se extrairmos uma tabela de frequência relativa, as linhas apareceram na ordem pré-definida. ftrofia = res %&gt;% group_by(Trofia) %&gt;% summarise(Frequencia = n()) ftrofia Trofia Frequencia Oligotrófico 24 Mesotrófico 3 Eutrófico 2 NA 2 Caso você não queira representar os dados faltantes (NA), é possível utilizar a função drop_na() para excluir linhas com NAs. # Exclui linhas da tabela referentes aos dados faltantes em `Trofia` ftrofia = res %&gt;% drop_na(Trofia) %&gt;% group_by(Trofia) %&gt;% summarise(Frequencia = n()) ftrofia Trofia Frequencia Oligotrófico 24 Mesotrófico 3 Eutrófico 2 Podemos adicionar uma uma coluna de frequência relativa como fizemos anteriormente. ftrofia_rel = ftrofia %&gt;% mutate(Freq_relativa = Frequencia / sum(Frequencia)) ftrofia_rel Trofia Frequencia Freq_relativa Oligotrófico 24 0.8275862 Mesotrófico 3 0.1034483 Eutrófico 2 0.0689655 3.3 Representação gráfica Variáveis categóricas não-ordenadas ou ordenadas podem ser representadas por gráficos de barras. Utilizaremos o pacote ggplot2 para representar graficamente as variáveis. O ggplot2 é instalado e habilitado juntamente com o tidyverse, de modo que neste momento você já o tem habilitado em sua seção do R. Para uma rápida explicação do ggplot2 veja aqui. Para uma explicação detalhada veja o site oficial (ggplot2) e o livro ggplot2: Elegant Graphics for Data Analysis. 3.3.1 Criando um gráfico no ggplot2 Um gráfico no ggplot2 é feito em camadas que devem ter minimamente: a definição da tabela de dados, a estética gráfica (quais serão as variáveis envolvidas e suas posições no gráficos) e o formato (gráficos de pontos, linhas, barras, etc.). Esta abordagem permite que tenhamos um método consistente para construir diferentes tipos gráficos. Gráfico de frequência Um gráfico de barras da variável Bacia ficaria: ggplot(data = res) + aes(x = Bacia) + geom_bar() Vamos entender o comando: ggplot(res): define a tabela de dados que será utilizada. aes(x = Bacia): define que o eixo \\(x\\) deste gráfico deverá contém os níveis da variável Bacia. geom_bar(): define o tipo gráfico (no ggplot2 é denominado de geometria gráfica) Estes argumentos devem ser inseridos sequencialmente separtados pelo símbolo +. O argumento geom_bar() espera como argumento uma variável qualitativa em um dos eixos e, por padrão, fará a contagem dos níveis dentro da variável e apresentará esta contagem no outro eixo. Deste modo, poderíamos ter feito o mesmo gráfico de barras indicando que a variável Bacia seria representada no eixo \\(y\\) o que resultaria em um gráfico de barras invertido conforme abaixo: ggplot(data = res) + aes(y = Bacia) + geom_bar() A estética gráfica (comando aes()) não precisa estar em uma linha separada. Também não é obrigatório escrevermos data = res. De fato, é mais comun escrevermos esta sequência de argumentos como: ggplot(res, mapping = aes(x = Bacia)) + geom_bar() ou simplesmente: ggplot(res, aes(x = Bacia)) + geom_bar() Finalmente, poderíamos organizar os barras em ordem decrescente como fizemos com as tabelas de frequência, utilizando a função fct_infreq(): ggplot(res, aes(x = fct_infreq(Bacia))) + geom_bar() ou em ordem crescente revertendo o comando anterior, com a função fct_rev(). ggplot(res, aes(x = fct_rev(fct_infreq(Bacia)))) + geom_bar() Formatanto a figura Para tormar a figura mais auto-explicativa, podemos adicionar camadas identificando os eixos e fornecendo, título, subtítulo e outras informações: ggplot(res, aes(x = Bacia)) + geom_bar() + labs(title = &#39;Reservatórios do Estado do Paraná&#39;, subtitle = &#39;Reservatórios por bacia hidrográfica&#39;, caption = &#39;Dados obtidos do livro: Biocenoses em Reservatórios&#39;, x = &#39;Bacia hidrográfica&#39;, y = &#39;Frequência&#39;) Gráfico de frequência relativa Utilizando o ggplot2 é simples construir um gráfico de frequência relativa. ggplot(res, aes(x = Bacia, y = ..prop.., group = 1)) + geom_bar() Veja que para isto transformou as contagens em proporções. Se quisermos transformar em percentuais então: ggplot(res, aes(x = Bacia, y = ..prop.., group = 1)) + geom_bar() + scale_y_continuous(labels = scales::percent) Outras opções para construir um gráfico de barras As figuras que acabamos de fazer apresentam, do modo gráfico, as mesmas informações das tabelas de frequência vistas no início do capítulo sem que fosse necessário contruir a tabela de frequencia, pois o comando geom_bar() já realiza esta contagem. Entretanto, caso já tivéssemos a tabela de frequência também poderíamos utilizá-la diretamente. No início do capítulo, contruímos a tabela fbacia_rel onde tínhamos 3 colunas: Bacia, Frequencia, Freq_relativa. Podemos construir gráficos de barras das tabelas Frequencia ou Freq_relativa da seguinte forma: ggplot(fbacia_rel, aes(x = Bacia, y = Frequencia)) + geom_bar(stat = &#39;identity&#39;) e ggplot(fbacia_rel, aes(x = Bacia, y = Freq_relativa)) + geom_bar(stat = &#39;identity&#39;) Para utilizar diretamente uma tabela de frequências, devemos oferecer a variável do eixo \\(x\\), do eixo \\(y\\) e no comando geom_bar() adicionar o argumento stat = 'identity'. Feito isso, o comando utiliza diretamente os números disponíveis em cada linha da coluna Frequencia. Gráfico de frequência para variáveis categóricas ordenadas Para variáveis categóricas ordenadas valem os mesmos comandos apresentados acima. Usamos a função geom_bar() para construir os gráficos de barras. A diferença é que antes da construção, é necessário que a variável em questão tenha sido transformada para um fator ordenado. Lembrando o que fizemos no início do capítulo, esta transformação pode ser feita para a variável Trofia com os comandos: res = res %&gt;% mutate(Trofia = factor(Trofia, ordered = TRUE, levels = c(&#39;Oligotrófico&#39;, &#39;Mesotrófico&#39;, &#39;Eutrófico&#39;))) Feito isto, o comando geom_bar() vai organizar os níveis de acordo com a sequência definida: ggplot(res, aes(x = Trofia)) + geom_bar() E caso seja necessário retirar reservatórios com dados faltantes em Trofia, podemos fazer: res %&gt;% drop_na(Trofia) %&gt;% ggplot(aes(x = Trofia)) + geom_bar() No comando acima, a tabela de dados não entra dentro do comando ggplot(), ela é inicialmente processada e usamos o operador %&gt;% para inserir o resultado do processamento no comnado gráfico. Esta é outra maneira de combinar capacidade de processamento de dados e apresentação gráfica com o conjunto de pacotes em tidyverse. "],["varquant.html", "Capítulo 4 Descrevendo variáveis quantitativas 4.1 Tabelas de frequência para variáveis quantitativas 4.2 Representação gráfica: histogramas", " Capítulo 4 Descrevendo variáveis quantitativas Variáveis quantitativas podem ser discretas ou contínuas. A descrição dos padrões de distribuição destes tipos de variáveis é feita utilizando tabelas (frequência e frequência acumulada) e gráficos (histogramas ou gráficos de frequência acumulada). 4.1 Tabelas de frequência para variáveis quantitativas A construção de tabelas de frequências para variáveis quantitativas necessita que agrupemos as observações em faixas de valores. Veja as observações abaixo por exemplo: \\(X =\\) {2.66, 3.72, 5.73, 9.08, 2.02, 8.98, 9.45, 6.61, 6.29, 0.62} Podemos agrupá-las nas seguintes faixas de valores: (0,2], (2,4], (4,6], (6,8], (8,10] Estas faixas de valores são denominadas de intervalos de classe. Se alocadas nestes intervalos, as observações ficam: X Classes 2.66 (2,4] 3.72 (2,4] 5.73 (4,6] 9.08 (8,10] 2.02 (2,4] 8.98 (8,10] 9.45 (8,10] 6.61 (6,8] 6.29 (6,8] 0.62 (0,2] Uma tabela de frequência para estas observações é construínda contando o número de observações por intervalo de classes. Neste caso: Classes Frequencia (0,2] 1 (2,4] 3 (4,6] 1 (6,8] 2 (8,10] 3 Na coluna Frequencia temos o número de observações da variável X para cada um dos intervalos de classe. 4.1.1 Alterando o tamanho dos intervalos de classe No exemplo anterior definimos os limites dos intervalos de classe de 2 em 2 unidades. Poderíamos ter escolhido outros tamanhos, por exemplo, de 4 em 4. Neste caso teríamos: Classes Frequencia (0,4] 4 (4,8] 3 (8,12] 3 Note que ao escolhermos o tamanho dos intervalos de classe, estamos criando a variável qualitativa ordinal Classes, a partir do agrupamento das observações em X. Neste sentido, não há um único tamanho correto para os intervalos de classe. O objetivo é encontrar um tamanho que permita evidenciar os padrões de distribuição da variável sem perdermos muitos detalhes. Poderíamos escolher um tamanho muito grande, de 5 em 5. Neste caso teríamos somente 2 grupos. Classes Frequencia (0,5] 4 (5,10] 6 Por outro lado, poderíamos escolher um tamanho muito pequeno, por exemplo de 1 e 1. Classes Frequencia (0,1] 1 (2,3] 2 (3,4] 1 (5,6] 1 (6,7] 2 (8,9] 1 (9,10] 2 Nas duas situações, não é possível evidenciar os padrões de distribuição da variável X. Na primeira, perdemos muita informação agrupando as observações em somente duas faixas e na última, perdemos a capacidade de visualizar os padrões de distribuição de X. 4.1.2 Tabela de frequência para CPUE Carregue o pacote tidyverse e importe novamente a base de dados Reservatorios_Parana_parcial.csv. # Carrega pacotes library(tidyverse) # Importa base de dados res = read_delim(&#39;Reservatorios_Parana_parcial.csv&#39;, delim = &#39;,&#39;, locale = locale(decimal_mark = &#39;.&#39;, encoding = &#39;latin1&#39;)) No objeto res temos 8 variáveis quantitativas: Fechamento, Area, pH, Condutividade, Alcalinidade, P.total, Riqueza, CPUE. Vamos verificar a como fica uma tabela de frequências para a variável CPUE que expressa a captura em \\(kg\\) de peixes em cada reservatório. Inicialmente, vamos selecionar somente esta coluna da tabela e visualizá-la em ordem crescente. res %&gt;% select(CPUE) %&gt;% arrange(CPUE) CPUE 2.05 2.43 4.01 4.71 5.60 5.95 6.29 7.35 7.51 7.75 7.95 9.22 9.40 11.59 11.73 11.74 12.55 13.04 13.12 13.67 13.72 13.86 16.10 16.50 17.95 20.83 20.92 21.82 24.88 28.73 30.76 Vemos que o menor valor é 2.05 \\(kg\\) e o maior 30.76 \\(kg\\). Assumindo que temos 31 observações, vamos criar um intervalo de classes de 5 em 5 unidades. Para isto, criaremos a variável cl_cpue que será uma sequência de \\(0\\) a \\(35\\), com tamanho \\(5\\). Os valores nesta sequencia são os limites de classe cl_cpue = seq(from = 0, to = 35, by = 5) cl_cpue ## [1] 0 5 10 15 20 25 30 35 Utilizaremos os limites de classe para gerar uma nova coluna, delimitando os intervalos a que cada observação pertence. para isto utilizaremos a função cut. tab_cpue = res %&gt;% select(CPUE) %&gt;% mutate(int_cpue = cut(CPUE, breaks = cl_cpue)) tab_cpue CPUE int_cpue 9.22 (5,10] 28.73 (25,30] 11.59 (10,15] 30.76 (30,35] 5.95 (5,10] 7.75 (5,10] 7.51 (5,10] 4.01 (0,5] 20.83 (20,25] 2.43 (0,5] 12.55 (10,15] 11.73 (10,15] 13.72 (10,15] 16.50 (15,20] 4.71 (0,5] 7.95 (5,10] 13.12 (10,15] 16.10 (15,20] 11.74 (10,15] 17.95 (15,20] 13.86 (10,15] 13.04 (10,15] 7.35 (5,10] 20.92 (20,25] 13.67 (10,15] 21.82 (20,25] 6.29 (5,10] 9.40 (5,10] 5.60 (5,10] 2.05 (0,5] 24.88 (20,25] A nova tabela tab_cpue tem agora duas colunas, os valores numéricos de CPUE e os valores transformados em intervalos de classe, int_cpue. É com esta última que montaremos a tabela de frequência. fre_cpue = tab_cpue %&gt;% group_by(int_cpue) %&gt;% summarise(Frequencia = n()) fre_cpue int_cpue Frequencia (0,5] 4 (5,10] 9 (10,15] 9 (15,20] 3 (20,25] 4 (25,30] 1 (30,35] 1 e, em seguida de frequência relativa: fre_cpue = fre_cpue %&gt;% mutate(Freq_relativa = Frequencia / sum(Frequencia)) fre_cpue int_cpue Frequencia Freq_relativa (0,5] 4 0.1290323 (5,10] 9 0.2903226 (10,15] 9 0.2903226 (15,20] 3 0.0967742 (20,25] 4 0.1290323 (25,30] 1 0.0322581 (30,35] 1 0.0322581 Veja que os intervalos de (5,10] e (10,15] contém o maior número de observações, cerca de 29% cada um, e que acima de \\(25\\) \\(kg\\) temos somentes duas observações. 4.1.3 Tabela de frequência acumulada Outra forma de representar o padrão de distribuição para uma variável quantitativa é apresentá-la em uma tabela de frequência acumulada. Fazemos isso, somando de forma cumulativa as observações em cada classe de intervalo e criando duas colunas adicionais de frequência acumulada e de frequência relativa acumulada. fre_cpue = fre_cpue %&gt;% mutate(Freq_ac = cumsum(Frequencia), Relativa_ac = cumsum(Freq_relativa)) fre_cpue int_cpue Frequencia Freq_relativa Freq_ac Relativa_ac (0,5] 4 0.1290323 4 0.1290323 (5,10] 9 0.2903226 13 0.4193548 (10,15] 9 0.2903226 22 0.7096774 (15,20] 3 0.0967742 25 0.8064516 (20,25] 4 0.1290323 29 0.9354839 (25,30] 1 0.0322581 30 0.9677419 (30,35] 1 0.0322581 31 1.0000000 Veja agora que a última linha da coluna de frequência acumulada é igual ao número de observações total e que a da frequência relativa acumulada é igual a \\(1\\). 4.2 Representação gráfica: histogramas Histogramas são regresentações das tabelas de frequência e de frequência relativa. Um histograma da coluna CPUE pode ser feito com o comando: ggplot(res, aes(x = CPUE)) + geom_histogram() Os intervalos de classe foram escolhidos automaticamente pela função geom_histogram. Se quisermos ter o controle sobre estes intervalos, podemos adicionar o argumento beaks = e a sequência com os limites de classe que criamos anteriormente: ggplot(res, aes(x = CPUE)) + geom_histogram(breaks = cl_cpue) A formatação do histograma acima pode ser melhorada de diversas formas, por exemplo: ggplot(res, aes(x = CPUE, label = ..count..)) + geom_histogram(breaks = cl_cpue, fill = &#39;darkblue&#39;, color = &#39;white&#39;) + labs(x = &#39;Captura em kg&#39;, y = &#39;Frequência&#39;) + geom_text(stat = &quot;bin&quot;, size = 6, vjust = 1.5, color = &#39;white&#39;, breaks = cl_cpue) + theme_classic() Modificamos a cor do preenchimento (fill = 'darkblue'), e identificamos as barras individualmente traçando uma linha branca entre elas (color = 'white'); Re-escrevemos o rótulo dos eixos \\(x\\) e \\(y\\) (labs()); identificamos as frequências em cada barra individualmente com o argumento label = ..count.. e a função geom_text. Experimente alterar os argumentos dentro de geom_text para entender cada um deles. modificamos o tema do gráfico para obter uma alteração geral na aparência da figura. Existem diversos outros temas possíveis que podem ser vistos aqui. Um histograma com a frequência relativa pode ser obtido com: ggplot(res, aes(x = CPUE, y = (..count..)/sum(..count..), label = round((..count..)/sum(..count..),2))) + geom_histogram(breaks = cl_cpue, fill = &#39;darkblue&#39;, color = &#39;white&#39;) + labs(x = &#39;Captura em kg&#39;, y = &#39;Frequência relativa&#39;) + geom_text(stat = &quot;bin&quot;, size = 6, vjust = 1.5, color = &#39;white&#39;, breaks = cl_cpue) + theme_classic() Aqui fizemos duas mudanças: inserimos o argumento y = (..count..)/sum(..count..) para dizer que as barras em \\(y\\) devem mostrar a contagem do número de observações em cada intervalo dividido pelo total e; modificamos o argumento label = round((..count..)/sum(..count..),2) de modo que também mostre a frequência relativa. Neste argumento, entretanto, tivemos que utilizar uma função de arredondamento round(__, 2) para dizer que os nomes abaixo das barras deveria ser mostrados somente com duas casas decimais. Finalmente, modificamos o nome do eixo \\(y\\) para y = 'Frequência relativa'. 4.2.1 Representando frequências acumuladas A única modificação neste caso será a identificarmos o eixo \\(y\\) por sua contagem acumulada: y=cumsum(..count..). ggplot(res, aes(x = CPUE, y = cumsum(..count..), label = round(cumsum(..count..),2))) + geom_histogram(breaks = cl_cpue, fill = &#39;darkblue&#39;, color = &#39;white&#39;) + labs(x = &#39;Captura em kg&#39;, y = &#39;Frequência acumulada&#39;) + geom_text(stat = &quot;bin&quot;, size = 6, vjust = 1.5, color = &#39;white&#39;, breaks = cl_cpue) + theme_classic() Para fazer o mesmo mostrando as frequências relativas fazemos: ggplot(res, aes(x = CPUE, y = cumsum(..count../sum(..count..)), label = round(cumsum(..count../sum(..count..)),2))) + geom_histogram(breaks = cl_cpue, fill = &#39;darkblue&#39;, color = &#39;white&#39;) + labs(x = &#39;Captura em kg&#39;, y = &#39;Frequência acumulada relativa&#39;) + geom_text(stat = &quot;bin&quot;, size = 6, vjust = 1.5, color = &#39;white&#39;, breaks = cl_cpue) + theme_classic() "],["tendcentral.html", "Capítulo 5 Medidas de tendência central 5.1 Média aritmética 5.2 Mediana 5.3 Moda 5.4 Ponto médio 5.5 Efeito da assimetria sobre os descritores de tendência central 5.6 Obtendo medidas de uma tabela de dados", " Capítulo 5 Medidas de tendência central Tabelas de frequência e histogramas (capítulo 4) permitem a visualização dos padrões de distribuição de uma variável quantitativa, evidenciando limites inferiores e superiores, faixas de valores mais ou menos frequentes etc. Neste capítulo, veremos medidas-resumo que possibilitam descrever a tendência central de um conjunto de dados. Estas medidas são a média aritmética, a mediana, a moda e o ponto médio. 5.1 Média aritmética Considere a variável \\(X\\) com \\(n\\) elementos \\(X_1\\), \\(X_2\\), \\(X_3\\), \\(\\cdots, X_n\\). A média aritmética (\\(\\overline{X}\\)) é dada por: \\[\\overline{X}=\\frac{X_1+X_2+X_3+\\cdots+X_n}{n}=\\frac{\\sum_{i=1}^n{X_i}}{n}\\] Exemplo Seja a variável \\(X\\) com 5 observações: \\(X =\\) {9, 4, 7, 1, 2} \\(\\overline{X}=\\frac{9 + 4 + 7 + 1 + 2}{5}\\) \\(\\overline{X}=\\frac{23}{5} = 4.6\\) IMPORTANTE! o símbolo \\(\\overline{X}\\) refere-se à média aritmética de uma amostra. Nas seções sobre Amostragem e Delineamento e Inferência e Teste de Hipóteses (Capítulos 12 a 18) faremos distinção entre média amostral (\\(\\overline{X}\\)) e a média populacional (\\(\\mu\\)). 5.2 Mediana É definida como o valor do meio de uma distribuição, de modo que metade dos valores estão abaixo e metade está acima da mediana. Se organizarmos a variável \\(X\\) em ordem crescente teremos: \\(X =\\) {1,2 , 4 , 7,9} sendo a mediana igual a \\(4\\). Neste exemplo, temos \\(n = 6\\) observações. Se tivermos um número par de observações, teremos duas na posição central. Por exemplo se: \\(X =\\) {9, 4, 7, 1, 2, 7} vemos que após ordenarmos \\(X\\): \\(X =\\) {1, 2, 4, 7, 7, 9} teremos o \\(4\\) e o \\(7\\) como valores do meio. Neste caso, a mediana fica como sendo: \\(\\frac{4 + 7}{2} = 5.5\\) 5.3 Moda É definida como o valor mais frequente de uma distribuição. Para \\(X =\\) {9, 4, 7, 1, 2, 7} a moda é 7, o valor que mais se repete na distribuição. A moda nem sempre existe. Caso cada valor se repita uma única vez, não haverá moda. Se vários valores repetem-se igualmente, teremos mais de uma moda na distribuição. 5.4 Ponto médio É calculado com base nos dois valores extremos da distribuição (mínimo e máximo), sendo obtido por: \\[P_{medio}=\\frac{X_{minimo} + X_{maximo}}{2}\\] Para \\(X =\\) {9, 4, 7, 1, 2, 7} o ponto médio é: \\(PM = \\frac{1 + 9}{2} = \\frac{10}{2} = 5\\) 5.5 Efeito da assimetria sobre os descritores de tendência central Cada um dos drescitores de tendência central descritos acima é mais ou menos sensível ao grau de assimetria de uma distribuiçãol. Em uma distribuição perfeitamente simétrica, onde as observações estão igualmente dispersas acima e abaixo do ponto central, os valores da média, mediana, moda e ponto médio coincidem. Por outro lado, pode ocorrer da distribuição ser assimétrica. Neste caso, a posição relativa dos descritores irá depender se a assimetria é à direita ou à esquerda. Esta discrepância ocorre devido à sensibilidade destes descritores a valores extremos na distribuição. O ponto médio é o mais sensível à presença de pontos extremos, seguido da média, mediana e a moda. Figure 5.1: Efeito da assimetria de uma distribuição sobre o ponto médio, a média aritmética, a mediana e a moda Média aritmética: utiliza todo o conjunto de dados. Relativamente sensível a valores extremos; Mediana: o valor do meio. Metade dos pontos está acima e metade abaixo da mediana. A mediana é uma medida resistente a valores extremos; Moda: valor mais frequente. Se mais de um valor aparece com a mesma frequência, os dados têm uma distribuição multimodal; Ponto médio: considera somente o valor mínimo e máximo. O ponto médio é fácil de calcular porém não utiliza a maioria do conjunto de dados e é muito sensível a valores extremos. 5.6 Obtendo medidas de uma tabela de dados Carregue o pacote tidyverse e importe novamente a base de dados Reservatorios_Parana_parcial.csv. # Carrega pacotes library(tidyverse) # Importa base de dados res = read_delim(&#39;Reservatorios_Parana_parcial.csv&#39;, delim = &#39;,&#39;, locale = locale(decimal_mark = &#39;.&#39;, encoding = &#39;latin1&#39;)) Medidas-resumo pode ser obtidas geralmente pela função summarise Vamos encontrar a média aritmética e a mediana da variável CPUE. res %&gt;% summarise(CPUE_medio = mean(CPUE), CPUE_mediana = median(CPUE)) CPUE_medio CPUE_mediana 12.70097 11.74 Os valores são parecidos, porém a média é um pouco superior. Provavelmente a distribuição deva ser ligeiramente assimétrica à direita. Podemos verificar isto por meio de um histograma de disdribuição: Neste caso, alguns valores de captura próximos a \\(30\\) kg estão fazendo com que a média esteja um pouco acima da mediana. Vamos verificar agora a média e a media da Area dos reservatórios: res %&gt;% summarise(CPUE_medio = mean(Area, na.rm = TRUE), CPUE_mediana = median(Area, na.rm = TRUE)) CPUE_medio CPUE_mediana 64.7369 12 Para esta variável a discrepância é muito maior e a causa disto pode ser compreendida observando o histograma da distribuição. obs: tivemos que utilizar o argumento na.rm = TRUE para excluir do cálculo reservatórios com dados faltantes para Area. cl_area = seq(0, 500, by = 50) ggplot(res, aes(x = Area)) + geom_histogram(breaks = cl_area, color = &#39;white&#39;) Existe uma grande concentração de reservatórios com área até \\(50\\) \\(km^2\\), porém poucos reservatórios muito grandes com mais de \\(200\\) \\(km^2\\). Este valores deslocam a média aritmética à direita. Podemos ver quem são estes reservatórios utilizando a função filter res %&gt;% filter(Area &gt;= 200) %&gt;% select(Reservatorio, Area) Reservatorio Area Salto Santiago 208.0 Capivara 419.3 Chavantes 400.0 Rosana 220.0 Vemos que dentro os 31 temos 4 com área acima de \\(200\\) \\(km^2\\), os reservatórios de Salto Santiago, Capivara, Chavantes, Rosana. A influência destes reservatórios é maior para a média aritmética que é mais sensível a valores extremos, do que para a mediana. Se calcularmos o ponto médio, veremos que esta influência é ainda maior. res %&gt;% summarise(CPUE_medio = mean(Area, na.rm = TRUE), CPUE_mediana = median(Area, na.rm = TRUE), P_medio = sum(range(Area, na.rm = TRUE))/2) CPUE_medio CPUE_mediana P_medio 64.7369 12 209.685 "],["variacao.html", "Capítulo 6 Medidas de variação 6.1 Variância 6.2 Desvio padrão 6.3 Coeficiente de variação 6.4 Amplitude de variação 6.5 Obtendo medidas variação de uma tabela de dados", " Capítulo 6 Medidas de variação As medidas de variação indicam o grau de dispersão das observações. Distribuições com observações muito próximas à média têm baixo grau de dispersão, enquanto aquelas com observações muito distantes da média têm alto grau de dispersão. Vamos apresentar quatro índices que medem o grau de dispersão: a variância, o desvio padrão, o coeficiente de variação e a amplitude de variação. 6.1 Variância A variância descrita pelo símbolo \\(s^2\\) mede quão distante as observações em uma variável estão de sua média aritmética. O \\(s^2\\) se refere à variância amostral. Na seção sobre inferência estatística (Capítulos 13 a 18) faremos distinção com o conceito de variância populacional (\\(\\sigma^2\\)). Para um conjunto de \\(n\\) observações, a variância amostral é dada por: \\[s^2=\\frac{\\sum_{i=1}^n{(X_i - \\overline{X})^2}}{n-1}\\] Seja a variável \\(X\\) com 5 observações: \\(X =\\) {9, 4, 7, 1, 2} Para calcularmos a variância devemos inicialmente, obter a média de \\(X\\), que neste caso é: \\(\\overline{X} = 4.6\\) E subtrair cada observação da média: X \\(X - \\overline{X}\\) 9 4.4 4 -0.6 7 2.4 1 -3.6 2 -2.6 Em seguida, elevamos cada a diferença ao quadrado: X \\(X - \\overline{X}\\) \\({(X - \\overline{X})}^{2}\\) 9 4.4 19.36 4 -0.6 0.36 7 2.4 5.76 1 -3.6 12.96 2 -2.6 6.76 Somamos esta quantia e dividimos por \\(n - 1\\) \\(s^2 = \\frac{19.36 + 0.36 + 5.76 + 12.96 + 6.76}{5 - 1} = \\frac{45.2}{4} = 11.3\\) 6.2 Desvio padrão O desvio padrão (\\(s\\)) é simplesmente a raiz quadrada da variância. \\[s=\\sqrt{\\frac{\\sum_{i=1}^n{(X_i - \\overline{X})^2}}{n-1}}\\] E em nosso exemplo: \\(s = \\sqrt{11.3} = 3.36\\) 6.3 Coeficiente de variação O coeficiente de variação (cv) relaciona o desvio padrão à média, sendo definido por: \\[cv = s/\\overline{X}\\] ou \\[cv_{\\%} = s/\\overline{X}\\cdot 100\\] Em nosso exemplo: \\(cv = \\frac{3.36}{4.6} \\cdot 100 = 73.08\\) 6.4 Amplitude de variação É a diferença entre os pontos máximo e mínimo de um grupo de observações Amplitude de variação = \\(X_{maximo} - X_{minimo}\\) que em nosso exemplo é Amplitude de variação = \\(9 - 1 = 8\\) IMPORTANTE! Nas seções sobre Amostragem e Delineamento e Inferência e Teste de Hipóteses (Capítulos 12 a 18) faremos distinção entre variância e desvio padrão amostral (respectivamente \\(s^{2}\\) e \\(s\\)) e a variância e desvio padrão populacional (respectivamente \\(\\sigma^{2}\\) e \\(\\sigma\\)). 6.5 Obtendo medidas variação de uma tabela de dados Carregue o pacote tidyverse e importe novamente a base de dados Reservatorios_Parana_parcial.csv. # Carrega pacotes library(tidyverse) # Importa base de dados res = read_delim(&#39;Reservatorios_Parana_parcial.csv&#39;, delim = &#39;,&#39;, locale = locale(decimal_mark = &#39;.&#39;, encoding = &#39;latin1&#39;)) Assim como fizemos no capítulo 5 usaremos a função summarise para obter descritores de variação para a variável CPUE. res %&gt;% summarise(CPUE_var = var(CPUE), CPUE_dp = sd(CPUE), CPUE_cv = sd(CPUE)/mean(CPUE) * 100, CPUE_amplutide = max(CPUE) - min(CPUE)) CPUE_var CPUE_dp CPUE_cv CPUE_amplutide 54.31838 7.3701 58.02786 28.71 "],["quartis.html", "Capítulo 7 Medidas de posição: quartis 7.1 Cálculo dos quartis na posição \\(j\\) (\\(Q_j\\), para \\(j = 1\\), \\(2\\) e \\(3\\)) 7.2 Cálculo dos quartis no R 7.3 Obtendo os quartis a partir de uma tabela de dados 7.4 Regresentação gráfica dos quartis: Boxplots", " Capítulo 7 Medidas de posição: quartis A média, mediana, moda e ponto médio são um tipo de medidas de posição que indicam uma posição particular, isto é, a posição central ao redor da qual os dados estão dispersos. Existem no entanto, outras medidas de posição como quartis, medidas comumente utilizadas na descrição, análise e interpretação de dados. Os quartis de uma distribuição de valores são obtidos após ordenarmos os dados em ordem crescente e em seguida agrupá-los em partes iguais, contendo cada uma 25% do número total de observações. Se temos 20 observações, cada parte conterá portanto cinco observações, \\(20 \\times 0.25 = 5\\) e os quartis são as posições que dividem estas partes. Os quartis podem ser indicados por \\(Q_1\\), \\(Q_2\\) e \\(Q_3\\), conforme a figura abaixo. Figure 7.1: Divisão de uma distribuição de valores em quartis \\(Q_{1}\\) O ponto que separa os 25% menores valores do restante da distribuição. \\(Q_{2}\\): O ponto que separa os 50% menores valores dos 50% maiores. Este coincide com a Mediana apresentada anteriormente. \\(Q_{3}\\): O ponto que separa os 25% maiores valores do restante da distribuição. Os quartis que veremos aqui são medidas empíricas dos limites indicados na figura acima. Calculamos estes limites a paritr de uma amostra de tamanho \\(n\\). 7.1 Cálculo dos quartis na posição \\(j\\) (\\(Q_j\\), para \\(j = 1\\), \\(2\\) e \\(3\\)) Existem diferentes algorítmos possíveis para o cálculo dos quartis. Veremos um tipo de algoritmo. Para isto, siga os passos abaixo: Re-organize \\(X\\) em ordem crescente de \\(k = 1\\) a \\(k = n\\). Seja \\(n\\) o número de observações em \\(X\\), teremos portantro \\(X_k\\) como o valor observado na posição \\(k\\) em ordem crescente. Deste modo, para \\(k = 1\\) teremos \\(X_1\\) como o menor valor e para \\(k = n\\) teremos \\(X_n\\) como o maior valor. Calcule \\(L = \\frac{j \\times (n+1)}{4}\\); Defina \\(k\\) como o maior número inteiro abaixo de \\(L\\); Calcule \\(Q_j = X_k + (L - k) \\times (X_{k+1}-X_k)\\); \\(Q_j\\) será um elemento entre \\(X_k\\) e \\(X_{k+1}\\). Se \\(X_k\\) for um número inteiro, \\(Q_j = X_k\\) Exemplo para o cálculo de \\(Q_1\\) Considere a variável \\(X\\) com \\(n =\\) 20 observações. \\(X\\) = 8.7, 10.4, 8.3, 13.2, 10.7, 8.4, 11, 11.5, 11.2, 9.4, 13, 10.8, 8.8, 5.6, 12.2, 9.9, 10, 11.9, 11.6, 11.2 Arrange \\(X\\) em ordem crescente para determinar \\(X\\) nas posições \\(k\\). Posicao k X ordenado 1a Posição 5.6 2a Posição 8.3 3a Posição 8.4 4a Posição 8.7 5a Posição 8.8 6a Posição 9.4 7a Posição 9.9 8a Posição 10.0 9a Posição 10.4 10a Posição 10.7 11a Posição 10.8 12a Posição 11.0 13a Posição 11.2 14a Posição 11.2 15a Posição 11.5 16a Posição 11.6 17a Posição 11.9 18a Posição 12.2 19a Posição 13.0 20a Posição 13.2 Para \\(j = 1\\) (\\(Q_1\\)) calcule: \\(L = \\frac{1 \\times (20+1)}{4} = 5.25\\); Defina \\(k\\) como o maior número inteiro abaixo de \\(L\\). Portanto, se \\(L = 5.25\\), \\(k = 5\\). Do item anterior, note que a observação correspondente à \\(k = 5\\) (5\\(^a\\) posição) é 8.8, enquanto a observação correspondente à \\(k = 5 + 1 = 6\\) (6\\(^a\\) posição) é 9.4. Deste modo, calcule \\(Q_1 = 8.8 + (5.25 - 5) \\times (9.4-8.8) = 8.95\\); Vemos então que para a variável \\(X\\) em questão, o primeiro quartil é: \\(Q_1 = 8.95\\) Exercício: Calcule agora os valores corrspondentes a \\(Q_2\\) e \\(Q_3\\) e verifique que os resultados são: \\(Q_2 = 10.75\\) e \\(Q_3 = 11.575\\) 7.2 Cálculo dos quartis no R Podemos programar a sequência de funções acima utilizando o R: X = c(8.7, 10.4, 8.3, 13.2, 10.7, 8.4, 11, 11.5, 11.2, 9.4, 13, 10.8, 8.8, 5.6, 12.2, 9.9, 10, 11.9, 11.6, 11.2) # Ordenando X em ordem crescente sX = sort(X, decreasing = FALSE) # Encontrando o número de observações em X n = length(X) # Encontrando os quartis (Q1, Q2 e Q3) j = c(1, 2, 3) L = j * (n + 1)/4 k = floor(L) Q = sX[k] + (L - k) * (sX[k+1] - sX[k]) names(Q) = c(&#39;Q1&#39;, &#39;Q2&#39;, &#39;Q3&#39;) # Vizualizando os quartis Q ## Q1 Q2 Q3 ## 8.950 10.750 11.575 No entando, existe uma função no R denominada quantile que pode ser utilizada da seguinte forma: quantile(X, probs = c(0.25, 0.50, 0.75)) ## 25% 50% 75% ## 9.250 10.750 11.525 Observações Lembre que o quartil \\(Q_1\\) delimita a posição \\(25\\%\\), \\(Q_2\\) delimita a posição \\(50\\%\\) (\\(=\\)mediana) e \\(Q_3\\) delimita as posição \\(75\\%\\). Por este motivo utilizamos o argumento probs = c(0.25, 0.50, 0.75). Deste modo, a função quantile é mais geral que a rotina passada anteriormente, uma vez que permite o cálculo para qualquer posição entre os quantis \\(0\\%\\) e \\(100\\%\\). Note também que os resultados foram ligeiramente diferentes uma vez que existem diferentes algoritmos para o cálculo dos quartis. A função quantile permite a escolha entre \\(9\\) algoritmos diferentes e por padrão, utiliza o type = 7. O passo-a-passo que mostramos nesta apostila corresponde ao type = 6. Você pode verificar que se digitar o comando abaixo, os resultados serão os mesmos que calculamos manualmente. quantile(X, probs = c(0.25, 0.50, 0.75), type = 6) ## 25% 50% 75% ## 8.950 10.750 11.575 Ainda que cada algoritmo possa resultar em pequenas diferenças, estas diferenças diminuem à medida que o tamanho amostral aumenta. Finalmente, os quartis discutidos aqui são casos particulares de limites mais gerais denominados de quantis que indicam uma deteminama posição na distribuição. Como vimos, o limite \\(Q_1\\) por exemplo, denominado de Quartil 1 delimita o trecho que separa \\(25\\%\\). Poderíamos denominar este limite de Quantil \\(0,25\\). Pensando desta maneira, poderíamos encontrar qualquer posição. Por exemplo o quantil \\(0,10\\), que delimita os \\(10\\%\\) dos menores valores, o quantil \\(0,025\\) que delimita os \\(2,5\\%\\) menores valores na distribuição, e assim, por diante. Veremos ao longo desta apostila que além dos limites de \\(Q_1\\), \\(Q_2\\) e \\(Q_3\\), outros limites importantes são aqueles que definem as posições \\(2,5\\%\\), \\(5\\%\\), \\(10\\%\\), \\(90\\%\\), \\(95\\%\\) e \\(97,5\\%\\). Todos estes limites aparecerão de forma recorrente em estatística inferencial e probabilidade. No calculo dos quantis para um limite \\(p\\) qualquer (\\(0 \\le p \\le 1\\)) a única mundança no algoritmno que apresentamos neste capítulo está na obtenção de \\(L\\) (passo 2), que é feita como: \\[L = p \\times (n+1)\\] 7.3 Obtendo os quartis a partir de uma tabela de dados Carregue o pacote tidyverse e importe novamente a base de dados Reservatorios_Parana_parcial.csv. # Carrega pacotes library(tidyverse) # Importa base de dados res = read_delim(&#39;Reservatorios_Parana_parcial.csv&#39;, delim = &#39;,&#39;, locale = locale(decimal_mark = &#39;.&#39;, encoding = &#39;latin1&#39;)) Assim como fizemos nos capítulos anteriores usaremos a função summarise para obter os quartis para a variável CPUE. res %&gt;% summarise(Quartis = quantile(res$CPUE, probs = c(0.25, 0.5, 0.75))) %&gt;% mutate(Limites = c(&#39;25%&#39;, &#39;50%&#39;, &#39;75%&#39;)) Quartis Limites 7.43 25% 11.74 50% 16.30 75% 7.4 Regresentação gráfica dos quartis: Boxplots Os quartis de uma distribuição no ajudam a entender o formato de uma distribuição. Uma das formas amplamemte estabelecidas de representarmos graficamente os quartis são por meio de um gráfico denominado de Boxplot. Para a variável acima, o boxplot será: Figure 7.2: Divisão em quartis de um boxplot Em um boxplot, a linha central mais expessa representa a Mediana ou \\(2^o\\) quartil (\\(Q_2\\)), os limites da caixa são o \\(1^o\\) e \\(3^o\\) quartis, respectivamente \\(Q_1\\) e \\(Q_3\\). As extremidades geralmente são os pontos máximo e mínimo da dsitribuição. Existe uma relação entre os histogramas e os boxplots. Ambos podem ser utilizados para avaliarmos o grau de assimetria de uma distribuição como apresentado abaixo. Em uma distribuição simétrica, a caixa do boxplot tende a se concentrar no meio da distribuição, enquanto em distribuições assimétricas, a caixa tende a ficar deslocada à esquerda ou à direita. Figure 7.3: Relação entre as representações por meio de histogramas e boxplots "],["escorez.html", "Capítulo 8 Medidas de posição: índice Z 8.1 Interpretando o valor de \\(Z\\) 8.2 Realizando a transformação \\(Z\\) a partir de uma tabela de dados 8.3 Valores esperados de \\(Z\\) em uma distribuição normal padronizada", " Capítulo 8 Medidas de posição: índice Z O Índice \\(Z\\) ou Escore \\(Z\\) indica a posição de uma observação particular (\\(X_i\\)). O valor de \\(Z\\) relaciona a posição de \\(X_i\\) com a média e o desvio padrão da distribuição. Suponha uma variável com média \\(\\overline{X}\\) e desvio padrão \\(s\\). O índice de \\(Z_i\\) para uma observação \\(i\\) particular é calculado por: \\[Z_i = \\frac{X_i - \\overline{X}}{s}\\] Seja por exemplo a variável \\(X\\) \\(X\\) = 8.7, 10.4, 8.3, 13.2, 10.7, 8.4, 11, 11.5, 11.2, 9.4, 13, 10.8, 8.8, 5.6, 12.2, 9.9, 10, 11.9, 11.6, 11.2 A média e o desvio padrão são respectivamente \\(\\overline{X} = 10.39\\) e \\(s = 1.82\\). O índice \\(Z_i\\) para a observação \\(X_i = 8.3\\) \\(Z_i = \\frac{8.3 - 10.39}{1.82} = -1.15\\) 8.1 Interpretando o valor de \\(Z\\) Ao calcularmos o valor de \\(Z\\) estamos primeiro fazendo uma centralização da variável \\(X\\) quando subtraímos cada observação \\(X_i\\) de \\(\\overline{X}\\). A porção \\(X_i - \\overline{X}\\) mede os desvios de cada observação, isto é, suas distâncias (positivas ou negativas) entre \\(X_i\\) e \\(\\overline{X}\\). Se calcularmos a média destes desvios por exemplo, veremos que o resultado será exatamente zero: \\(\\sum_{i=1}^{n}\\frac{(X_i - \\overline{X})}{n} = 0\\) Portanto, acabamos de centralizar a variável \\(X\\) ao redor de sua média. Em seguida dividimos a quantia \\(X_i - \\overline{X}\\) pelo desvio padrão de \\(X\\) e, ao fazermos, isto estamos padronizando a nova variável que denominaremos de \\(Z\\). Se calcularmos o desvio padrão desta nova variável veremos que será exatamente igual a \\(1\\). Dizemos que o Índice \\(Z\\) consiste de uma transformação que faz com que a nova variável tenha média = \\(0\\) e desvio padrão = \\(1\\). Um valor de \\(Z_i\\) particular associado a uma observação \\(X_i\\) nos indica quantos desvios padrões \\(X_i\\) está acima ou abaixo da média de seu grupo. A relação entre a nova variável \\(Z\\) e a variável original \\(X\\) é: Se \\(Z_i = 0\\), então \\(X_i = \\overline{X}\\); Se \\(Z_i &gt; 0\\), então \\(X_i &gt; \\overline{X}\\); Se \\(Z_i &lt; 0\\), então \\(X_i &lt; \\overline{X}\\); Para uma distribuição com média igual \\(10\\) e desvio padrão igual a \\(3\\) por exemplo, uma observação \\(X_i = 16\\) terá um valor de \\(Z = \\frac{16-10}{3} = 2\\), indicando que está dois desvios padrões acima da média de \\(X\\). Vamos calcular os valores de \\(Z\\) para a variável \\(X\\) do exemplo acima e observar os resultados. Posicao k X ordenado Z ordenado 1a Posição 5.60 -2.63 2a Posição 8.30 -1.15 3a Posição 8.40 -1.09 4a Posição 8.70 -0.93 5a Posição 8.80 -0.87 6a Posição 9.40 -0.54 7a Posição 9.90 -0.27 8a Posição 10.00 -0.21 9a Posição 10.40 0.01 10a Posição 10.70 0.17 11a Posição 10.80 0.23 12a Posição 11.00 0.34 13a Posição 11.20 0.45 14a Posição 11.20 0.45 15a Posição 11.50 0.61 16a Posição 11.60 0.66 17a Posição 11.90 0.83 18a Posição 12.20 0.99 19a Posição 13.00 1.43 20a Posição 13.20 1.54 Média 10.39 0.00 Desvio padrão 1.82 1.00 Veja na tabela que conforme o valor de \\(X_i\\) se distancia da média de \\(X = 10.39\\), mais distante de zero será o valor de \\(Z_i\\). Neste exemplo, as observações mais extremas de \\(X\\) estão, respectivamente, a -2.63 desvios padrões abaixo e 1.54 desvios padrões acima da média. Como discutimos acima, a nova variável \\(Z\\) tem média \\(\\overline{Z} = 0\\) (está centralizada) e desvio padrão \\(1\\) (está padronizada). 8.2 Realizando a transformação \\(Z\\) a partir de uma tabela de dados Carregue o pacote tidyverse e importe novamente a base de dados Reservatorios_Parana_parcial.csv. # Carrega pacotes library(tidyverse) # Importa base de dados res = read_delim(&#39;Reservatorios_Parana_parcial.csv&#39;, delim = &#39;,&#39;, locale = locale(decimal_mark = &#39;.&#39;, encoding = &#39;latin1&#39;)) Vamos manter somente a variável CPUE e criar outra coluna denominada CPUE_z utilizando a função mutate. df_z = res %&gt;% select(CPUE) %&gt;% mutate(CPUE_z = (CPUE - mean(CPUE))/sd(CPUE)) df_z CPUE CPUE_z 9.22 -0.4723094 28.73 2.1748731 11.59 -0.1507398 30.76 2.4503103 5.95 -0.9159940 7.75 -0.6717640 7.51 -0.7043280 4.01 -1.1792198 20.83 1.1029745 2.43 -1.3935995 12.55 -0.0204838 11.73 -0.1317442 13.72 0.1382657 16.50 0.5154655 4.71 -1.0842414 7.95 -0.6446273 13.12 0.0568557 16.10 0.4611921 11.74 -0.1303873 17.95 0.7122064 13.86 0.1572614 13.04 0.0460010 7.35 -0.7260373 20.92 1.1151860 13.67 0.1314816 21.82 1.2373010 6.29 -0.8698617 9.40 -0.4478864 5.60 -0.9634832 2.05 -1.4451592 24.88 1.6524921 Se calcularmos a média e desvio padrão das variáveis verermos que CPUE mantém os valores originais, enquanto CPUE_z terá média = \\(0\\) e desvio padrão = \\(1\\). df_z %&gt;% summarize(CPUE_media = mean(CPUE), CPUE_dp = sd(CPUE), CPUE_z_media = round(mean(CPUE_z),2), CPUE_z_dp = round(sd(CPUE_z),2)) CPUE_media CPUE_dp CPUE_z_media CPUE_z_dp 12.70097 7.3701 0 1 8.3 Valores esperados de \\(Z\\) em uma distribuição normal padronizada A interpretação de \\(Z\\) faz sentido quando desejamos posicionar uma determinada observação \\(X_i\\) como função da média e desvio padrão de seu grupo. Veremos nos capítulos 14 e 15 que se uma variável \\(X\\) puder ser descrita adequadamente por uma Distribuição Normal de probabilidades, uma regra empírica nos permite determinar qual o percentual das observações está acima e abaixo de alguns limites conhecidos. Na figura abaixo vemos estes limites para uma distribuição normal teórica. Suponha que tomemos uma observação ao acaso desta distribuição. Existe uma probabilidade de aproximadamente \\(68\\%\\) de que esta observação esteja entre os limites de \\(-1\\) e \\(+1\\) desvios padrões da média. Ou ainda, existe uma probabilidade de aproximadamente \\(95\\%\\) de que esta observação esteja entre \\(-2\\) e \\(+2\\) desvio padrões da média. Por outro lado, que é muito improvável amostrarmos um valor a mais de \\(3\\) desvios padrões distantes da média. Isto deverá ocorrer em somente de cerca de \\(0,2\\%\\) dos casos em que sortearmos uma amostra aleatoriamente. Este assunto será abordado em mais detalhes na seção sobre inferência estatística (Capítulos 12 a 18). Por hora, suponha que a distribuição de altura de homens adultos siga uma distribuição normal com média \\(\\mu = 175\\) cm e desvio padrão de \\(\\sigma = 175\\) cm. Neste caso, se tomarmos os limites entre \\(-2\\) e \\(+2\\) desvios padrões teremos: \\(\\mu - 2 \\times \\sigma = 175 - 2 \\times 10 = 155\\) cm e \\(\\mu + 2 \\times \\sigma = 175 + 2 \\times 10 = 195\\) cm Sugerindo que somente cerca de \\(5\\%\\) dos homens adultos teriam mais de \\(195\\) cm ou menos de \\(155\\) cm de altura. Os processos centralização e padronização de uma variável são úteis em diferentes momentos da estatística descritiva e inferencial. Veremos este processo aparecendo novamente quando formos medir a associação entre variáveis quantitativas por meio do coeficiente de correlação de Pearson (Capítulo 10) e também quando formos falar sobre associação entre variáveis quantitativas e qualitativas no capítulo 11. "],["biquali.html", "Capítulo 9 Análise bidimensional: variáveis qualitativas 9.1 Tabelas de contingência 9.2 O gráfico de barras para duas variáveis qualitativas 9.3 Medindo a discrepância com o índice de \\(\\chi^2\\) de Pearson 9.4 O índice de \\(\\chi^2\\) em uma tabela de contigência 9.5 Valores de \\(\\chi^2\\) quando existe associação 9.6 Variações do índice de \\(\\chi^2\\) 9.7 Obtendo o índice de \\(\\chi^2\\) de uma tabela de dados", " Capítulo 9 Análise bidimensional: variáveis qualitativas Até agora exploramos formas de representar visualmente uma única variável por vez (Capítulos 3 e 4) e discutimos medidas para descrever algumas propriedades desta variável (Capítulo 5 a 8). A partir de agora veremos como representar e resumir padrões de associação entre pares de variáveis. Segundo a natureza dos dados envolvidos, estas associações podem ser entre: duas variáveis qualitativas (este capítulo); duas variáveis quantitativas (Capítulo 10) e; uma variável qualitativa e uma quantitativa (Capítulo 11) Exemplo Imagine que haverá uma obra de revitalização de uma área na região central da cidade. A obra implicará na melhoria de acesso e segurança, ferta de serviços, mas levará tempo para ser concluída e precisará passar por ações de remoção de moradias irregulares, interdição de ruas e avenidas por longos períodos, etc. A prefeitura encomenda uma pesquisa para saber a opinião dos munícipes. A cada entrevistado são feitas duas perguntas: Qual sua opinião sobre a necessidade de realização da obra? A favor Contra Você reside na região diretamente afetada: Residente Não-Residente A base de dados está disponível em: Entrevista_municipes.csv Importe a tabela e veja os primeiros 12 resultados das entrevistas: # Carrega pacotes library(tidyverse) # Importa base de dados mun = read_delim(&#39;Entrevista_municipes.csv&#39;, delim = &#39;,&#39;) n = nrow(mun) # Número de entrevistados Entrevistado Opiniao Moradia 1 A favor Residente 2 A favor Residente 3 A favor Residente 4 A favor Residente 5 Contra Não-Residente 6 Contra Residente 7 A favor Não-Residente 8 Contra Não-Residente 9 A favor Não-Residente 10 A favor Não-Residente 11 A favor Residente 12 A favor Não-Residente Após entrevistar \\(200\\) pessoas selecionadas ao acaso de uma lista da moradores da cidade, é construída uma tabela com três colunas: Entrevistado (sequência numérica do primeiro ao último respondente), Opinião e Moradia. Estão descritos acima os resultados das primeiras 12 entrevistas, onde é possível ver ao menos uma combinação de todas as possíveis respostas. O entrevistado pode ser: i) A favor e ser Residente da região; ii) A favor e ser Não-Residente, iii) Contra e ser Residente; iv) Contra e ser Não-Residente. Vamos inicialmente representar cada uma das variáveis individualmente por meio de uma tabela de frequência dos 200 entrevistados. resumo_opiniao = mun %&gt;% group_by(Opiniao) %&gt;% summarise(Op_n = n()) %&gt;% mutate(Op_rel = Op_n/sum(Op_n)) resumo_opiniao Opiniao Op_n Op_rel A favor 144 0.72 Contra 56 0.28 Das \\(200\\) respostas tivemos \\(144\\) pessoas A favor (\\(72\\%\\)) e \\(56\\) pessoas Contra (\\(28\\%\\)). Com relação ao local de residência: resumo_morad = mun %&gt;% group_by(Moradia) %&gt;% summarise(Morad_n = n()) %&gt;% mutate(Morad_rel = Morad_n/sum(Morad_n)) resumo_morad Moradia Morad_n Morad_rel Não-Residente 117 0.585 Residente 83 0.415 Temos um total de \\(117\\) pessoas Não-Residente (\\(58.5\\%\\)) e \\(83\\) pessoas Residente (\\(41.5\\%\\)) Se visualizarmos estes totais em gráficos de barras individuais teremos: plt_op = ggplot(mun, aes(x = Opiniao)) + geom_bar(fill = &#39;darkblue&#39;, color = &#39;white&#39;) + coord_cartesian(ylim = c(0, 150)) + labs(y = &#39;Número de respostas&#39;) + theme_classic() plt_morad = ggplot(mun, aes(x = Moradia)) + geom_bar(fill = &#39;darkred&#39;, color = &#39;white&#39;) + coord_cartesian(ylim = c(0, 150)) + labs(y = &#39;Número de respostas&#39;) + theme_classic() # Obs: para que a linha abaixo funcione, é necessário habilitar o pacote &#39;patchwork&#39;. # Caso não tenha, instale e carregue com os comandos: # install.packages(&#39;patchwork&#39;) # library(patchwork) plt_op + plt_morad Existe portanto um predomínio de pessoas A Favor e um ligeiro predomínio de entrevistados Não-Residentes. Estamos interessados em entender se existe alguma associação entre as respostas dadas às duas perguntas, uma questão que pode ser colocada da seguinte forma: Será que moradores Residentes tendem ter uma opinião consistentemente diferente de moradores Não-residentes no que se refere a ser A favor ou Contra a obra? Para começar a entender esta questão vamos explorar a distribuição das variáveis conjuntamente. 9.1 Tabelas de contingência Tabelas de contigência são organizadas para verificarmos a associação entre duas variáveis qualitativas. São conhecidas também como tabelas de dupla entrada. Nas colunas estão os níveis da variável \\(X\\) e nas linhas os níveis da variável \\(Y\\). Para nosso exemplo, podemos fazer simplesmente: tcont = table(mun$Opiniao, mun$Moradia) tcont ## ## Não-Residente Residente ## A favor 81 63 ## Contra 36 20 Temos portanto: 81 - A favor e Não-Residente; 63 - A favor e Residente; 36 - Contra e Não-Residente; 20 - Contra e Residente Podemos ver os totais marginais das linhas: tcont_linhas = apply(tcont, 1, sum) # Totais das linhas tcont_linhas ## A favor Contra ## 144 56 Ou os totais marginais das colunas: tcont_colunas = apply(tcont, 2, sum) # Totais das colunas tcont_colunas ## Não-Residente Residente ## 117 83 Que são justamente os totais que verificamos nas distribuições individuais. Se quisermos ver as frequências relativas marginais podemos fazer: trel_linha = prop.table(tcont, 1) # Frequencia relativa marginal das linhas trel_linha ## ## Não-Residente Residente ## A favor 0.5625000 0.4375000 ## Contra 0.6428571 0.3571429 Neste caso, estamos vendo as frequências relativas das linhas, isto é, cada linha nesta tabela soma \\(1\\). O que vemos nesta tabela é: dos \\(144\\) entrevistados que são A favor, cerca de \\(56.25\\%\\) são Não-Residente, enquanto os demais \\(43.75\\%\\) são Residente dos \\(56\\) entrevistados que são Contra, cerca de \\(64.29\\%\\) são Não-Residente, enquanto os demais \\(35.71\\%\\) são Residente Podemos fazer exatamente a mesma coisa olhando agora para as frequências marginais por colunas: trel_coluna = prop.table(tcont, 2) # Frequencia relativa marginal das colunas trel_coluna ## ## Não-Residente Residente ## A favor 0.6923077 0.7590361 ## Contra 0.3076923 0.2409639 Neste caso são as colunas que somam \\(1\\), portanto: dos \\(117\\) entrevistados que são Não-Residente, cerca de \\(69.23\\%\\) são A favor, enquanto os demais \\(30.77\\%\\) são Contra dos \\(83\\) entrevistados que são Residente, cerca de \\(75.9\\%\\) são A favor, enquanto os demais \\(75.9\\%\\) são Contra Podemos finalmente ver a frequência relativa conjunta: trel_conjunta = prop.table(tcont) # Frequencia relativa conjunta trel_conjunta ## ## Não-Residente Residente ## A favor 0.405 0.315 ## Contra 0.180 0.100 Neste caso, o somatório das linhas é igual a: tcont_linhas / sum(tcont_linhas) ## A favor Contra ## 0.72 0.28 E indica os valores relativos das opiniões A Favor e Contra. O somatório das colunas é igual a: tcont_colunas / sum(tcont_colunas) ## Não-Residente Residente ## 0.585 0.415 E indica os valores relativos de Não-Residentes e Residentes. Na tabela de frequência relativa conjunta, o somatório total da tabela deve ser igual a \\(1\\). 9.2 O gráfico de barras para duas variáveis qualitativas Existem várias formas de gerar um gráfico de barras combinando as duas variáveis. Se quisermos utilizar a própria tabela de contingência obtida a partir do comando table(mun$Opiniao, mun$Moradia), podemos utilizar o comando barplot(). Por outro lado, se quisermos utilizar a tabela original de dados (objeto mun) podemos fazer uso do pacote ggplot2: plt_bar1 = ggplot(mun) + aes(x = Moradia, fill = Opiniao) + geom_bar(color = &#39;white&#39;, position = &#39;dodge&#39;) + scale_fill_manual(values = c(&#39;Contra&#39; = &#39;darkred&#39;, &#39;A favor&#39; = &#39;darkblue&#39;)) + coord_cartesian(ylim = c(0, 80)) + labs(y = &#39;Número de respostas&#39;) + theme_classic() plt_bar1 Veja que nesta figura, existem mais opiniões A favor independente do entrevistado ser ou não residente. Este padrão é o mesmo que observamos no gráfico da variável Opinião isoladamente. Isto sugere que não existe associação entre as variáveis Opinião e Moradia. Isto é, a opinião de um entrevistado sobre a construção da obra não depende do seu local de moradia. Exemplos de associações entre duas variáveis Abaixo são apresentadas situações em que existe uma associação. Em todos estes exemplos, note que a relação entre as opiniões A favor ou Contra depende se o entrevistado é ou não Residente na região. Todos estes padrões configuram uma associação entre as variáveis Opinião e Moradia. Figura A: Não-Residentes tendem a ser A favor e Residentes são em sua maioria Contra; Figura B: Todos tendem a ser Contra, mas a diferença de opiniões é maior entre os Residentes; Figura C: Não-Residentes tendem a ser Contra, enquanto não parece haver diferenças entre os Residentes; Figura D: Residentes tendem a ser A favor, enquanto não parece haver diferenças entre os Residentes; 9.3 Medindo a discrepância com o índice de \\(\\chi^2\\) de Pearson O índice de qui-quadrado (\\(\\chi^2\\)) mede a discrepância entre os valores observados e os valores esperados em uma tabela de contingência. Digamos que um município tenha \\(20\\%\\) de sua população morando em área Rural e os outros \\(80\\%\\) em área Urbana. Se fizermos uma amostragem ao acaso dos moradores é esperado que esta frequência relativa se reflita na amostra. Neste caso se sorteamos \\(200\\) pessoas, seria esperado: Zona Rural: \\(40\\) moradores Zona Urbana \\(160\\) moradores Moradia Freq Rural 40 Urbana 160 Entretando, se fazemos um sorteio ao acaso, haverá alguma variação ao redor destes valores. O \\(\\chi^2\\) mede esta discrepância entre as frequências observadas (\\(o\\)) e esperadas (\\(e\\)) para cada célula da tabela: \\[\\chi^2 = \\sum_{i=1}^{n}\\frac{(o_i - e_i)^2}{e_i}\\] Para uma tabela de frequências, devemos determinar portanto os valores de \\(o_i\\) e \\(e_i\\). Suponha que uma amostra de \\(200\\) moradores tenha resultado em: Moradia_obs Freq Rural 31 Urbana 169 As frequências observadas e esperadas serão: Zona Rural: \\(o_{Rural} = 31\\) \\(e_{Rural} = 0.2 \\times 200 = 40\\) Zona Urbana \\(o_{Urbana} = 169\\) \\(e_{Urbana} = 0.8 \\times 200 = 160\\) De modo que o valor de \\(\\chi^2\\) será: \\(\\chi^2 = \\frac{(31 - 40)^2}{40} + \\frac{(169 - 160)^2}{160} = \\frac{(-9)^2}{40} + \\frac{(9)^2}{160} = 2.025 + 0.50625 = 2.53125\\) 9.4 O índice de \\(\\chi^2\\) em uma tabela de contigência No exemplo acima, as contagens esperadas foram definidas a partir de um modelo que dizia que as populações rurais e urbanas se dividiam nas proporções \\(20\\%:80\\%\\). Em uma tabela de contigência, a hipótese em verificação é a de que não há associação entre \\(X\\) e \\(Y\\). Se for assim, é esperado que as frequências conjuntas sejam porporcionais às frequências marginais*. Vamos apresentar esta ideia utilizando uma notação geral para tabelas de contingência e em seguida discutir com um exemplo. A tabela abaixo apresenta \\(r\\) linhas por \\(s\\) colunas com as contagens de todas as combinações dos níveis da variável \\(X\\) (Níveis \\(A_{1}\\) a \\(A_{r}\\)) e da variável \\(Y\\) (Níveis \\(B_{1}\\) a \\(B_{s}\\)). Os totais marginais de \\(X\\) e \\(Y\\) são expressos respectivamente na última coluna e na última linha. X ⟍ Y \\(B_{1}\\) \\(B_{2}\\) \\(\\cdots\\) \\(B_{j}\\) \\(\\cdots\\) \\(B_{s}\\) Totais em \\(X\\) \\(A_{1}\\) \\(n_{11}\\) \\(n_{12}\\) \\(\\cdots\\) \\(n_{1j}\\) \\(\\cdots\\) \\(n_{1s}\\) \\(n_{1.}\\) \\(A_{2}\\) \\(n_{21}\\) \\(n_{22}\\) \\(\\cdots\\) \\(n_{2j}\\) \\(\\cdots\\) \\(n_{2s}\\) \\(n_{2.}\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(A_{i}\\) \\(n_{i1}\\) \\(n_{i2}\\) \\(\\cdots\\) \\(n_{ij}\\) \\(\\cdots\\) \\(n_{is}\\) \\(n_{i.}\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\cdots\\) \\(\\cdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(A_{r}\\) \\(n_{r1}\\) \\(n_{r2}\\) \\(\\cdots\\) \\(n_{rj}\\) \\(\\cdots\\) \\(n_{rs}\\) \\(n_{r.}\\) Totais em \\(Y\\) \\(n_{.1}\\) \\(n_{.2}\\) \\(\\cdots\\) \\(n_{.j}\\) \\(\\cdots\\) \\(n_{rs}\\) \\(n\\) Sob a hipótese de não associação entre \\(X\\) e \\(Y\\) teremos que: \\(\\frac{n_{i1}}{n_{.1}} = \\frac{n_{i2}}{n_{.2}} = \\cdots = \\frac{n_{is}}{n_{.s}} = \\frac{n_{i.}}{n}\\) e assim: \\(\\frac{n_{ij}}{n_{.j}} = \\frac{n_{i.}}{n}\\) Deste modo: \\(n_{ij}^{e} = \\frac{n_{i.} \\times n_{.j}}{n}\\) A notação \\(n_{ij}^{e}\\) está sendo utilizada para denotar que a expressão acima determina a contagem de cada célula da tabela sob a hipótese de não associação e portanto, se refere ao valor esperado de \\(n_{ij}\\). Tendo definido os valores esperados em uma tabela de contingência de \\(r \\times s\\), o \\(\\chi^2\\) é dado por: \\[\\chi^2 = \\sum_{i=1}^{r}\\sum_{j=1}^{s}\\frac{(n_{ij} - n_{ij}^{e})^2}{n_{ij}^{e}}\\] Retornando ao exemplo das entrevistas A tabela de contingência contendo os dados observados do início do capítulo pode ser escrita como: Não-Residente Residente Total Opinião A favor 81 63 144 Contra 36 20 56 Total Moradia 117 83 200 Os Valores esperados na linha \\(i\\) e coluna \\(j\\) são: Linha \\(1\\) - Coluna \\(1\\) (Não-Residente - A favor): \\(n_{ii}^{e} = \\frac{n_{1.} \\times n_{.1}}{n} = \\frac{144 \\times 117}{200} = 84.24\\) Linha \\(1\\) - Coluna \\(2\\) (Residente - A favor): \\(n_{ii}^{e} = \\frac{n_{1.} \\times n_{.2}}{n} = \\frac{144 \\times 83}{200} = 59.76\\) Linha \\(2\\) - Coluna \\(1\\) (Não-Residente - Contra): \\(n_{ii}^{e} = \\frac{n_{2.} \\times n_{.1}}{n} = \\frac{56 \\times 117}{200} = 32.76\\) Linha \\(2\\) - Coluna \\(2\\) (Residente - Contra): \\(n_{ii}^{e} = \\frac{n_{2.} \\times n_{.2}}{n} = \\frac{56 \\times 83}{200} = 23.24\\) De modo que a tabela com os valores esperados será: Não-Residente Residente Total Opinião A favor 84.24 59.76 144 Contra 32.76 23.24 56 Total Moradia 117 83 200 Finalmente, o valor \\(\\chi^2\\) pode ser obtido por: \\(\\chi^2 = \\frac{(81 - 84.24)^2}{84.24} + \\frac{(36 - 32.76)^2}{84.24} + \\frac{(63 - 59.76)^2}{84.24} + \\frac{(20 - 23.24)^2}{84.24} = 1.072\\) 9.5 Valores de \\(\\chi^2\\) quando existe associação O valor de \\(\\chi^2\\) será zero somente se os valores observados forem exatamente igauis aos valores esperados. Pequenas discrepâncias irão gerar valores de \\(\\chi^2\\) acima de zero. Os valores se tornarão mais altos à medida que aumentam as diferenças entre \\(n_{ij}\\) e \\(n_{ij}^e\\). Abaixo estão diferentes exemplos em que existe associação entre Opinião e Moradia. Compare os valores e os gráficos abaixo os que fizemos no exemplo do capítulo e veja que todos os valores de \\(\\chi^2\\) são mais elevados. Tente aplicar a fórmula do \\(\\chi^2\\) para chegar aos resultados apresentados em cada exemplo. 9.6 Variações do índice de \\(\\chi^2\\) O valor de \\(\\chi^2\\) aumenta com o tamanho da amostra, o que torna difícil comparações entre diferentes estudos. Para corrigir este efeito existe o coeficiente de contigência de Pearson (\\(C\\)) que é baseado no resultado de \\(\\chi^2\\) \\[C = \\sqrt{\\frac{\\chi^2}{\\chi^2 + n}}\\] em que \\(n\\) é o tamanho da amostra. O valor máximo de \\(C\\) depende do número de linhas (\\(r\\)) e de colunas (\\(s\\)) na tabela de contingêcia, deste modo podemos ainda definir um outro coeficiente que varia entre \\(0\\) e \\(1\\): \\[T = \\sqrt{\\frac{\\frac{\\chi^2}{n}}{(r-1) \\times (s-1)}}\\] O valor \\(T = 0\\) ocorre quando não há associação (\\(\\chi^2 = 0\\)) e o valor máximo de \\(T = 1\\) só será atingido quando houver associação e \\(r = s\\) Todos estes índices aumentam conforme aumenta o grau de associação entre as variáveis. Entretanto, não dissemos em que ponto passamos a aceitar a hipótese de associação as variáveis \\(X\\) e \\(Y\\). Iremos tratar deste assunto na seção sobre estatística inferencial. Ao tratarmos deste ponto é importante termos claro o que siginifica uma associação (ou dependêcia) entre duas variáveis qualitativas e como o índice de \\(\\chi^2\\) mede esta associação. 9.7 Obtendo o índice de \\(\\chi^2\\) de uma tabela de dados A função para o cálculo do \\(\\chi^2\\) no R é chisq.test e pode ser utilizada a partir da tabela de contigência gerada pela função table: tcont = table(mun$Opiniao, mun$Moradia) chisq.test(tcont) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: tcont ## X-squared = 0.76697, df = 1, p-value = 0.3812 O resultado mostra o o valor de \\(\\chi^2\\) calculado (X-squared) e outras duas quantias denominadas de graus de liberdade (df) e valor de p (p-value), sobre as quais falaremos na seção sobre Inferência estatística (Capítulos 12 a 18). Note que o resultado é diferente do que obtivemos neste capítulo. Isto ocorre pois, por padrão, a função utiliza a correção de Yates (veja aqui), em que \\(\\chi_{Yates}^{2}\\) é calculado por: \\[\\chi_{Yates}^{2} = \\sum_{i=1}^{r}\\sum_{j=1}^{s}\\frac{(|n_{ij} - n_{ij}^{e}| - 0,5)^2}{n_{ij}^{e}}\\] O termo \\(|n_{ij} - n_{ij}^{e}|\\) se refere ao módulo da distância entre os valores observados e calculados. Esta correção será abordada ao falarmos de testes de hipóteses para variáveis categóricas. Por hora, se quisermos obter exatamente os resultados descritos no exemplo deste capítulo, basta fazermos: chisq.test(tcont, correct = FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: tcont ## X-squared = 1.0724, df = 1, p-value = 0.3004 "],["biquant.html", "Capítulo 10 Análise bidimensional: variáveis quantitativas 10.1 Covariância entre \\(Y\\) e \\(X\\) 10.2 Coeficiente de correlação linear de Pearson \\(r\\) 10.3 Exemplo", " Capítulo 10 Análise bidimensional: variáveis quantitativas Neste capítulo iremos medir o grau de associação entre duas variáveis quantitativas denominadas de \\(X\\) e \\(Y\\). Para isto falaremos dos conceitos de covariância e correlação. Não estamos interessados em verificar se \\(Y\\) depende funcionalmente de \\(X\\) ou vice-versa. Estamos interessados somente em medir a intensidade de associação entre as duas variáveis. Quando calcularmos a covariância entre \\(Y\\) e \\(X\\) (\\(s_{YX}\\)), por exemplo, poderíamos inverter a ordem fazendo \\(s_{XY}\\) e teríamos exatamente os mesmo resultados. O mesmo vale para o coeficiente de correlação (\\(r_{YX} = r_{XY}\\)). Dizemos que existe uma simetria entre as variáveis quando calculamos a covariância ou correlação. Esta simetria não vale quando formos abordar o conceito de regressão linear (Capítulo 22), quando assumimos explicitamente que \\(Y\\) é função de \\(X\\). Estamos interessados em diferenciar três situações que podem ser visualizadas nos gráficos de dispersão abaixo: 10.1 Covariância entre \\(Y\\) e \\(X\\) Vamos retomar o conceito de variância amostral para em seguida introduzir o conceito de covariância amostral. A variância amostral de \\(Y\\) por exemplo, pode ser obtida subtraindo cada observação em \\(Y\\) de sua média (\\(\\overline{Y}\\)) e elevando esta subtração ao quadrado \\((Y_i - \\overline{Y})^2\\). Ao somar para todos os valores de \\(Y_i\\) teremos o somatório dos quadrados de \\(Y\\) (\\(SQ_Y\\)). \\[SQ_Y = \\sum_{i-1}^{n} (Y_i - \\overline{Y})^2 = \\sum_{i-1}^{n}(Y_i - \\overline{Y}) (Y_i - \\overline{Y})\\] Dividindo \\(SQ_Y\\) por \\(n-1\\) teremos a variância amostral de \\(Y\\) (\\(s^2_Y\\)). \\[s^2_Y = \\frac{\\sum_{i-1}^{n} (Y_i - \\overline{Y})^2}{n-1}\\] No capítulo 6 denominamos esta quantia simplesmente por \\(s^2\\). Aqui vamos usar uma notação diferente (\\(s^2_Y\\)), pois haverá outros estimadores de variância envolvidos, de modo que deveremos ser mais claros a respeito de qual estimador estaremos nos referindo. Adotando o mesmo procedimento para \\(X\\), podemos calcular o somatório dos quadrados de \\(X\\) (\\(SQ_X\\)). \\[SQ_X = \\sum_{i-1}^{n} (X_i - \\overline{X})^2 = \\sum_{i-1}^{n}(X_i - \\overline{X}) (X_i - \\overline{X})\\] e a variância amostral de \\(X\\) (\\(s^2_X\\)). \\[s^2_X = \\frac{\\sum_{i-1}^{n} (X_i - \\overline{X})^2}{n-1}\\] Combinando as duas ideias, teremos o produto cruzado de \\(Y\\) e \\(X\\) (\\(SQ_{YX}\\)) \\[SQ_{YX} = \\sum_{i-1}^{n}(Y_i - \\overline{Y}) (X_i - \\overline{X})\\] e finalmente a covariância amostral entre \\(Y\\) e \\(X\\) (\\(s_{YX}\\)). \\[s_{YX} = \\frac{\\sum_{i-1}^{n}(Y_i - \\overline{Y}) (X_i - \\overline{X})}{n-1}\\] 10.2 Coeficiente de correlação linear de Pearson \\(r\\) Quando estamos interessados em medir o grau de correlação entre duas variáveis utilizamos o coeficiente de correlação de Pearson (\\(r\\)). O coeficiente \\(r\\) mede a intensidade da correlação linear entre \\(Y\\) e \\(X\\). Vimos que a covariância amostral (\\(s_{YX}\\)) mede a intensidade de uma associação linear entre \\(Y\\) e \\(X\\). A covariância entretanto, não tem limite superior ou inferior, pois sua magnitude depende da ordem de grandeza das variáveis envolvidas. O coeficiente de correlação \\(r\\) é calculado como a covariância entre \\(Y\\) e \\(X\\) padronizada pelo produto dos desvios padrões de \\(Y\\) e de \\(X\\). \\[r = \\frac{s_{YX}}{s_Y s_X} = \\frac{\\frac{\\sum{(Y_i - \\overline{Y})(X_i - \\overline{X})}}{n-1}} {\\sqrt{\\frac{\\sum{(Y_i - \\overline{Y})^2}}{n-1}} \\times \\sqrt{\\frac{\\sum{(X_i - \\overline{X})^2}}{n-1}}}\\] \\[r = \\frac{\\sum{(Y_i - \\overline{Y})(X_i - \\overline{X})}}{\\sqrt{\\sum{(Y_i - \\overline{Y})^2 \\sum{(X_i - \\overline{X})^2}}}}\\] Esta padronização garante que o índice pode variar entre \\(-1\\) (correlação perfeitamente linear e negativa) e \\(+1\\) (correlação perfeitamente linear e positiva), se aproximando de zero quando não existe correlação. 10.3 Exemplo No conjunto de dados abaixo (Haddon 2010) (disponível em ctigre_haddon.csv) mostra dados da pesca do camarão tigre e do camarão rei entre nos anos de 1976 a 1987. O camarão tigre constitui a espécie alvo da pesca, enquanto o camarão rei aparece como uma espécie acidental. Importe a base de dados (Disponível em :): tigre = read_csv(&quot;ctigre_haddon.csv&quot;) tigre Ano Camarão tigre Camarão rei 1976 566 10 1977 1437 22 1978 1646 42 1979 2056 33 1980 3171 64 1981 2743 34 1982 2838 59 1983 4434 146 1984 4149 78 1985 3480 75 1986 2375 81 1987 3355 52 c1 = ggplot(tigre, aes(x = Year, y = Tiger)) + geom_line(color = &quot;red&quot;) + geom_point(color = &quot;red&quot;, shape = 19) + geom_line(mapping = aes(y = King), color = &quot;blue&quot;) + geom_point(mapping = aes(y = King), color = &quot;blue&quot;, shape = 19) + geom_segment(x = 1976, xend = 1976.3, y = 4000, yend = 4000, color = &quot;red&quot;) + geom_segment(x = 1976, xend = 1976.3, y = 3700, yend = 3700, color = &quot;blue&quot;) + geom_text(x = 1976.4, y = 4000, label = &quot;Camarão tigre&quot;, hjust = 0) + geom_text(x = 1976.4, y = 3700, label = &quot;Camarão rei&quot;, hjust = 0) + scale_x_continuous(breaks = tigre$Year) + theme_classic() c2 = ggplot(tigre, aes(y = King, x = Tiger)) + geom_point(shape = 19) + scale_y_continuous(breaks = seq(0, 150, by = 20)) + scale_x_continuous(breaks = seq(500, 5000, by = 500)) + labs(x = &quot;Camarão tigre (Ton)&quot;, y = &quot;Camarão rei (Ton)&quot;) + theme_classic() c1 + c2 A captura em toneladas do camarão tigre é sempre mais elevada. Entretanto, a figura da direita sugere haver uma associação linear entre as capturas. Nos anos em que houve maiores capturas do camarão tigre parece ter havido também um aumento nas capturas do camarão rei. Dizemos as capturas covariam positivamente. Portanto existe uma correlação positiva entre a captura das duas espécies. Em nenhum momento estamos dizendo que a captura de uma espécie é a causa do aumento na captura da outra. Muito provavelmente, as abundâncias das duas espécies estão relacionadas a um terceiro fator que gera um comportamento similar na variação das capturas ano a ano. Estamos interessados em mensurar o grau de associação seja pela covariância ou pelo coeficiente de correlação de Pearson. Em nosso exemplo, a covariância entre as abundâncias dos camarões tigre e rei é positiva (\\(s_{tigre-rei} = 3.3293\\times 10^{4}\\)) e consequentemente a correlação de Pearson também é positiva (\\(r = 0.82\\)). Confira os cálculos utilizando as expressões apresentadas no capítulo. No R, a covariância entre \\(Y\\) e \\(X\\) pode ser obtida pela função cov: cov(tigre$Tiger, tigre$King) ## [1] 33293 E a correlação pela função cor: cor(tigre$Tiger, tigre$King) ## [1] 0.8196913 References "],["biquantquali.html", "Capítulo 11 Análise bidimensional: variáveis quantitativas e qualitativas 11.1 Visualizando a distribuição de \\(Y\\) em diferentes grupos 11.2 Partição das Soma dos Quadrados 11.3 Voltando ao conjundo de dados medley.csv", " Capítulo 11 Análise bidimensional: variáveis quantitativas e qualitativas Para finalizar a seção sobre estatística descritiva, vamos entender como podemos visualizar a relação entre uma variável \\(Y\\) contínua e uma variável \\(X\\) categórica e mensurar o grau de associação entre elas. Iremos nos referir a \\(Y\\) como variável dependente (ou variável resposta) e a \\(X\\) como variável independente (ou preditora). Assim, assumimos explicitamente que \\(Y\\) é função (depende) de \\(X\\) e não o contrário. O nome preditora vem do fato que, se \\(Y\\) e \\(X\\) estão associadas, ao conhecemos \\(X\\) somos capazer de prever a resposta média em \\(Y\\). 11.1 Visualizando a distribuição de \\(Y\\) em diferentes grupos Considere o conjunto de dados medley.csv (disponível também em Chapter 10 - Single factor classification - ANOVA) que avalia o impacto da presença de metais pesados na diversidade de espécies de diatomácias em riachos (Medley and Clements (1998); Queen, Quinn, and Keough (2002); Logan (2011)). medley = read_csv(&quot;medley.csv&quot;) medley STREAM ZINC DIVERSITY Eagle BACK 2.27 Eagle HIGH 1.25 Eagle HIGH 1.15 Eagle MED 1.62 Blue BACK 1.70 Blue HIGH 0.63 Blue BACK 2.05 Blue BACK 1.98 Blue HIGH 1.04 Blue MED 2.19 Blue MED 2.10 Snake BACK 2.20 Snake MED 2.06 Snake HIGH 1.90 Snake HIGH 1.88 Snake HIGH 0.85 Arkan LOW 1.40 Arkan LOW 2.18 Arkan LOW 1.83 Arkan LOW 1.88 Arkan MED 2.02 Arkan MED 1.94 Arkan LOW 2.10 Chalk LOW 2.38 Chalk HIGH 1.43 Chalk HIGH 1.37 Chalk MED 1.75 Chalk LOW 2.83 Splat BACK 1.53 Splat BACK 0.76 Splat MED 0.80 Splat LOW 1.66 Splat MED 0.98 Splat BACK 1.89 A coluna STREAM é uma variável categórica contendo o nome dos \\(6\\) riachos amostrados (Arkan, Blue, Chalk, Eagle, Snake, Splat). A coluna ZINC é uma variável categórica ordinal com \\(4\\) níveis de concentração de zinco na água (BACK &lt; LOW &lt; MED &lt; HIGH). O primeiro nível (BACK) é o nível de referência (BACKGROUND). Finalmente, a coluna DIVERSITY é uma variável contínua que contém a diversidade de diatomácieas (medida pelo índice de diversidade de Shannon) medida de cada uma das 34 amostras. Vamos nos concentrar nas variáveis DIVERSITY (\\(Y\\)) e ZINC (\\(X\\)). Em delineamento experimental, dizemos que ZINC é um tratamento, isto é, uma condição experimental sob a qual nossa variável dependente \\(Y\\) foi mensurada. Para verificarmos a distribuição de diversidade para cada concentração de zinco podemos fazer simplesmente um gráfico de dispersão como fizemos no capítulo 10. A diferença agora é que \\(X\\) trata-se de uma variável categórica (neste caso, ordinal com \\(4\\) níveis). ggplot(medley) + aes(x = ZINC, y = DIVERSITY) + geom_point() + theme_classic() 11.1.1 Boxplots para os níveis do tratamento Não há problema em apresentarmos um gráfico de dispersão. No entanto, em situações deste tipo estamos comumente interessados em representar medidas-resumo que nos permitam comparar os diferentes níveis do tratamento. A forma mais comum de representar esta situação é por meio de um boxplot (Capítulo 7) para cada nível do tratamento. ggplot(medley) + aes(x = ZINC, y = DIVERSITY) + geom_boxplot() + theme_classic() Neste caso, não estamos vendo as observações individuais, mas a posição da mediana, dos quartis (\\(1^o\\) e \\(3^o\\)) e dos pontos máximo e mínimo para cada nível separadamente. Alguns pontos extremos podem aparecer isoladamente para indicar que estes pontos estão muito distantes dos demais. Podemos controlar esta representação com o argumento coef na função geom_boxplot. Rode o comando abaixo e compare os resultados. ggplot(medley) + aes(x = ZINC, y = DIVERSITY) + geom_boxplot(coef = 3) + theme_classic() Vemos que o boxplot referente ao nível HIGH está em uma posição inferior aos demais, sugerindo que a diversidade de diatomáceas tende a ser mais baixa para níveis elevados de zinco. No capítulo 20 iremos testar formalmente esta afirmação, mas por hora estamos interessados apenas em reconhecer/descrever possíveis tendências. Exitem outras variações que podem nos ajudar a entender melhor os padrões. Podemos sobrepor os pontos individuais sobre os boxplots: ggplot(medley) + aes(x = ZINC, y = DIVERSITY) + geom_errorbar() + theme_classic() 11.1.2 O gráfico de erros Nas figuras acima representamos os quartis das distribuições. Podemos estar interessados em apresentar somente os pontos médios (média aritimética) juntamente com barras de erro que representem alguma medida de dispersão (ex. desvio padrão). Para isto é necessário inicialmente criar um data.frame com estas medidas. ZINC Media Desvio BACK 1.797500 0.4852613 LOW 2.032500 0.4449960 MED 1.717778 0.5030104 HIGH 1.277778 0.4268717 medley_barras = medley %&gt;% group_by(ZINC) %&gt;% summarise(Media = mean(DIVERSITY), Desvio = sd(DIVERSITY)) medley_barras E em seguida plotar a figura. ggplot(medley_barras, aes(x = ZINC)) + geom_point(aes(y = Media)) + geom_errorbar(aes(ymin = Media - Desvio, ymax = Media + Desvio), width = 0.4) + labs(y = &#39;DIVERSITY&#39;) + theme_classic() Aqui vemos somente os pontos médios e as barras de erro, que estão à distância de \\(1\\) desvio padrão acima e abaixo da média (\\(\\overline{Y} \\pm 1s\\)). Embora tenhamos expressado as distâncias utilizado o desvio padrão como medida de variação, poderíamos ter utilizado outras medidas como o erro padrão ou o intervalo de confiânça, como veremos no capítulo 16. O importante é sempre deixar claro qual medida de variação está semdo representada no gráfico de erros (Veja: Krzywinski &amp; Altman, 2013 - Error bars - Points of Significance). ggplot(medley) + aes(x = ZINC, y = DIVERSITY) + geom_boxplot(coef = 3) + geom_point(color = &#39;red&#39;) + theme_classic() 11.2 Partição das Soma dos Quadrados Ao representarmos a distribuição de uma variável \\(Y\\) contínua em função de uma variável \\(X\\) categórica, geralmente estamos interessados em determinar se os diferentes níveis de \\(X\\) (diferentes grupos) têm médias similares ou se ao menos um dos níveis têm média diferente dosdemais. Queremos um métodos que nos permita diferenciar situações como as apresentadas abaixo: Na figura \\(I\\) todos os grupos são provenientes da mesma distribuição e têm médias aproximadamente iguais (\\(\\overline{Y}_A \\approx \\overline{Y}_B \\approx \\overline{Y}_C \\approx \\overline{Y}_D\\)). Na figura \\(II\\) o segundo grupo B tem média mais elevada que os demais e na da figura \\(III\\), todas as médias parecem ser diferentes entre si (\\(\\overline{Y}_A \\ne \\overline{Y}_B \\ne \\overline{Y}_C \\ne \\overline{Y}_D\\)). Para mensurar o grau de associação entre \\(Y\\) e \\(X\\) e entender como podemos diferenciar as situações acima, vamos introduzir o processo de Partição da Soma dos Quadrados. Para entendermos do que se trata a partição da soma dos quadrados vamos iniciar com um exemplo simples descrito na figura e na tabela abaixo: Para deixarmos claro as notações que iremos adotar adiante vamos definir que: Temos \\(k = 3\\) grupos (A, B ou C) e para cada grupo \\(n = 5\\) observações. Denotamos por \\(n_{ij}\\) o número de observações dentro de cada grupo, em que \\(i\\) é a i-ésima observação (\\(i = 1\\) a \\(5\\)) do j-ésimo grupo (\\(j = 1\\) a \\(3\\) - grupos A ao C). Neste exemplo, o número de observações em cada grupo é o mesmo (\\(n_1 = n_2 = n_3 = n\\)), de modo que o total de observações é dado por: \\(N = k \\times n = n_1 + n_2 + n_3 = 15\\) A média de cada grupo será denotada por \\(\\overline{Y}_j\\), que neste exemplo são: \\(Y_1 = 20.64\\) (grupo A), \\(Y_2 = 28.68\\) (grupo B) e \\(Y_3 = 12.18\\) (grupo C). Vamos denotar por \\(\\overline{\\overline{Y}}\\) a Grande Média, isto é, a média geral de todas as observações independente do grupo de origem. \\[\\overline{\\overline{Y}} = \\sum_{j = 1}^{k}\\sum_{i = 1}^{n}\\frac{Y_{ij}}{N} = \\frac{\\overline{Y_1} + \\overline{Y_2} + \\overline{Y_3}}{3} = 20.5\\] Podemos agora observar estes elementos no gráfico de dispersão. Em seguida, precisamos calcular \\(3\\) quantias: i - a Soma dos Quadrados Totais (\\(SQ_{Total}\\)), ii - a Soma dos Quadrados dos Tratamentos \\(SQ_{Trat}\\) e iii - a Soma dos Quadrados dos Resíduos \\(SQ_{Res}\\). Soma dos Quadrados Totais \\(SQ_{Total}\\): mede as diferenças entre \\(Y_{ij}\\) e \\(\\overline{\\overline{Y}}\\). Temos nesta expressão o somatório dos desvios ao quadrado de todas as observações com relação à grande média independente do grupo de origem de cada observação. \\[SQ_{Total} = \\sum_{j = 1}^{k}\\sum_{i = 1}^{n}(Y_{ij} - \\overline{\\overline{Y}})^2\\] Soma dos Quadrados dos Tratamentos \\(SQ_{Trat}\\): mede as diferenças entre as médias dos tratamentos \\(\\overline{Y}_j\\) e \\(\\overline{\\overline{Y}}\\), sendo portanto os desvios ao quadrado da média de cada tratamento subtraída da grande média. \\(SQ_{Trat}\\) também é chamada de soma dos quadrados entre grupos ou entre tratamentos \\[SQ_{Trat} = \\sum_{j = 1}^{k}\\sum_{i = 1}^{n_{j}}(\\overline{Y}_{j} - \\overline{\\overline{Y}})^2 = \\sum_{j = 1}^{k}n_{j}(\\overline{Y}_{j} - \\overline{\\overline{Y}})^2\\] Soma dos Quadrados dos Resíduos \\(SQ_{Res}\\): mede as diferenças entre cada observação \\(Y_{ij}\\) e a média de seu próprio grupo \\(\\overline{Y}_{j}\\). \\(SQ_{Res}\\) também é chamada de soma dos quadrados dentro dos grupos ou dentro dos tratamentos \\[SQ_{Res} = \\sum_{j = 1}^{k}\\sum_{i = 1}^{n_{j}}(Y_{ij} - \\overline{Y}_{j})^2\\] 11.2.1 A característica aditiva das somas dos quadrados A partição da soma dos quadrados consiste em decompor a variação total do experimento em uma parcela atribuída à variação entre tratamentos e outra parcela da variação dentro dos tratamentos. Isto é possível pois as somas dos quadrados definidas acima podem ser expressas de forma aditiva como: \\[SQ_{Total} = SQ_{Trat} + SQ_{Res}\\] Deste modo, é possível demostrar que: \\(\\sum_{j = 1}^{k}\\sum_{i = 1}^{n}(Y_{ij} - \\overline{\\overline{Y}})^2 = \\sum_{j = 1}^{k}n_{j}(Y_{j} - \\overline{\\overline{Y}})^2 + \\sum_{j = 1}^{k}\\sum_{i = 1}^{n}(Y_{ij} - \\overline{Y}_{j})^2\\) 11.2.2 Medindo a associação entre \\(Y\\) e \\(X\\) A característica aditiva das somas dos quadrados pode ser utilizada para mensurar o grau de dependência de \\(Y_{ij}\\) com respeito aos diferentes tratamentos. Compare as duas figuras abaixo: A soma dos quadrados dentro dos grupos é a mesma nas duas figuras (\\(SQ_{Res} = 362.6\\)). No entanto, na figura da esquerda, em que as médias dos tratamentos são similares (e consequentemente próximas à grande média), a soma dos quadrados entre os tratamentos é muito menor (\\(SQ_{Trat}^{esquerda} = 15.8\\)) que na figura da direita, em que as médias dos tratamentos estão distantes entre si (\\(SQ_{Trat}^{direita} = 680.8\\)). É desta forma que a partição das somas dos quadrados nos permite diferenciar situações em que: i - a média dos grupos depende dos níveis do tratamento (figura da direita); de situações em que ii - a média não depende dos níveis do tratamento (figura da esquerda). A compreensão desta ideia é central para entendermos os delineamentos de Análise de Variância (Capítulos 20 a 25) e de Regressão linear (Capítulos 22 e 23) que veremos nas próximas seções. 11.2.3 O coeficiente de determinação (\\(R^2\\)) Podemos expressar a relação entre \\(SQ_{Trat}\\) e \\(SQ_{Total}\\) pela expressão: \\[R^2 = \\frac{SQ_{Trat}}{SQ_{Trat} + SQ_{Res}} = \\frac{SQ_{Trat}}{SQ_{Total}}\\] \\(R^2\\) é chamado de coeficiente de determinação e varia entre \\(0\\) e \\(1\\). Se \\(R^2 = 0\\) toda a variação em \\(Y\\) é causada por \\(SQ_{Res}\\) (\\(\\overline{\\overline{Y}} = \\overline{Y}_1 = \\overline{Y}_2 = \\cdots = \\overline{Y}_k\\)). À medida que as médias dos tratamentos se distanciam umas das outras, \\(R^2\\) se aproxima de \\(1\\) pois a maior parte da variação em \\(Y\\) é causada por \\(SQ_{Trat}\\). Voltaremos a falar do coeficiente de determinação nos capítulos 22 e 23. 11.3 Voltando ao conjundo de dados medley.csv Existe uma série de formas de obtermos os somatórios dos quadrados no R, todos eles associados aos comandos utilizados em Análise de Variância e Regressão Linear. O comando aov por exemplo, pode ser utilizado da seguinte forma: aov(DIVERSITY ~ ZINC, data = medley) ## Call: ## aov(formula = DIVERSITY ~ ZINC, data = medley) ## ## Terms: ## ZINC Residuals ## Sum of Squares 2.566612 6.516411 ## Deg. of Freedom 3 30 ## ## Residual standard error: 0.4660619 ## Estimated effects may be unbalanced O resultado retorna a soma dos quadrados dos tratamentos (neste caso a coluna ZINC) e dos resíduos (coluna Residuals). Como o \\(SQ_{Total}\\) é simplesmente a soma dos dois anteriores, podemos obtê-lo facilmente: ## [1] 9.083024 Por fim, o \\(R^2\\) pode ser calculado por: ## [1] 0.2825725 References "],["popamostra.html", "Capítulo 12 Descrevendo populações e amostras 12.1 População, amostra e unidade amostral 12.2 Distribuição de frequências na população estatística 12.3 Distribuições de frequências na amostra 12.4 Parâmetros e estimadores 12.5 Amostragem e inferência", " Capítulo 12 Descrevendo populações e amostras 12.1 População, amostra e unidade amostral Em estatística, a população se refere a todos os elementos sobre os quais queremos tirar conclusões. É comum a confusão entre os termos população estatística e população biológica (nas ciências naturais) ou população humana (em ciências sociais). No entanto, população estatística refere-se ao conjunto de medidas, isto é, de variáveis que podem ser mensuradas como resultado de um experimento ou de um estudo observacional. A população estatística não pode portanto, ser confundida com conjuntos de pessoas ou de organismos ou mesmo com os elementos físicos nos quais as variáveis foram mensuradas. As medidas que compõem a população estatística podem ser pesos, temperaturas, velocidades, tempos de reação, entre outras, a depender das características de um estudo particular. A abrangência da população estatística depende do contexto e do escopo da pergunta que se pretende responder. Exemplo 1: Suponha um estudo para descrever o comprimento do lambari Deuterodon iguape em riachos do litoral de São Paulo. A população estatística não são os peixes em si, mas o comprimento de cada indivíduo que habita os riachos destas bacias. Dado o escopo do estudo (bacias do litoral de São Paulo), a população estatística abrange somente comprimentos dos organismos existem nesta região. Exemplo 2: Suponha agora que desejamos estudar a diversidade de espécies de peixes em bacias costeiras do litoral de São Paulo. Neste caso, a população estatística seria constituida de um índice de diversidade calculado para cada uma das bacias costeiras do litoral. Fica claro que, neste caso, população estatística não tem qualquer relação com população biológica, mas sim com uma variável que pode ser mensurada a partir do conjunto de espécies que habitam cada bacia. Nestes dois exemplos é inviável obter informações de todos os elementos que compõem a população estaística. No caso dos comprimentos, não temos como capturar todos os animais presentes em uma bacia hidrográfica e, ainda que tivéssemos, existem provavelmente alguns milhares de peixes somente em um pequeno trecho de riacho e consequentemente, o mesmo número de comprimentos individuais. Já o número de Bacias costeiras no litoral do Estado de São Paulo é bem menor, porém ainda é inviável mensurar a diversidade de espécies em todas elas. Um censo ocorre nos raros exemplos em que é possível mensurar todos os elementos da população estatística. Entretanto, a prática científica lida com a maioria dos casos em que mensuramos um subconjunto da população estatística, definido como uma amostra. Finalmente, unidade amostral é definida como um único elemento da população estatística. A unidade amostral é a menor unidade independente associada ao estudo. A necessidade das unidades amostrais constituirem elementos independentes é um dos pressupostos centrais da estatística e suas implicações ficarão mais claras quando tratarmos do processo de amostragem. No exemplo dos lambaris, unidade amostral é o comprimento mensurado em um indivíduo da espécies de interesse, enquanto no exemplo das bacias costeiras, as unidades amostrais são cada um dos valores de diversidade calculados para cada bacia costeira. População estatística: todos os elementos que podem compor uma amostra. Podem ser medidas como comprimentos, temperaturas, velocidades, etc. Unidade amostral: um único elemento da população. Censo: o levantamento de todos os elementos da população. Amostra: um subconjunto extraído da população. Tamanho populacional (N): o número de elementos da população. Tamanho amostral (n): o número de elementos da amostra. 12.2 Distribuição de frequências na população estatística Os valores em uma população estatística não são idênticos, certamente existe uma distribuição de frequência em que algumas faixas de valores são mais frequentes, enquanto outras faixas de valores são inexistentes. Os comprimentos de Deuterodon iguape devem variar por exemplo de alguns milímetros (pós-larva) a cerca de 20 cm (adulto), em que nem todos os comprimentos são igualmente representados. Certamente, existem mais lambaris pequenos e médios do que lambaris grandes. De fato, animais muito grandes são os mais raros, de modo que se tivessemos informação da população estatística, deveríamos que faixas de valores muito elevados se tornaria cada vez menos frequentes. Se fosse possível observar todos os elementos da população estatística, saberíamos exatamente qual o formato de sua distribuição de frequências. Suponha por exemplo, a altura de adultos acima de 18 anos. Seria razoável supor que a maioria das alturas está em valores intermediários ao redor de, por exemplo, 1.7 metros. É razoável supor também que a frequência de pessoas muito altas ou muito baixas vai diminuindo gradativamente, se modo que é muito raro encontrarmos adultos muito altos (ex. acima de \\(2\\) metros) ou muito baixos (ex. menores que \\(1.5\\) metros). A figura abaixo, descreve uma distribuição de frequência possível para esta variável. Figure 12.1: Distribuição em uma população estatística das alturas (em metros) de adultos acima de 18 anos. Vemos que existem mais valores entre \\(1.6\\) e \\(1.8\\) metros e poucas observações extremas, o que é condizente com nossa expectativa para a distribuição de frequências das alturas de indivíduos adultos. Este exemplo é fictício e segue uma distribuição normal de probabilidades. A distribuição é simétrica, ou seja, os valores extremos são igualmente representados acima e abaixo da região central (média). Você poderá encontrar o termo bell curve em inglês devido à sua forma de sino ou distribuição gaussiana em homenagem a Carl Friedrich Gauss um dos mais importantes matemáticos do século XXI. Gauss lidou com a distribuição normal quando desenvolveu a Teoria da distribuição dos erros observacionais que é também um tópico central ao desenvolvimento da estatística e do método científico. 12.3 Distribuições de frequências na amostra Não é viável ou mesmo possível obter informações de todos os elementos que compõem a população estatística. Ainda assim, gostaríamos de ter informações sobre esta população para que possamos avançar no conhecimento científico. A amostragem é o meio de se obter tais informações. Ao amostrar uma população estatística, iremos constatar que nem todos os seus valores são idênticos. No caso dos exemplos anteriores, os lambaris de uma região não têm todos o mesmo comprimento e do mesmo modo, nem todas as todas as bacias costeiras do estado de São Paulo têm os mesmos valores de diversidade. Um vez que não é possível ou viável observar toda a população estatística, a amostra é o meio de nos aproximarmos desta informação. Suponha, uma amostra de 30 adultos do exemplo anterior. Se organizarmos esta amostra em valores crescente teríamos: 1.4, 1.51, 1.52, 1.53, 1.54, 1.58, 1.58, 1.6, 1.6, 1.6, 1.6, 1.6, 1.61, 1.62, 1.62, 1.63, 1.64, 1.64, 1.65, 1.66, 1.66, 1.67, 1.67, 1.67, 1.69, 1.7, 1.71, 1.72, 1.73, 1.74, 1.74, 1.74, 1.75, 1.75, 1.76, 1.76, 1.76, 1.77, 1.78, 1.78, 1.78, 1.78, 1.81, 1.81, 1.84, 1.84, 1.88, 1.88, 1.89, 1.92 Os valores estão entre \\(1.4\\) e 1.92, o que certamente não é igual aos valores máximos e mínimos da população. Vamos representar a amostra por meio de um histograma Figure 12.2: Amostra de tamanho n = 50 adultos Na figura acima, ainda que a distribuição não seja igual à população estatística, podemos perceber que há uma concentração de valores ligeiramente maior entre \\(1.6\\) e \\(1.8\\), assim como na população estatística. A diferença entre a distribuição da população e da amostra é esperada, e ocorre justamente porque estamos observando um subconjunto de elementos. Vamos verificar por exemplo, as distribuição de frequência de outras amostras possíves de tamanho n = 50 desta mesma população. Figure 12.3: Amostras de tamanho n = 50 adultos Note que a cada amostra, a distribuição de valores foi diferente. No entanto, de forma geral, observamos a situação em que a faixa intermediária de valores foi mais frequente e que os extremos foram mais raros. Desta forma, o que esperamos no processo de amostragem de uma é obter uma amostra representativa da população estatística, em que a distribuição de frequências se aproxime da distribuição de frequencias da população. Como conhecíamos a população estatística neste exemplo fictício, este comportamento já era esperado. Na pratica científica entretanto, não conhecemos a população estatística e consequentemente não temos como saber se a nossa amostra em particular foi ou não representativa. O que devemos garantir é que o processo de amostragem seja conduzido de tal forma que a teoria da amostragem nos garante que, em média, nossa amostragem será representativa. Como veremos no Capítulo 13, o modo mais simples de garantir este comportamento é realizarmos uma amostra aleatória dos elementos da população estatística. 12.4 Parâmetros e estimadores Vimos nos capítulos 5 e 6 que um conjunto de observações costuma ser caracterizada por descritores de tendência central e de variação. Considere a questão levantada no inícip deste capítulo: Qual o comprimento de Deuterodon iguape em riachos do litoral de São Paulo? Geralmente, entendemos esta questão como: - Qual o comprimento de um lambari típico. Do ponto de vista estatítico, um lambari típico pode ser entendido como aquele de que tenha um comprimento médio. Ou seja, entendemos que a média aritmética pode ser uma forma de representar o valor esperado na popuilação estatística. Se este comprimento médio fosse calculado a partir de TODOS os elementos da população, teríamos acesso a um parâmetro desta distribuição, ou seja, um descritor da população estatística. Os parâmetros só podem ser obtidos por meio de um censo, pois para serem calculados requerem que todos os elementos da população sejam mensurados. Amédia não é o único parãmetro desta população, poderíamos também calcular a variância a partir de todos os elementos da população e teríamos um parâmetro que mede a dispersão de todos os elementos da população estatística ao redor da mérdia. IMPORTANTE! Denominamos de parâmetro um descritor obtido a partir da mensuração de todos os elementos da população estatística. Os parâmetros são comumente representados por letras gregas. O símbolo \\(\\mu\\), por exemplo, é utilizado para representar a média populacional, enquanto o símbolo \\(\\sigma^2\\) para representar a variância populacional. Ao conduzirmos uma amostragem de \\(n\\) elementos teremos um subconjunto possível da população estatística que nos permitirá calcular descritoers da amostra. Descritores da amostra são conhecidos também como estimadores ou estatísticas. A média aritmética deste subconjunto por exemplo, é denominada de média amostral, sendo representanta por \\(\\overline{X}\\). Poderíamos estimar também a variância desta amostra (variância amostral), representada por \\(s^2\\). Como definições importantes temos: Parâmetro: a medida que descreve uma característica da população estatística. Ex.: a média (\\(\\mu\\)) ou a variância (\\(\\sigma^2\\)) populacional. Estimador ou Estatística: Uma medida que descreve uma característica da amostra. Ex.: a média amostral (\\(\\overline{X}\\)) ou a variância amostral (\\(s^2\\)). Estimativa: é o valor numérico assumido pelo estimador. Ex. o valor número calculado da média ou variância de uma amostra em particular. 12.5 Amostragem e inferência Em última instância, não estamos interessados na amostra em si, mas na população da qual ela é proveniente. Entretanto, o processo de amostragem é a única forma de obtermos alguma informação sobre esta população. Tendo obtido uma amostra representativa, calculamos descritores (ex. de tendência central e dispersão) que serão utilizados como estimadores dos parâmetros populacionais, um processo que denominamos de inferência estatística. Como verificamos anteriormente, cada amostra que retirarmos da mesma população estatística será diferente. Isto significa que os estimadores destas amostras (ex. \\(\\overline{X}\\)) serão diferentes entre si e numericamente diferentes dos parâmetros populacionais. No entanto, veremos nos capítulos 13 a 16 que podemos conhecer algumas propriedades destes estimadores que nos permitirão estabelecer limites de confiânça de para nossa inferência sobre a população estatística. Inferência Estatística O processo que consiste em utilizamos descritores da amostra para tirar conclusões sobre as características da população estatística. Figure 12.4: Processo de amostragem e inferência estatística "],["amostrmedias.html", "Capítulo 13 Amostrando uma População Estatística 13.1 Amostragem aleatória simples 13.2 Amostragem aleatória estratificada 13.3 Amostragem sistemática 13.4 Erro amostral, acurácia e precisão", " Capítulo 13 Amostrando uma População Estatística Como discutimos no capítulo 12 o objetivo da amostragem é descrever características da população estatística por meio de características da amostra. E um estudo sobre o tamanho do pescado em uma determinada área de pesca, a população estatística são os diâmetros de todos os peixes que podem ser pescados na reião. A população estatística pode ser descrita por parâmetros que representam medidas de centro como o diâmetro médio (\\(\\mu\\)), ou por medidas de variação como o desvio padrão (\\(\\sigma\\)), que representam o grau de dispersão das unidades amostrais ao redor da média. Se amostramos n elementos desta população, a média amostral (\\(\\overline{X}\\)) e o desvio padrão amostral (\\(s\\)) dos diâmetros serão os estimadores destas características. Dependendo da questão envolvida e do conhecimento prévio sobre a população, diferentes métodos de amostragem são apropriados. A teoria da amostragem é a área da ciência que estuda estes métodos. Neste capítulo vamos discutir três tipos de amostragem: aleatória simples, estratificada e sistemática. 13.1 Amostragem aleatória simples É aquela em que cada elemento da população tem a mesma probabilidade de ser selecionado para compor a amostra. Por exemplo, se a população consiste de \\(1000\\) elementos, cada um terá uma probabilidade de \\(\\frac{1}{1000}\\) de ser escolhido. Isto isenta o pesquisador de tomar qualquer decisão com base em julgamentos pré-concebidos, sobre quais elementos devem ou não compor a amostra. Suponha uma população hipotética de somente \\(10\\) elementos: População: 3, 10, 14, 19, 27, 28, 29, 41, 42, 43 Em uma amostra aleatória simples de cinco elementos, qualquer combinação destes \\(10\\) elementos é igualmente provável. Se por puro acaso sortearmos uma amostra aleatória contendo os cinco menores valores da população: Amostra 1: 3, 10, 14, 19, 27 A amostra seria tão aleatória e válida do ponto de vista estatístico quanto qualquer outra como: Amostra 2: 27, 28, 42, 3, 43 Isto significa que uma amostra aleatória pode não ser necessariamente representativa da população. Amostras pequenas por exemplo, têm uma chance maior de selecionar apenas valores extremos, ou seja, os maiores ou menores elementos da população. A média amostral (\\(\\overline{X}\\)) calculada para estas amostras estará distante da média populacional (\\(\\mu\\)). No entanto, a importância central da amostragem aleatória em estatística está no fato de que a aleatoriedade produz, em média, amostras representativas da população. Deste modo, é esperado que na maioria das vezes, uma amostra aleatória gere médias amostrais próximas à média populacional. Por este motivo, é fundamental prezar pela aleatoriedade no processo amostral, pois de outro modo não poderemos garantir que a inferência seja válida com base nas leis de probabilidade. O modo mais direto de se obter uma amostra aleatória é por meio de sorteio. Após atribuir números de 1 a \\(N\\) a cada unidade amostral, estas unidades são sorteadas até que seja atingido o tamanho \\(n\\) desejado. Na prática, nem sempre é simples, ou mesmo possível obter uma amostra aleatória nestes moldes. Não seria possível enumerar todos os peixes de uma região para, após um sorteio, tomar as medidas somente dequeles selecionados. Entretanto, se empregarmos ummétodo de coleta variado em que indivíduos de todos os tamanhos sejam aproximadamente igualmente sujeitos a serem capturados poderíamos no aproximar do que seria uma amostra verdsadeiramente aleatória. Outras dificuldades práticas obviamente seriam possíveis neste procedimento, como garantir acesso irrestrito à toda a área de ocorrência da espécie ou tempo disponível para percorrer a toda região. Questões como estas não desmerecem o requisito básico de se obter uma amostra aleatória, mas devem nos auxiliar a decidir como conciliar a prática experimental com a necessidade de garantirmos uma amostra aleatória. 13.2 Amostragem aleatória estratificada Se tivermos algum conhecimento prévio de como a população está estruturada, a amostra aleatória simples, embora não esteja incorreta, pode não ser a estratégia mais eficiente. Se for possivel identificar estratos ou subgrupos dentro da população, podemos conduzir uma amostragem aleatória estatificada. Voltemos ao exemplo do comprimento do pescado. Suponha que existam duas áreas de ocorrência da espécie. Uma delas sujeita a intensa atividade pesqueira e oura sendo uma área protegida. Poderíamos supor que na área protegida estejam os maiores indivíduos, justamente porque nesta área não hpá atividade de pesca (que em geral busca indivíduos maiores). Dizemos que os comprimentos em cada uma das duas regiões compõem estratos da população estatística. Nesta situação, uma amostra puramente aleatória sem considerar a existência dos dois estratos pode fazer com que, puramente ao acaso, um deles se torne mais representados na amostra. Se por exemplo da maioria dos pontos selecionados estiverem na região intensamente pescada, o comprimento médio da amostra (\\(\\overline{X}\\)) tenderá a ficar consistentemente abaixo de \\(\\mu\\). A chance disto ocorrer se torna maior principalmente se o tamanho amostral for pequeno. Entretanto, se a seleção dos indivíduos foi feita por meio de sorteio, o simples fato de observarmos este padrão não seria por si só justificativa para refarzermos a amostra. O ponto relevante aqui é que em uma amostra aleatória simples, estes extremos indesejáveis (um estrato mais representado que outro) são mais prováveis de acontecer. Se tyemos conhecimento da existência dos dois estratos portanto, a amostragem aleatória estratificada seria a mais indicada. Neste tipo de amostragem, o esforço amostral é subdividito entre os estratos. O tamanho amostral em cada estrato será o mesmo, ou proporcional ao tamanho do estrato. Uma vez definirmos os tamanhos amostrais que será aplicado aos estratos, as unidades são selecionadas por meio de uma amostragem aleatória simples em cada um. A amostragem aleatória estratificada garante que todos os estratos estejam presentes na amostra conforme sua representatividade na população. Ao fazer isto, as estimativas da amostra tenderão a se concentrar mais próximas ao parâmetro da população. Deste modo, quando os estratos são identificados corretamente, a principal vantagem da amostra aleatória estratificada sobre a amostra aleatória simples está em aumentar a precisão das estimativas. Mais a frente iremos discutir os conceitos de precisão e acurácia e relacioná-los com as estratégias amostrais discutidas aqui. 13.3 Amostragem sistemática Uma amostragem sistemática o pesquisador escolhe umelemento inicial e toma medidas a cada \\(k\\) ocorrências, seguindo a ordem de observação. No caso do comprimento de pescado, para facilitar a tomada de dados, o pesquisador pode medir o primeiro peixe coletado e, em seguida, medir os peixes em intervalos regulares, por exemplo a cada \\(10\\) observados. A escolha da amostragem sistemática ao invés de uma amostragem aleatória simples, deve-se à sua praticidade. Se a característica de interesse das unidades amostrais estiver disposta de forma aleatória ao longo da sequência escolhida, a amostragem aleatória e sistemática irão gerar resultados similares. Na maioria dos casos, é isto que o pesquisador assume (ainda que implicitamente) quando opta por uma amostragem sistemática. Por outro lado, se houver um gradiente justamente na direção do transecto, a variância amostral irá superestimar a variância populacional equanto, se houver uma periodicidade que coincida com o intervalo escolhido, a variância amostral irá subestimar a variância populacional. 13.4 Erro amostral, acurácia e precisão Como as estimativas são obtidas de um subconjunto da população (a amostra), é regra que o resultado obtido de uma amostra aleatória particular, não será igual ao verdadeiro valor da população (o parâmetro), embora exista uma grande probabilidade estar próximo. Erro amostral: é a diferença entre uma estimativa em particular e o parâmetro na população e portanto, é inerente à variabilidade do processo de amostragem. Suponha que, puramente ao acaso, a amostra inclua os menores elementos da população. A média amostral (\\(\\overline{X}\\)) estará muito abaixo da média populacional (\\(\\mu\\)) e o erro amostral será grande. Se calcularmos a média (\\(\\overline{X}\\)) de uma amostra particular, o erro amostral será dado por: \\[E = \\overline{X} - \\mu\\] A estatística estuda o comportamento probabilístico dos erros amostrais. Existe também o erro não amostral que decorre de equívocos de amostragem, inexperiência do amostrador, falha de equipamentos, enganos no cômputo dos resultados, etc. A estatística lida com este tiuṕo de erro. Acurácia: se refere à proximidade entre o parâmetro e o estimador. Um estimador acurado é, em média, igual ao parâmetro populacional. Diferente do erro amostral, a acurácia não se refere a uma estimativa em particular, mas ao valor esperado do estimador, caso a amostragem fosse repetida um grande número de vezes. Um estimador não-acurado (viciado) resulta em valores consistentemente diferentes do parâmetro, podendo estar acima (viés positivo) ou abaixo (viés negativo) do valor populacional. A média aritmética amostral (\\(\\overline{X}\\)) é um estomador não-viciado da média populacional (\\(\\mu\\)) pois: \\[\\mu_{\\overline{X}} - \\mu\\] Precisão: tem relação com a variabilidade do estimador. Estimadores que geram estimativas similares entre si são mais precisos. Porém, se as estimativas estiverem distantes de sua média, o estimador é dito pouco preciso. Exemplo: Para uma população normalmente distribuída, tanto a média aritmética quanto a mediana são estimadores acurados. Entretanto, a variância da mediana é maior que da média aritmética. Dizemos portanto, que a média aritmética é um estimador mais preciso que a mediana. A precisão de um estimador é medida pelo erro padrão da média. \\[\\sigma_{\\overline{X}} =\\frac{\\sigma}{\\sqrt{n}}\\] A figura abaixo é comnmente utilizada para representar os conceitos de precição e acurácia. O centro do alvo é o valor do parâmetro populacional e os pontos em preto são as estimativas. Estimadores acurados geram, em média, estimativas ao redor do parâmetro populacional (viés \\(= 0\\)). Estimadores não-acurados geram, em média, valores deslocados do parâmetro populacional (viés \\(\\ne 0\\)). Estimadores precisos resultam sempre em estimativas próximas entre si, enquanto estimadores não precisos resultam em estimativas distantes umas das outras. Figure 13.1: Representação dos conceitos de precisão e acurácia. 13.4.1 Erro amostral Voltemos à nossa população fictícia com \\(N = 10\\) elementos: População: 3, 10, 14, 19, 27, 28, 29, 41, 42, 43 Para esta população em particular nós conhecemos a média populacional (\\(\\mu\\) = 25.6), de modo que será possível compará-la com as estimativas amostrais. O que acontece se tomarmos uma amostra aleatória de tamanho \\(n = 5\\): Amostra 1: \\(41, 14, 42, 29, 19\\) Para esta amostra, a média vale: \\(\\overline{X} =\\frac{41+14+42+29+19}{5} = 29\\). Os valores \\(\\mu = 25.6\\) e \\(\\overline{X} = 29\\) não são idênticos, pois a amostra contém somente alguns elementos da população. A diferença entre \\(\\mu\\) e \\(\\overline{X}\\) é o chamamos de erro amostral. Neste caso, o erro amostral é: Erro amostral 1: \\(E_1 = 29 - 25.6 = 3.4\\) Se tomarmos outra amostra aleatória, teremos outro conjunto de unidades amostrais, e consequentemente, um \\(\\overline{X}\\) e um erro amostral diferentes. Por exemplo: Amostra 2: \\(27, 29, 19, 10, 14\\) Média amostral 2: \\(\\overline{X_2} = 19.8\\) Erro amostral 2: \\(E_2 = 19.8 - 25.6 = -5.8\\) 13.4.2 Acurácia Até agora, analisamos duas amostras diferentes da população. Porém, quantas amostras distintas seriam possíveis? Para uma população com 10 elementos, a teoria combinatória nos diz que são possíveis: \\[{{10}\\choose{5}} = \\frac{10!}{(10-5)! \\times 5!} = 252\\] formas diferentes de combinarmos \\(N = 10\\) elementos em amostras de tamanho \\(n = 5\\). Inicialmente vamos avaliar a questão com um número menor. Sejam por exemplo, 8 amostras tomadas aleatoriamente, gerando os resultados a seguir: A 1 A 2 A 3 A 4 A 5 A 6 A 7 A 8 19.0 29 10.0 19.0 42.0 29.0 28 43.0 29.0 43 14.0 41.0 29.0 10.0 42 28.0 10.0 28 42.0 43.0 41.0 27.0 10 42.0 42.0 3 28.0 28.0 14.0 42.0 43 27.0 43.0 27 29.0 3.0 28.0 43.0 27 19.0 Medias 28.6 26 24.6 26.8 30.8 30.2 30 31.8 Cada coluna desta matriz corresponde a uma possível amostra aleatória e suas respectivas médias. Algumas amostras tiveram médias muito distantes de \\(\\mu\\), como: \\(\\overline{X_{8}} = 31.8\\) ou \\(\\overline{X_{3}} = 24.6\\). Esta variação é natural do processo amostral. Os métodos de amostragem e de inferência estatística tratam justamente de como lidar e interpretar esta variação. Para entender melhor este processo, vamos obter todas as 252 combinações possíveis de amostras com \\(n = 5\\) e, em seguida, extrair suas respectivas médias. Os resultados das 252 médias possíveis podem ser vistos a seguir ordenados da menor para a maior média possível: 14.6 14.8 15.0 17.4 17.6 17.8 16.4 16.6 19.0 19.2 19.4 16.8 19.2 19.4 19.6 19.4 19.6 19.8 22.0 22.2 22.4 17.4 17.6 20.0 20.2 20.4 17.8 20.2 20.4 20.6 20.4 20.6 20.8 23.0 23.2 23.4 19.4 21.8 22.0 22.2 22.0 22.2 22.4 24.6 24.8 25.0 22.2 22.4 22.6 24.8 25.0 25.2 25.0 25.2 25.4 27.8 18.2 18.4 20.8 21.0 21.2 18.6 21.0 21.2 21.4 21.2 21.4 21.6 23.8 24.0 24.2 20.2 22.6 22.8 23.0 22.8 23.0 23.2 25.4 25.6 25.8 23.0 23.2 23.4 25.6 25.8 26.0 25.8 26.0 26.2 28.6 21.2 23.6 23.8 24.0 23.8 24.0 24.2 26.4 26.6 26.8 24.0 24.2 24.4 26.6 26.8 27.0 26.8 27.0 27.2 29.6 25.6 25.8 26.0 28.2 28.4 28.6 28.4 28.6 28.8 31.2 28.6 28.8 29.0 31.4 31.6 19.6 19.8 22.2 22.4 22.6 20.0 22.4 22.6 22.8 22.6 22.8 23.0 25.2 25.4 25.6 21.6 24.0 24.2 24.4 24.2 24.4 24.6 26.8 27.0 27.2 24.4 24.6 24.8 27.0 27.2 27.4 27.2 27.4 27.6 30.0 22.6 25.0 25.2 25.4 25.2 25.4 25.6 27.8 28.0 28.2 25.4 25.6 25.8 28.0 28.2 28.4 28.2 28.4 28.6 31.0 27.0 27.2 27.4 29.6 29.8 30.0 29.8 30.0 30.2 32.6 30.0 30.2 30.4 32.8 33.0 23.4 25.8 26.0 26.2 26.0 26.2 26.4 28.6 28.8 29.0 26.2 26.4 26.6 28.8 29.0 29.2 29.0 29.2 29.4 31.8 27.8 28.0 28.2 30.4 30.6 30.8 30.6 30.8 31.0 33.4 30.8 31.0 31.2 33.6 33.8 28.8 29.0 29.2 31.4 31.6 31.8 31.6 31.8 32.0 34.4 31.8 32.0 32.2 34.6 34.8 33.4 33.6 33.8 36.2 36.4 36.6 A menor e maior médias possíveis são 14.6 e 36.6 respectivamente. Estes valores são os mais distantes do parâmetro populacional (\\(\\mu = 25.6\\)) e ocorrem quando, puramente ao acaso, são amostrados os 5 menores (3, 10, 14, 19, 27) ou os 5 maiores (43, 42, 41, 29, 28) elementos da população estatística. Estes casos extremos são raros. Em nosso exemplo, valores superiores a 33.8 ou inferiores a 17.4 são muito improváveis. Podemos avaliar graficamente a distribuição das médias amostrais através de um histograma. A grande maioria das médias amostrais concentra-se na porção intermediária do gráfico entre estes limites. Por exemplo, somente 3.2% das observações estão acima de 33.8. Da mesma forma, somente 3.2% das observações estão abaixo de 17.4 Se calcularmos a média das médias (\\(mu_{\\overline{X}}\\)), ou seja, somarmos todos estes valores e dividirmos por 252, o resultado será 25.6, que é exatamente o valor da média populacional \\(\\mu\\). Isto têm uma implicação central em inferência estatística. Significa que a média amostral \\(\\overline{X}\\) é um estimador acurado (= não-viciado), pois tende a estimar corretamente o valor da média populacional \\(\\mu\\). Ou seja, o histograma acima está centrado ao redor de \\(\\mu\\), o que significa que em média uma amostra particular tem maior probabilidade de expressar um \\(\\overline{X}\\) próximo ao valor populacional. 13.4.3 Precisão: o erro padrão da média (\\(\\sigma^{\\overline{X}}\\)) Suponha agora que tomemos ao acaso amostras com \\(n = 7\\) desta mesma população. Existem ao todo: \\[{{10}\\choose{7}} = \\frac{10!}{(10-7)! \\times 7!} = 120\\] amostras diferentes de tamanho \\(n = 7\\) que podem ser retiradas de uma população de tamanho \\(n = 10\\). Se tomarmos estas 120 amostras e calcularmos suas respectivas médias amostrais, teremos os resultados abaixo: 18.6 20.3 20.4 20.6 20.4 20.6 20.7 22.3 22.4 22.6 20.6 20.7 20.9 22.4 22.6 22.7 22.6 22.7 22.9 24.6 21.7 21.9 22.0 23.6 23.7 23.9 23.7 23.9 24.0 25.7 23.9 24.0 24.1 25.9 26.0 22.4 22.6 22.7 24.3 24.4 24.6 24.4 24.6 24.7 26.4 24.6 24.7 24.9 26.6 26.7 25.7 25.9 26.0 27.7 27.9 28.0 23.0 23.1 23.3 24.9 25.0 25.1 25.0 25.1 25.3 27.0 25.1 25.3 25.4 27.1 27.3 26.3 26.4 26.6 28.3 28.4 28.6 27.0 27.1 27.3 29.0 29.1 29.3 30.4 24.0 24.1 24.3 25.9 26.0 26.1 26.0 26.1 26.3 28.0 26.1 26.3 26.4 28.1 28.3 27.3 27.4 27.6 29.3 29.4 29.6 28.0 28.1 28.3 30.0 30.1 30.3 31.4 28.6 28.7 28.9 30.6 30.7 30.9 32.0 32.7 Se compararmos os histogramas com \\(n = 5\\) e \\(n = 7\\), veremos que os dois resultam em estimadores acurados, pois \\(mu_{\\overline{X}} = \\mu\\). No entando, o intervalo de variação é menor para amostras de tamanho \\(n = 7\\). Para esta figura, os valores estão mais concentrados ao redor da média. Portanto, à medida que aumenta o tamanhol amostral, diminui a dispersão das médias amostrais ao redor de \\(\\mu\\). Assim, para amostras grandes torna-se mais improvável obter uma média amostral distante da média populacional. Dizemos então que Conforme aumenta o tamanho amostral, conseguimos estimativas mais precisas. A precisão de um estimador pode ser medida pelo Erro padrão da média (\\(\\sigma_{\\overline{X}}\\)) que pode ser calculado por: \\[\\sigma_{\\overline{X}} = \\frac{\\sigma}{\\sqrt{n}}\\] O erro padrão da média é o desvio padrão de todas as médias amostrais que poderiam ser obtidas de uma amostra com tamanho \\(n\\). Para nosso exemplo com \\(n = 5\\), \\(\\sigma_{\\overline{X}}\\) = 5.93, enquanto para \\(n = 7\\), \\(\\sigma_{\\overline{X}}\\) = 5.01. Dizemos que o último exemplo provê estimativas mais precisas. IMPORTANTE! Na prática científica não conhecemos o desvio padrão populacional \\(\\sigma\\) e, consequentemente, não temos obter o erro padrão populacional \\(\\sigma_{\\overline{X}}\\). No entanto, dado que temos uma amostra particular, podemos estimá-lo a partir do desvio padrão amostral \\(\\sigma_{\\overline{X}}\\) pela expressão: \\[s_{\\overline{X}} = \\frac{s}{\\sqrt{n}}\\] em que \\(s_{\\overline{X}}\\) é denominado de erro padrão amostral "],["normdist.html", "Capítulo 14 O modelo da distribuição normal 14.1 O modelo normal de probabilidades 14.2 Entendendo a função normal de densidade de probabilidade 14.3 Cálculo de probabilidade com a função normal de densidade 14.4 A distribuição normal padronizada 14.5 Exercícios resolvidos 14.6 Exercícios propostos", " Capítulo 14 O modelo da distribuição normal As técnicas de estatística descritiva apresentadas anteriormente (capítulos 3 a 11) nos permitem entender os padrões resultantes de fenômenos que já aconteceram. Seria interessante no entanto, se pudessemos utilizar estes padrões para fazer predições sobre o que poderá acontecer. Em estatística, a predição se torna possível pelo uso de modelos probabilísticos, dentre os quais a distribuição normal é um dos mais importantes. Modelos probabilísticos são definidos por funções de probabilidade. A variável envolvida neste modelo é denominada de variável aleatória (capítulos 30 e 31). Uma variável aleatória resulta de um experimento aleatório, que nos provê sobre algum fenômeno de interesse. Neste sentido, medir a altura de um aluno, tomar a temperatura de uma cidade, medir a taxa de crescimento de uma bactéria, são todos experimentos aleatórios. A questão relevante nestes experimentos é que antes de ser finalizado, não temos certeza sobre qual será seu resultado. Desta forma, embora não saibamos qual será o resultado exato do experimento, podemos nos basear em algum modelo probabilidades para prever a chance deste resultado estar dentro de determinados limites. O papel de um modelo probabilísticos é portanto, delimitar a incerteza ao redor dos resultados possíveis de um experimento aleatório. Ao medir a altura de um aluno, podemos supor que existe grande probabilidade desta ficar abaixo de \\(1,9\\) m. Supomos isto pois temos conhecimento de que a altura de maior parte das pessoas está abaixo deste limite. Entretanto, se quisermos atribuir um valor de probabilidade a esta suposição devemos: Assumir que a variável altura segue um determinado modelo de probabilidades, e Utilizar dados de um experimento para estimar os parâmetros deste modelo a fim de calcularmos a probabilidade \\(P(X \\le 1,9)\\). Discutiremos isto em detalhes no capítulo 30 para diferentes modelos de probabilidades. Neste capítulo iremos discutir pela primeira vez o modelo de distribuição normal. A distribuição normal de probabilidades descreve uma curva em forma de sino também chamada de distribuição gaussiana. Um dos motivos que tornam a distribuição normal central em estatística é a percepção de que o comportamento de muitos fenômenos naturais podem ser descritos adequadamente por este modelo teórico. Veja por exemplo, o histograma de alturas de 136 alunos da turma de Introdução a Estatística de \\(2019\\) do curso de Bacharelado Interdisciplinar em Ciências do Mar (UNIFESP). A linha sobre este histograma representa a distribuição normal teórica. À direita desta figura está um histograma da temperatura média anual em uma cidade americana, onde também foi sobreposta uma curva normal teórica. Embora estes dados descrevam fenômenos completamente distintos, a distribuição normal se adequa razoavelmente bem aos dois histogramas. Figure 14.1: Altura (m) de alunos de um curso de estatística e temperatura média anual de uma cidade americana O segundo motivo que torna a distribuição normal uma das mais importantes em estatística, é que ela surge como o modelo esperado para a distribuição das médias amostrais sob determinadas condições (Capítulo 15), o que nos permite utilizar uma variedade de procedimentos analíticos no campo da inferência e testes de hipótese. 14.1 O modelo normal de probabilidades O modelo normal de probabilidades é uma função matemática dada por: \\(f(x) = \\frac{1}{\\sqrt(2\\pi\\sigma^2)}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}\\), \\(x \\in \\mathbb{R} | -\\infty \\le y \\le +\\infty\\) A expressão envolve as quantias \\(\\mu\\) e \\(\\sigma\\), definidas como os parâmetros da distribuição que representam respectivamente, sua média e o desvio padrão. Para dizer que uma variável aleatória \\(X\\) tem distribuição normal por meio da expressão: \\(X \\sim \\mathcal{N}(\\mu,\\,\\sigma^2)\\) Esta expressão diz de \\(X\\) é normalmente distribuída (\\(\\mathcal{N}\\)) e que esta distribuição tem parâmetros \\(\\mu\\) e \\(\\sigma\\). A média de uma distribuição normal é o ponto central da curva e o desvio padrão mede o espalhamento das observações ao redor de \\(\\mu\\). Em um fenômeno descrito por valores baixos de \\(\\sigma\\), a maioria das observações estará próxima a \\(\\mu\\), enquanto para valores altos de \\(\\sigma\\) as observações estarão mais distantes de \\(\\mu\\). Deste modo, podemos alterar o formato da distribuição normal alterando seu parâmetos de posição (média - \\(\\mu\\)) e de dispersão (desvio padrão - \\(\\sigma\\)). Se as mensurações sobre um determinado fenômeno apresentarem um padrão em forma de sino, podemos buscar a melhor combinação de \\(\\mu\\) e \\(\\sigma\\) e descrever o fenômeno por meio de um modelo normal. Ao fazer isto, poderemos utilizar este modelo para entender quais são as probabilidade de eventos futuros estarem em diferentes faixas de valores. No caso das alturas dos alunos por exemplo, vemos que a probabilidade de um aluno ter mais de \\(2\\) metros ou menos de \\(1,5\\)m é extremamente baixa. 14.2 Entendendo a função normal de densidade de probabilidade A função \\(f(x) = \\frac{1}{\\sqrt(2\\pi\\sigma^2)}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}\\) é uma função de densidade de probabilidade. Antes de aplicar esta distribuição para encontrar valores de probabilidade, vamos aprender simplesmente para descrever a funções de densidade assumindo valores particulares de \\(\\mu\\) e \\(\\sigma\\). Para isto, vamos tentar mimetizar o histograma de altura dos alunos do BICT Mar mostrado anteriormente. Vamos assumir da distribuição normal de altura sejam: \\(\\mu = 1.7\\) \\(\\sigma = 0.11\\) Seja uma determinada altura \\(x = 1.6\\)m. Neste ponto, a \\(f(x)\\) assume o valor: \\(f(1.6) = \\frac{1}{\\sqrt(2\\pi \\times0.11^2)}e^{-\\frac{1}{2}(\\frac{1.6 - 1.7}{0.11})^2} = 2.399\\) Este resultado corresponde ao ponto em \\(x\\) no gráfico da distribuição normal de densidade de probabilidade. Podemos encontar \\(f(x)\\) para quaisquer valores dentro dos reais \\(\\mathbb{R}\\) entre \\(-\\infty\\) e \\(+\\infty\\). Assim, se calcularmos \\(f(x)\\) para diferentes pontos em \\(x\\) teremos um esboço da função de densidade normal. Na figura abaixo, por exemplo, apresentamos \\(f(x)\\) para os valores: \\(X = 1.4, 1.45, 1.5, 1.55, 1.6, 1.65, 1.7, 1.75, 1.8, 1.85, 1.9, 1.95, 2\\) assumindo \\(\\mu = 1.7\\) e \\(\\sigma = 0.11\\) 14.2.1 Calculando de \\(f(x)\\) no R: a função dnorm() No R, os resultados acima podem ser obtidos com a função dnorm(), que fornece um modo simples para calcularmos \\(f(x)\\) na distribuição normal. Nesta função a letra ‘d’ vem de densidade da distribuição normal. Para encontrar \\(f(x)\\) para um dado valor fazemos simplesmente: mu &lt;- 1.7 dp &lt;- 0.11 dnorm(1.5, mean = mu, sd = dp) ## [1] 0.6945048 Se quisermos obter \\(f(x)\\) para múltiplos valores de \\(x\\) podemos fazer: x &lt;- c(1.4, 1.5, 1.6, 1.7) dnorm(x, mean = mu, sd = dp) ## [1] 0.0879777 0.6945048 2.3991470 3.6267480 14.3 Cálculo de probabilidade com a função normal de densidade Encontrar a probabilidade de uma variável aleatória \\(X\\) estar dentro de uma deteminada faixa de valores significa fazer predições a respeito da probabilidade de ocorrência de uma observação futura. Por ser uma função de probabilidade, a área abaixo de \\(f(x)\\) na distribuição normal soma \\(1\\). \\[P(-\\infty \\le X \\le +\\infty) = \\int_{-\\infty}^{+\\infty}f(x) dx = 1\\] Assim, se desejamos obter probabilidade de uma variável estar dentro de um determinado limite, devemos calcular a área abaixo da curva para este limite. Por exemplo, a probabilidade de uma observação em \\(X\\) estar entre \\(x_1\\) e \\(x_2\\) será: Você lembra-se que a área abaixo de uma função matemática é dada pela integral definida desta função. \\[P(x_1 \\le X \\le x_2) = \\int_{x_1}^{x_2}f(x) dx\\] 14.3.1 Calculando probabilidades no R: a função pnorm() Usando o R, a probabilidade de amostrarmos um aluno que tenha entre menos de \\(1.5\\) metros pode ser obtida por meio da função pnorm: mu &lt;- 1.7 dp &lt;- 0.11 pnorm(q = 1.5, mean = mu, sd = dp, lower.tail = TRUE) ## [1] 0.03451817 Os argumentos desta função são (veja o menu de ajuda digitando ?pnorm no Console do R): q: o valor de \\(x\\) mean: média \\(\\mu\\) da função normal sd: desvio padrão \\(\\sigma\\) da função normal lower.tail: se a função irá retornar a probabilidade abaixo (TRUE) ou acima (FALSE) de q Se quisermos encontrar a probabilidade \\(P(X \\ge 1.5)\\) alteramos o parâmetro lower.tail pnorm(q = 1.5, mean = mu, sd = dp, lower.tail = FALSE) ## [1] 0.9654818 Se desejamos obter a probabilidade de \\(x\\) estar entre \\(1.5\\)m e \\(1.7\\)m podemos fazer: \\[P(1.5 \\le X \\le 1.7) = P(X \\le 1.7) - P(X \\le 1.5)\\] No R temos: p1 &lt;- pnorm(q = 1.7, mean = mu, sd = dp, lower.tail = TRUE) p2 &lt;- pnorm(q = 1.5, mean = mu, sd = dp, lower.tail = TRUE) pfinal &lt;- p1 - p2 pfinal ## [1] 0.4654818 ou simplesmente: diff(pnorm(q = c(1.7, 1.5), mean = mu, sd = dp, lower.tail = TRUE) ) ## [1] -0.4654818 Aqui estão representados cada um dos intervalos calculados. 14.4 A distribuição normal padronizada A integral para a função normal é difícil de ser calculada pois não tem solução analítica. Isto era um problema para os cientistas até meados do século \\(XX\\) que precisavam calcular valores de probabilidades para diferentes combinações de \\(\\mu\\) e \\(\\sigma\\). Naquele momento, a solução para facilitar a vida dos pesquisadores foi criar uma tabela descrevendo estas probabilidades em uma distribuição normal padronizada, ou seja para valores particulares de \\(\\mu\\) e \\(\\sigma\\). Padronizar aqui, significa transfomar cada valor \\(x_i\\) de modo que as observações resultantes tenham média igual a \\(0\\) e desvio padrão igual a \\(1\\). Esta transformação é apicada a cada observação \\(x_i\\), obtendo-sem um valor de \\(z_i\\) correspondente por meio da expressão. \\[z_i = \\frac{x_i - \\mu}{\\sigma}\\] Lembre-se que no capítulo 8 apresentamos \\(z_i\\) como uma medida de posição que representava uma medida relativa à média e ao desvio padrão de um conjunto de dados particular. Por exemplo, um valor de \\(z_i = 2\\) significa que a observação original \\(x_i\\) está \\(2\\) desvios padrões acima de sua respectiva média \\(\\mu\\). A transformação \\(Z\\) é útil, pois ainda que seja difícil calcular as probabilidades para uma variável aleatória \\(X\\), após a transformação teremos uma variável \\(Z\\) para a qual os valores de probabilidade estão tabelados. Deste modo, \\(Z\\) é uma variável aleatória com \\(\\overline{z} = 0\\) e \\(s = 1\\) tal que: \\[Z \\sim \\mathcal{N}(0,\\,1)\\] Após a transformação \\(Z\\) nos exemplos sobre altura dos alunos e chuva mensal temos: IMPORTANTE! Antes de continuar, releita o tópico “Interpretando o valor de Z” no capítulo 8. 14.4.1 Probabilidades em uma distribuição normal padronizada Nos dois exemplos anteriores, verifica-se que todas as observações estão situadas, aproximadamente, entre \\(z = -3\\) e \\(z = +3\\). De fato, a distribuição normal padronizada ou distribuição Z tem propriedades bem conhecidas. Como sua média é \\(\\mu = 0\\) e seu desvio padrão é \\(\\sigma = 1\\), a maior parte das observações fica limitada entre \\(z = -3\\) e \\(z = +3\\). Para ser exato, podemos descrever as probabilidades de uma observação estar dentro de alguns limites conhecidos. Por exemplo, \\(95\\%\\) das observações estará entre \\(z = -1.96\\) e \\(z = +1.96\\), isto é, \\[P(-1.96 \\le Z \\le +1.96) = 0.95\\]. De forma similar, \\(90\\%\\) da área central da curva se encontra entre \\(z = -1.64\\) e \\(z = +1.64\\). Estes e outros limites na distribuição normal padronizada podem ser verificados na figura abaixo. Vamos exemplificar o uso da distribuição \\(Z\\) no cálculo de probabilidades utilizando os dados de altura dos alunos. Para estes dados, iremos encontrar \\(P(X \\le 1.5)\\). Este procedimento consiste de: Transformar \\(x = 1.5\\) em \\(z_{1.5}\\) por meio de \\(z_{1.5} = \\frac{1.5 - 1.7}{0.11} = -1.818\\); mu &lt;- 1.7 dp &lt;- 0.11 x &lt;- 1.5 z_1.5 &lt;- (x - mu)/dp z_1.5 ## [1] -1.818182 Encontrar encontrar \\(P(Z \\le z_{1.5}) = P(Z \\le -1.818)\\). pnorm(q = z_1.5, mean = 0, sd = 1, lower.tail = TRUE) ## [1] 0.03451817 Compare este resultado com o obtido anteriormente para verificar que é equivalente a \\(P(X \\le 1.5)\\). 14.4.1.1 A transformação \\(Z\\) Do mesmo modo, suponha uma variável aleatória \\(X\\) nomalmente distribuída conforme \\(X \\sim \\mathcal{N}(\\mu,\\,\\sigma^2)\\). Desejamos encontrar \\(m\\) tal que: \\(P(X \\le m) = \\alpha\\) \\(\\alpha\\) aqui representa um valor de probabilidade qualquer determinada pela área na distribuição normal abaixo de \\(m\\). Ao aplicar a transformação \\(Z\\) teremos: \\(P(\\frac{X - \\mu}{\\sigma} \\le \\frac{m - \\mu}{\\sigma}) = \\alpha\\) como \\(\\frac{X - \\mu}{\\sigma} = Z\\) temos que: \\(P(Z \\le \\frac{m - \\mu}{\\sigma}) = \\alpha\\) Por meio desta expressão, você pode encontar \\(m\\) uma vez fornecido \\(\\alpha\\) ou encontrar \\(\\alpha\\), desde que seja fornecido \\(m\\). O mesmo vale se quisermos encontrar a probabilidade determinada por um intervalo definido de \\(m\\) até \\(n\\) (\\(m &lt; n\\)). Para isto fazemos: \\(P(m \\le X \\le n) = \\alpha\\) \\(P(\\frac{m - \\mu}{\\sigma} \\le \\frac{X - \\mu}{\\sigma} \\le \\frac{n - \\mu}{\\sigma}) = \\alpha\\) \\(P(\\frac{m - \\mu}{\\sigma} \\le Z \\le \\frac{n - \\mu}{\\sigma}) = \\alpha\\) 14.4.2 Tabela \\(Z\\) Ao utilizarmos um software estatístico não é necessário fazer esta transformação. A transformação \\(Z\\) era necessária na ausência de ferramentas computacionais, ou seja, quando a única opção era utilizarmos a Tabela \\(Z\\) para evitar cálculos tediosos considerando cada combinação de \\(\\mu\\) e \\(\\sigma\\). A tabela \\(Z\\) disponibiliza os valores de probabilidade para um grande número de valores e é apresentada na grande maioria dos livros de estatística. Você pode utilizar a Tabela \\(Z\\) para encontrar \\(P(X \\le 1.5)\\). Note que o valor transformado é \\(z_{1.5} = -1.818\\). Este será o valor que iremos buscar na tabela. Para isto: Encontre a página que oferece valores negativos, uma vez que \\(z_{1.5} &lt; 0\\); Na coluna 1 desta página (coluna z) encontre a linha -1.8 que refere-se à unidade, e à primeira casa decimal de \\(z_{1.5}\\); Encontre a coluna 0.02 (quarta coluna da tabela \\(Z\\)) que apresenta a segunda casa decimal de \\(z_{1.5}\\). Isto nos leva ao valor mais próximo do calculado (\\(z_{1.5} = -1.818\\)). Cruze a linha escolhida no item 3 com a coluna escolhida no item 4. Você irá encontrar o valor \\(0,0344\\). Este valor e a probabilidade de obtermos um valor de \\(z \\le 1.5\\) na distribuição normal padronizada, ou seja, \\(P(Z \\le z_{1.5})\\). A diferença entre este valor e o encontrado com o R se deve unicamente à limitação da precisão utilizando a Tabela \\(Z\\). 14.5 Exercícios resolvidos 14.5.1 Distribuição de comprimento As comunidades de peixes em riachos de cabeceira são compostas por espécies de pequeno porte. Rhamdioglanis transfasciatus é uma destas espécies, desconhecida do público em geral, porém muito abundante em pequenos riachos bem preservados. Dados de captura sugerem que o tamanho dos indivíduos pode ser razoavelmente bem descrito por um modelo de distribuição normal. Suponha o comprimento desta espécie tenha uma distribuição normal com \\(\\mu = 10\\) cm e \\(\\sigma = 3\\) cm. Encontre: A probabilidade de capturar um indivíduo maior de 14 cm de comprimento, \\(P(X \\ge 14)\\). A probabilidade de capturar um indivíduo menor de 5 cm de comprimento, \\(P(X \\le 5)\\). A probabilidade de encontrar um indivíduo entre 5 e 14 cm, \\(P(5 \\le X \\le 14)\\). Se um trecho de riacho contém 800 indivíduos, quantos são maiores que 14 cm de comprimento. RESOLUÇÃO Para resolver estas questões iremos utilizar a transformação \\(Z\\). i. Para encontrar \\(P(X \\ge 14)\\): Vamos encontrar o respectivo valor de \\(Z\\) pela transformação \\(z_{14} = \\frac{14 - 10}{3} = 1.33\\) Na tabela \\(Z\\) procuramos a linha que mostra a unidade e \\(1^a\\) casa decimal de \\(1.33\\) e em seguida encontramos a coluna que representa a \\(2^a\\) casa decimal de \\(1.33\\). Cruzando linha e coluna encontramos o valor \\(0,9082\\). Note que este valor representa a área abaixo de 1.33, isto é, \\(P(Z \\le z_{14})\\). No entanto, queremos \\(P(Z \\ge z_{14})\\) que representa a área da curva acima de \\(1.33\\). Para isto basta fazermos \\(1 - 0,9082\\). Deste modo, \\(P(Z \\ge z_{14}) = 1 - P(Z \\le z_{14}) = 1 - 0,9082 = 0.0918\\) ii. Para encontrar \\(P(X \\le 5)\\): \\(z_{5} = \\frac{5 - 10}{3} = -1.67\\) Na tabela \\(Z\\) procuramos a linha que mostra a unidade e \\(1^a\\) casa decimal de \\(-1.67\\) e em seguida encontramos a coluna que representa a \\(2^a\\) casa decimal de \\(-1.67\\). Cruzando linha e coluna encontramos o valor \\(0,0475\\) que representa a área desejada. Deste modo, \\(P(X \\le 5) = P(Z \\le z_{5}) = 0,0475\\) iii. Para encontrar a área central da curva \\(P(5 \\le X \\le 14)\\): Vamos subtrair as quantias \\(P(Z \\le 14) - P(Z \\le 5)\\) Estes valores já foram encontrados nos itens anteriores, de modo que basta fazermos: \\(P(5 \\le X \\le 14) = 0,9082 - 0,0475 = 0.8607\\) iv. Indivíduos maiores que 14 cm de comprimento Se a proporção de indivíduos acima de 14 é \\(P(X &gt; 14) = 0.0918\\) e a população tem \\(N = 800\\) indivíduos, teremos: \\(0.0918 \\times 800 = 73\\) indivíduos maiores que 14 cm. RESOLUÇÃO no R O exercício pode ser resolvido pelo R por meio da função pnorm. mu &lt;- 10 sigma &lt;- 3 N &lt;- 800 la &lt;- 14 lb &lt;- 5 i. \\(P(Z \\ge 14)\\) pnorm(q = la, mean = mu, sd = sigma, lower.tail = FALSE) ## [1] 0.09121122 ii. \\(P(Z \\le 5)\\) pnorm(q = lb, mean = mu, sd = sigma, lower.tail = TRUE) ## [1] 0.04779035 iii. \\(P(5 \\le X \\le 14)\\) diff( pnorm(q = c(lb, la), mean = mu, sd = sigma, lower.tail = TRUE) ) ## [1] 0.8609984 iv. Número de indivíduos maiores que \\(14\\) cm de comprimento pg_la &lt;- pnorm(q = la, mean = mu, sd = sigma, lower.tail = FALSE) N * pg_la ## [1] 72.96898 14.5.2 Intervalos em uma distribuição normal Suponha variável aleatória \\(X\\) normalmente distribuída conforme com \\(\\mu = 50\\) e \\(\\sigma = 10\\). Encontre: O valor de \\(a\\) tal que \\(P(X \\le a) = 0,10\\). O valor de \\(b\\) tal que \\(P(X \\ge b) = 0,85\\). O intervalo simétrico ao redor da média delimitado por \\(c\\) e \\(d\\) (\\(c &lt; d\\)), que contém \\(95\\%\\) da área sob a curva. O valor de \\(e\\) tal que \\(P(50-e \\le X \\le 50+e) = 0.99\\) RESOLUÇÃO Veja que neste exercício, foram oferecidos valores de probabilidades e solicitado que você obtivesse os limites em uma distribuição normal específica. Este processo é oposto ao do excercício anterior. i. O valor de \\(a\\) Se \\(P(X \\le a) = 0,10\\), a área da curva abaixo de \\(a\\) é \\(0,10\\). Procurando por este valor na tabela \\(Z\\) vemos que o valor mais próximo é \\(0,1003\\) que corresponde a um escore \\(z = -1,28\\). Vamos utilizar este valor para encontrar sua correspondência para a variável aleatória \\(X\\) que tem média \\(\\mu = 50\\) e desvio padrão \\(\\sigma = 10\\). \\(z = \\frac{a - \\mu}{\\sigma} :: -1,28 = \\frac{a - 50}{10}\\) \\(a = (-1,28 \\times 10) + 50 = 37.2\\) ii. O valor de \\(b\\) Se \\(P(X \\ge b) = 0,85\\), a área abaixo de \\(b\\) que devemos encontrar na tabela \\(Z\\) é \\(1 - 0,85 = 0.15\\). Vemos que o valor mais próximo é \\(0,1492\\) que corresponde a \\(z = -1,04\\). Ao utilizar este resultado na expressão abaixo temos: \\(z = \\frac{b - \\mu}{\\sigma} :: -1,04 = \\frac{b - 50}{10}\\) \\(b = (-1,04 \\times 10) + 50 = 39.6\\) iii. O intervalo simétrico ao redor da média delimitado por \\(c\\) e \\(d\\) (\\(c &lt; d\\)), que contém \\(95\\%\\) da área sob a curva. Se entre \\(c\\) e \\(d\\) está \\(95\\%\\) da área da curva, temos uma área de \\(1 - 0,95 = 0,05\\) fora da curva. Como o intervalo é simétrico, teremos \\(0,025\\) abaixo de \\(c\\) e \\(0,025\\) acima de \\(d\\). Ao procurar na tabela \\(Z\\) por \\(0,025\\) encontraremos \\(z = -1,96\\) que equivale ena distribuição de X a: \\(z = \\frac{c - \\mu}{\\sigma} :: -1,96 = \\frac{c - 50}{10}\\) \\(c = (-1,96 \\times 10) + 50 = 30.4\\) Novamente, como o intervalo é simétrico e a dsitribuição de \\(Z\\) é centrada em zero, o ponto \\(d\\) será de +\\(1,96\\) que resulta em: \\(z = \\frac{d - \\mu}{\\sigma} :: +1,96 = \\frac{d - 50}{10}\\) \\(d = (+1,96 \\times 10) + 50 = 69.6\\) iv. O valor de \\(e\\) tal que \\(P(50-e \\le X \\le 50+e) = 0.99\\) Podemos fazer aqui: \\(P(50-e \\le X \\le 50+e) = P(\\frac{50-e - \\mu}{\\sigma} \\le \\frac{X-\\mu}{\\sigma} \\le \\frac{50+e-\\mu}{\\sigma}) = 0.99\\) como \\(\\mu = 50\\) e \\(\\sigma = 10\\) temos: \\(P(\\frac{-e}{10} \\le Z \\le \\frac{e}{10}) = 0.90\\) Como a área central ocupa \\(0,99\\) da distribuição, restam \\(0,005\\) na cauda superior e \\(0,005\\) na cauda inferior: Para encontrar \\(-e\\) buscamos por \\(0,005\\) na tabela \\(Z\\) e encontramos \\(0,0051\\) como valor mais próximo, referente a \\(z_{-e} = -2,57\\). Substituindo na equação temos: \\(\\frac{-e}{10} \\le -2,57 :: -e = -2,57 \\times 10 :: e = 25,7\\) *Note na figura acima que os limite das áreas em azul são: \\(\\mu - e = 50 - 25.7 = 24.3\\) e \\(\\mu - e = 50 + 25.7 = 75.7\\) RESOLUÇÃO no R O exercício pode ser resolvido pelo R por meio da função qnorm. Em qnorm, o ‘q’ vem de quantis da distribuição normal. mu = 50 sigma = 10 (a &lt;- qnorm(p = 0.10, mean = mu, sd = sigma, lower.tail = TRUE)) ## [1] 37.18448 (b &lt;- qnorm(p = 1-0.85, mean = mu, sd = sigma, lower.tail = TRUE)) ## [1] 39.63567 (c &lt;- qnorm(p = (1-0.95)/2, mean = mu, sd = sigma, lower.tail = TRUE)) ## [1] 30.40036 (d &lt;- qnorm(p = (1-0.95)/2, mean = mu, sd = sigma, lower.tail = FALSE)) ## [1] 69.59964 (e &lt;- -qnorm(p = (1-0.99)/2, mean = mu, sd = sigma, lower.tail = TRUE) + 50) ## [1] 25.75829 14.5.3 Quantos desvios padrões? Suponha uma variável aleatória normalmente distribuída representada por \\(X \\sim \\mathcal{N}(\\mu,\\,\\sigma^2)\\), determine: O valor de \\(a\\) tal que \\(P(X &lt; a) = 0,20\\). \\(P(X \\le \\mu + 2\\sigma)\\). O valor de \\(c\\) tal que \\(P(\\mu -c\\sigma \\le X \\le \\mu +c\\sigma) = 0.99\\) RESOLUÇÃO i. O valor de \\(a\\) tal que \\(P(X &lt; a) = 0,20\\). \\(P(X &lt; a) = P(\\frac{X - \\mu}{\\sigma} &lt; \\frac{a - \\mu}{\\sigma}) = P(Z &lt; \\frac{a - \\mu}{\\sigma}) = 0,20\\) Procurando pelo valor de \\(z\\) que delimita \\(0,20\\) da área abaixo de \\(a\\) encontramos por \\(z = -0,84\\), de modo que: \\(-0,84 = \\frac{a - \\mu}{\\sigma}\\) \\(a = \\mu -0,84\\sigma\\) ii. \\(P(X \\le \\mu + 2\\sigma)\\) A expressão \\(\\mu + 2\\sigma\\) nos diz que o limite de interesse está \\(2\\) desvios padrões acima de \\(\\mu\\). Ao procurar pelo valor de \\(z = 2,0\\) na tabela \\(Z\\), veremos que a probabilidade de interesse é \\(P(X \\le \\mu + 2\\sigma) = 0,9772\\) iii. O valor de \\(c\\) tal que \\(P(\\mu -c\\sigma \\le X \\le \\mu +c\\sigma) = 0.99\\) Desenvolvendo esta expressão teremos \\(P(-c \\le \\frac{X - \\mu}{\\sigma} \\le +c) = P(-c \\le Z \\le +c) = 0.99\\) Fora deste intervalo simétrico, teremos uma área de \\(0,005\\) na cauda inferior e \\(0,005\\) na cauda superior da distribuição \\(Z\\). Ao procurar por \\(0,005\\) na tabela \\(Z\\) encontramos \\(z = -2,57\\), de modo que \\(c = 2,57\\). 14.6 Exercícios propostos Leia o tópico 7.4.2 O Modelo Normal em (Bussab and Morettin 2010) (pag. 176 a 181) e faça os exercícios 14 a 20 da página 184. References "],["tcl.html", "Capítulo 15 Distribuição das médias amostrais 15.1 Teorema Central do Limite 15.2 Exercícios resolvidos: 15.3 Exercícios propostos", " Capítulo 15 Distribuição das médias amostrais Vamos retomar algumas ideias discutidas nos capítulos 12 e 13, quando apresentamos a distribuição das médias amostrais e o ciclo de amostragem \\(\\Rightarrow\\) inferência estatística. Ao amostrar uma população estatística por meio de um experimento, seremos capazes de calcular estatísticas descritivas desta população. A média amostral \\(\\overline{X}\\) é uma destas estimativas, mas a mesma ideia vale para qualquer outra estatística \\(\\theta\\). Figure 15.1: Processo de amostragem e inferência estatística Neste processo, o resultado de um experimento pode ser visto como uma observação particular de uma população de experimentos que podem ser reproduzidos sob as mesmas condições. A estimativa obtida deste experimento é portanto, somente uma entre uma população de estimativas que o experimento pode gerar. A inferência estatística é possível se pudermos entender o que é esperado como resultados possíveis desta população de experimentos. Figure 15.2: Distribuição das médias amostrais provinientes de um experimento 15.1 Teorema Central do Limite No capítulo 13 apresentamos a distribuição de médias como uma distribuição normal, centrada na média populacional \\(\\mu\\) e com desvio padrão igual a \\(\\sigma_{\\mu} = \\frac{\\sigma}{\\sqrt{n}}\\). Este resultado é previsto pelo Teorema Central do Limite (TCL) que fornece um modelo teórico para o comportamento esperado da média amostral de um experimento. DEFINIÇÃO DO TCL Seja uma população estatística com média \\(\\mu\\) e desvio padrão \\(\\sigma\\). A distribuição das médias amostrais desta população tenderá a apresentar uma distribuição normal de probabilidades com média \\(\\mu\\) e desvio padrão \\(\\frac{\\sigma}{\\sqrt(n)}\\) à medida que o tamanho amostral \\(n\\) aumenta, ainda que a distribuição das observações originais não possua uma distribuição normal. Segundo o TCL, as médias amostrais \\(\\overline{X}\\) de um experimento distribuem-se como: \\[\\overline{X} \\sim \\mathcal{N}(\\mu_{\\overline{X}},\\,\\sigma^{2}_{\\overline{X}})\\] em que \\(\\mu_{\\overline{X}} = \\mu\\) \\(\\sigma^{2}_{\\overline{X}} = \\frac{\\sigma^2}{n}\\) Note que a variância de \\(\\overline{X}\\) depende do tamanho amostral \\(n\\). Isto justifica o que será discutido no tópico Introdução à suficiência amostral do capítulo 16. 15.1.1 Probabilidades na amostra original e na distribuição de médias Seja uma variável \\(X\\) qualquer com \\(\\mu = 50\\) e \\(\\sigma = 10\\). As figuras abaixo comparam as probabilidades acima de \\(x_1 = 55\\) para as observações originais e para as distribuições de médias amostrais de tamanho \\(n_1 = 2\\) e \\(n_2 = 10\\). Note que existe uma probabilidade razoável de que uma determinada observação em \\(X\\) esteja acima de \\(55\\), \\(P(X \\leq 55) = 0.309\\). No entando se tomarmos ao acaso uma amostra de tamanho \\(n_1 = 2\\), a probabilidade de que a média destas duas amostras esteja acima de \\(55\\) diminui para \\(P(\\overline{X} \\leq 55) = 0.24\\). Se tormarmos uma amostra ainda maior (\\(n_2 = 10\\)), a probabilidade se reduz ainda mais para \\(P(\\overline{X} \\leq 55) = 0.057\\). Vemos portanto, como mencionado no capítulo 13, que a precisão de um experimento aumenta à medida que aumentamos o tamanho amostral, pois para amostras grandes, a probabilidade de obtermos um \\(\\overline{X}\\) distante de \\(\\mu\\) torna-se cada vez menor. 15.1.2 Distribuições não-normais O TCL é válido inclusive para distribuições não-normais. Isto torna a distribuição normal uma das mais importantes em inferência estatística, pois ainda que o resultado de um experimento particular seja descrito por qualquer outro modelo de probabilidades, as médias das amostras deste experimento seguirão uma distribuição normal, à medida que \\(n\\) aumenta. Isto justifica muitos dos processos de análise e inferência estatística que serão descritos nos capítulos posteriores. A figura abaixo por exemplo, simula a distribuição de médias amostrais para variáveis com diferentes distribuições de probabiidades e tamanhos crescentes de \\(n\\). Podemos observar que independente do formato da distribuição original, a distribuição das médias amostrais tende à normalidade. O padrão normal aparece mais rápido se a distribuição original é simétrica. Por outro lado, para populações estatísticas com distribuições assimétricas, será necessário um tamanho amostral maior para que se alcance a normalidade. 15.2 Exercícios resolvidos: 15.2.1 Tamanho médio de robalos no mercado de peixes Em 2014 no estuário do rio Itanhaém - SP foi pescado o “maior robalo já encontrado” (G1 Santos). O peixe tinha \\(133\\) cm e \\(27,8\\) kg . Em 2018 em Bertioga, também no litoral de SP “Robalo ‘gigante’ quebra recordes e vira atração” (G1 Santos) pesando \\(33\\) kg. Em “uma das salas da Colônia de Pesca Z2 de Atafona” RJ está uma imagem de um robalo de \\(28\\) kg capturado muitas décadas atrás (Ambiente Cult). Estas capturas viram notícias pois são certamente inusitadas. Dados de desembarque sugerem que a distribuição de tamanho de robalos comumente capturados está muito abaixo destes limites (Ximenes-Carvalho 2006) ( Acesse aqui o trabalho completo ). Figure 15.3: Dados de desembarque no Mercado de São Pedro (Niterói, RJ). Extraídos de XIMENES-CARVALHO, 2006. Esta distribuição é altamente assimétrica e claramente não-normal. Um dos motivos para este forte grau de assimetria deve-se ao limite inferior de captura. A captura e comercialização de animais muito pequenos é proibida. Suponha que o comprimento de robalos (\\(L\\)) disponíveis para compra tenha média \\(\\mu = 44.1\\) e desvio padrão \\(\\sigma = 13.4\\). Você compra 10 robalos escolhidos ao acaso dos que estão disponíveis. Qual a probabilidade de que: O tamanho médio de uma compra esteja acima de \\(52,4\\) cm, isto é \\(P(\\overline{L} &gt; 52,4)\\)? Em \\(95\\%\\) das vezes que fizer a compra, determine o intervalo simétrico que conterá o tamanho médio dos robalos selecionados, isto é \\(P(a \\le \\overline{L} \\le b) = 0,95\\) Responda novamente aos itens i. e ii. no caso de sua compra constar de \\(4\\) robalos. RESOLUÇÃO Ainda que a distribuição original claramente não siga uma distribuição normal, podemos utilizar o TCL para estimarmos as probabilidades de obter uma média amostral \\(\\overline{X}\\) a determinada distância de \\(\\mu\\). Para isto, no entanto devemos recordar que o desvio padrão das médias amostrais será dado por: \\(\\sigma_{\\overline{X}} = \\frac{\\sigma}{\\sqrt{n}}\\). i. \\(P(\\overline{L} &gt; 52,4)\\) Com base no TCL, uma amostra de \\(n = 10\\) terá média normalmente distribuída com parâmetros \\(\\mu\\) e \\(\\sigma_{\\mu} = \\frac{\\sigma}{\\sqrt{10}}\\). Podemos assim, realizar a transformação \\(Z\\) como segue: \\(Z_{\\overline{L}} = \\frac{\\overline{L} - \\mu}{\\sigma_{\\mu}} = \\frac{\\overline{L} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\) \\(Z_{\\overline{L}} = \\frac{52,4 - 44.1}{\\frac{13.4}{\\sqrt{10}}} = 1.96\\) Note que o denomidador aqui é diferente do que fizemos no capítulo 14, pois aqui estamos falando da distribuição das médias amostrais \\(\\overline{L}\\) e não nas observações individuais \\(L\\). Se buscarmos na Tabela Z, veremos que a área da distribuição normal padronizada abaixo de \\(1.96\\) é de \\(0,975\\). Consequentemente \\(P(\\overline{L} &gt; 52,4) = 1 - 0,975 = 0,025\\) ii. \\(P(a \\le \\overline{L} \\le b) = 0,95\\) Se o intervalo é simétrico e contém \\(0,95\\) das observações, restam \\(0,025\\) em cada uma das caudas. Vimos no item anterior que \\(z = 1,96\\) que delimita \\(0,025\\) da cauda superior. Portanto os limites aqui serão dados por: \\(a = -1.96\\) e \\(b = 1.96\\). Na distribuição original estes limites resultarão em: \\(-1,96 = \\frac{a - 44.1}{\\frac{13.4}{\\sqrt{10}}}:: a = 44.1 -1,96 \\frac{13.4}{\\sqrt{10}} = 35.8\\) cm e \\(+1,96 = \\frac{b - 44.1}{\\frac{13.4}{\\sqrt{10}}}:: b = 44.1 +1,96 \\frac{13.4}{\\sqrt{10}} = 52.4\\) cm Responda novamente aos itens i. e ii. no caso de sua compra constar de \\(4\\) robalos. Aqui você deve repetir exatamente os passos de i. e ii. substituindo \\(n = 10\\) por \\(n = 4\\). **RESOLUÇÃO no R: i. \\(P(\\overline{L} &gt; 52,4)\\) pnorm(52.4, mean = 44.1, sd = 13.4/sqrt(10), lower.tail = FALSE) ## [1] 0.02507255 ii. \\(P(a \\le \\overline{L} \\le b) = 0,95\\) a &lt;- qnorm((1-0.95)/2, mean = 44.1, sd = 13.4/sqrt(10), lower.tail = TRUE) a ## [1] 35.79475 b &lt;- qnorm((1-0.95)/2, mean = 44.1, sd = 13.4/sqrt(10), lower.tail = FALSE) b ## [1] 52.40525 iii. \\(P(\\overline{L} &gt; 52,4)\\) para \\(n = 4\\) pnorm(52.4, mean = 44.1, sd = 13.4/sqrt(4), lower.tail = FALSE) ## [1] 0.1077087 ii. \\(P(a \\le \\overline{L} \\le b) = 0,95\\) para \\(n = 4\\) a &lt;- qnorm((1-0.95)/2, mean = 44.1, sd = 13.4/sqrt(4), lower.tail = TRUE) a ## [1] 30.96824 b &lt;- qnorm((1-0.95)/2, mean = 44.1, sd = 13.4/sqrt(4), lower.tail = FALSE) b ## [1] 57.23176 15.3 Exercícios propostos Leia dos tópicos 10.6 Estatísticas e Parâmetros a 10.8 Distribuição Amostral da Média em (Bussab and Morettin 2010) (pag. 271 a 281) e faça os exercícios 7 a 10 da página 281. References "],["inferenc.html", "Capítulo 16 Estimando a média populacional 16.1 Estimação pontual e estimação intervalar 16.2 Introdução à suficiência amostral", " Capítulo 16 Estimando a média populacional 16.1 Estimação pontual e estimação intervalar A média \\(\\overline{X}\\) obtida a partir de uma determinada amostra varia em função das características das unidades amostrais que foram selecionadas (Capítulo 13). Portanto, \\(\\overline{X}\\) não será igual à média \\(\\mu\\). No entanto, o TLC (Capítulo 15) nos garante que a distribuição esperada das médias amostrais terá uma distribuição normal e que a média das médias (\\(\\mu_{\\overline{X}}\\)) será igual a \\(\\mu\\). Vimos ainda que o desvio padrão da distribuição das médias amostrais (conhecido como erro padrão - \\(\\sigma_{\\overline{X}}\\)) dependerá do tamanho da amostra \\(n\\), de acordo com a expressão: \\[\\sigma_{\\overline{X}} = \\frac{\\sigma}{\\sqrt{n}}\\] Uma vez que não conhecemos \\(\\mu\\), temos que estimá-lo a partir da amostra. Neste caso, \\(\\overline{X}\\) será nossa melhor estimativa da média populacional. Dizemos que \\(\\overline{X}\\) é o estimador pontual de \\(\\mu\\). Como \\(\\overline{X}\\) varia em função de nossa amostra particular, devemos obter além da estimativa pontual, uma estimativa intervalar que nos é fornecida pelo intervalo de confiança. 16.1.1 Intervalo de confiança DEFINIÇÃO DE INTERVALO DE CONFIANÇA É o intervalo de valores associado a um determinado nível de significância (\\(\\alpha\\)). Quando dizemos que um intervalo foi calculado a um nível de confiança de \\(95\\%\\) (\\(1 - \\alpha\\)), estamos dizendo que a probabilidade do IC conter o valor da média populacional \\(\\mu\\) é de \\(95\\%\\). O IC é calculado por: \\[IC_{1-\\alpha} = \\mu \\pm z_{\\alpha/2} \\times \\frac{\\sigma}{\\sqrt{n}}\\] O valor de \\(z_{\\alpha/2}\\) é o valor do índice \\(z\\) associado ao nível de confiança desejado. Se desejamos definir o intervalo de confiança a 95% precisamos garantir que haja uma probabilidade de 95% de que a média amostral esteja ao redor da média populacional. Deste modo, o limite deve excluir 2.5% da porção superior e 2.5% da porção inferior da curva. Para isto, definimos \\(z_{\\alpha/2} = 1.96\\), sendo \\(\\alpha\\) fixado em 0.05. O valor \\(z_{\\alpha/2} = 1.96\\) foi retirado da Tabela \\(Z\\) como o módulo do valor de \\(z\\) que delimida uma área inferior igual a \\(0.025\\). Se queremos um nível de confiança diferente, basta ajustar o valor de \\(\\alpha\\). Por exemplo, se queremos um nível de significância a 99%, fixamos \\(\\alpha\\) em \\(0.01\\) e portanto \\(z = 2.58\\). Da mesma forma, o \\(IC_{90\\%}\\) poderá ser obtido com \\(\\alpha = 0.10\\) e consequentemente \\(z = 1.64\\). Estes e outros limites descrevem as probabilidades em uma distribuição normal padronizada (Capítulo 14), que podem ser obtidos com o uso da maioria dos softwares estatísticos, além de estarem inclusos nas Tabelas \\(Z\\), encontradas na grande maioria dos livros de estatística básica. Os valores de \\(Z\\) são os mesmos discutidos no tópico Medidas de posição quando discutimos os Índice \\(Z\\) (Capítulo 8) Para o cálculo do intervalo de confiança, estamos assumindo que as médias amostrais têm Distribuição Normal com média \\(\\mu\\) e desvio padrão \\(\\frac{\\sigma}{\\sqrt{n}}\\). Fazendo isto, estamos no Teorema Central do Limite (TCL) (Capítulo 15). Geralmente não temos os valores de \\(\\mu\\) e \\(\\sigma\\), de modo que utilizamos os valores de \\(\\overline{X}\\) e \\(s\\) calculados a partir de nossa amostra. Quando as amostras são grandes (\\(n\\ge{30}\\)) não há problema em utilizar o valor de \\(z_{\\alpha/2}\\), e assim: \\[IC_{1-\\alpha} = \\overline{X} \\pm z_{\\alpha/2} \\times \\frac{s}{\\sqrt{n}}\\] 16.1.1.1 Distribuiçao \\(t\\) de Student: \\(\\mu\\) e \\(\\sigma\\) desconhecidos Quando não conhecemos \\(\\mu\\) e \\(\\sigma\\) e as amostras são pequenas (ex. \\(n&lt;30\\)), a dsitribuição normal não é a melhor aproximação para o comportamento das médias amostrais. Nestes casos, substituímos a distribuição de \\(z\\) pela Distribuição \\(t\\) de Student, sendo o intervalo de confiança obtido por: \\[IC_{1-\\alpha} = \\overline{X} \\pm t_{\\alpha/2, gl} \\times \\frac{s}{\\sqrt{n}}\\] Em que \\(\\alpha\\) continua sendo o nível de significância e \\(gl\\) é definido como os graus de liberdade. Neste caso, os graus de liberdade são dados por: \\[gl = n-1\\] O formato da distribuição \\(t\\) de student não é constante. À medida que o tamanho amostral aumenta, o formado da distribuição \\(t\\) converge para a distribuição normal. Isto faz com que na prática raremente se utilize a distribuição \\(Z\\), substituindo-a pela distribuição \\(t\\) de Student. Para amostras pequenas (\\(n = 2\\)) o formato da distribuição de \\(t\\) é distinto da distribuição normal. No entanto, para tamanhos amostrais maiores (\\(n = 30\\)) as o formato da distribuição \\(t\\) tende a a convergir para o mesmo formato a distribuição normal. Esta característica implica que a área a partir de um determinado limite \\(t_i\\) não é constante como na distribuição normal, mas depende do tamanho da amostra, como pode ser visto abaixo. 16.2 Introdução à suficiência amostral Uma decisão central ao planejamento de um experimento é quanto recurso (ex. tempo, dinheiro, mão de obra) devem ser investidos para se obter boas estimativas dos parâmetros populacionais. Por boas estimativas, entendemos amostras precisas, ou seja, que podem ser definida por amostras com baixo erro padrão e acuradas, que em média apontem para o verdadeiro valor do parâmetro. Neste caso, uma das primeiras questões a ser feita é “Qual tamanho amostral aplicar em meu estudo?”. Vimos que aumentar o tamanho amostral resulta em estimativas mais precisas, isto é com menor erro padrão. Portanto, um bom delineamento amostral é aquele que permita, a um custo mínimo, obter estimativas com a precisão desejada. Uma pesquisa que resulte em estimativas demasiadamente imprecisas pode se mostrar inútil. O que dizer por exemplo, se um estudo conclui que o comprimento médios de uma espécie de pescado é de \\(35\\) cm com uma incerteza a \\(95\\%\\) entre \\(15\\) e \\(55\\) cm? Uma estimativa com tal nível de imprecisão não terá qualquer implicação prática. Por outro lado, partir de um determinado tamanho amostral o ganho em precisão torna-se mínimo. Isto significa que amostras demasiadamente grandes podem ter um custo muto alto porém não serem capazes aumentar de forma relevante a precisão do experimento. Veja o que ocorre com o erro padrão de uma amostra à medida que aumenta o tamanho \\(n\\). Neste exemplo, para amostras de tamanho 1, \\(\\sigma_{\\overline{X}} = 4\\). Se tivermos agora amostras de tamanho 10, \\(\\sigma_{\\overline{X}} = 1.2\\), uma redução de mais de 50%. No entanto aumentarmos o tamanho amostral para 50 o erro padrão cai somente de \\(1,2\\) para \\(0,56\\). Isto significa que a partir de determinado ponto (neste exemplo a partir de \\(10\\) ou \\(20\\) amostras), a redução no erro padrão torna-se mínima. Neste momento podemos podemos refletir sobre o custo de continuar aumentando o tamanho amostral para obter um ganho cada vez menor em precisão. Para encontrarmos o tamanho amostral desejado, devemos decidir sobre dois pontos: i - que nível de acurácia desejado, ou seja, quão distante do valor real (média populacional) queremos que nossa esimativa esteja; e ii - qual o nível de confiança do resultado, ou seja, com que precisão queremos fazer esta estimativa. 16.2.1 Nível de acurácia desejado (margem de erro) e nível de confiança na estimativa O nível de acurácia desejado é comumente conhecido com margem de Erro (E), definida como diferença máxima provável (com probabilidade \\(1-\\alpha\\)) entre a média amostral e a média populacional. A margem de erro para a média amostral pode ser obtida por (compare esta expressão com a do intervalo de confiança): \\[E = z_{\\alpha/2} \\times \\frac{\\sigma}{\\sqrt{n}}\\] O nível de confiança na estimativa nos garante que nossa estimativa estará dentro da margem de erro assumida com probabilidade \\(1-\\alpha\\). Como vimos acima, valores típicos para o nível de confiança são \\(99\\%\\), \\(95\\%\\) e \\(90\\%\\). Uma representação esquemática do erro amostral e do nível de confiança na distribuição de \\(z\\) pode ser vista abaixo: A definição da margem de erro e do nível de confiança depende de estimativas prévias dos parâmetros populacionais \\(\\mu\\) e \\(\\sigma\\). Estas estimativas podem ser obtidas na literatura, buscando estudos similares, ou por meio de um projeto piloto. Em um experimento piloto, o pesquisador irá conduzir seu plano de amostragem com um tamanho mínimo, justamente para avaliar a eficiência metodológica, adequabilidade dos resultados e prever o esforço amostral adequado. As informações de um pequeno estudo piloto, se bem aproveitadas, podem evitar erros simples de delineamento, além de invariavelmente, permitir economia de recusros e consequentemente ganho em qualidade. 16.2.2 Determinando o tamanho de uma amostra Podemos voltar a nossa questão anterior: “Qual tamanho amostral aplicar em meu estudo?”. Esta questão pode ser reformulada como sendo “Qual tamanho amostral aplicar para obter uma estimativa de \\(\\mu\\) que possua uma margem de erro \\(E\\) e nivel de confiança \\(1-\\alpha\\) pré-determinados. Iniciando com a fórmula da margem de erro: \\[E = z_{\\alpha/2} \\times \\frac{\\sigma}{\\sqrt{n}}\\] isolamos a variável \\(n\\) para obter: \\[n = (\\frac{ z_{\\alpha/2} \\times \\sigma}{E})^2\\] Novamente, uma vez que não conhecemos o desvio padrão populacional \\(\\sigma\\) podemos substituí-lo pelo desvio padrão (\\(s\\)) de um experimento piloto ou estimá-lo a partir da literatura. "],["th.html", "Capítulo 17 Introdução ao Teste de Hipóteses 17.1 Probabilidade e teste de hipóteses 17.2 Exemplificando um teste de hipóteses: o teste z 17.3 Erros de decisão em um teste de hipóteses 17.4 Estabelecendo a hipótese alternativa: testes Bicaudais e Unicaudais", " Capítulo 17 Introdução ao Teste de Hipóteses Um dos objetivos centrais em estatística é fazer inferências válidas para a população examinando as características de uma amostra. Considere as afirmações abaixo: “a fragmentação de habitats reduz a diversidade em \\(x\\) espécies”; “em níveis elevados de poluentes, a taxa de sobrevivência de um determinado organismo cai em \\(y\\%\\)”; “a remoção da área de mangue implica na redução em \\(m\\%\\) da captura de carbono”. Todas estas afirmações são na realidade hipóteses, sobre um ou mais parâmetros de uma população estatística que podem ser testadas por meio de um experimento adequado. A experimentação nos permite tirar conclusões sobre determitada hipótese com base na amostra. Mais especificamente, queremos saber se os dados em mãos nos permitem ou não refutar uma hipótese inicial. Se desejamos fazer uma inferência sobre um parâmetro da população estatística (ex.: sua média \\(\\mu\\)), devemos iniciar com uma afirmação sobre a posição deste parâmetro, que denominamos de hipótese nula (\\(H_0\\)). Vamos a um exemplo simples; Imagine que um modelo de climático estabeleça que a pluviosidade média entre junho e agosto nas cidades litorâneas do estado de São Paulo seja de \\(110\\) mm/mês. Um cientista acredita que o modelo têm falhas e resolve tomar algumas observações sobre chuva mensal a fim de testar esta afirmação. Este cientista iria iniciar por formalizar suas hipóteses estatísticas. Inicialmente será necessário estabelecer o que chamamos de hipótese nula. Se uma análise estatística concluir que a hipótese nula deve ser falsa, então precisaremos ter me mãos uma hipótese alternativa (\\(H_a\\)). Assim, no caso de rejeição de \\(H_0\\), passaremos a assumir \\(H_a\\) como verdadeira. Neste exemplo poderíamos ter: \\(H_0: \\mu = 110\\) mm de chuva (HIPÓTESE NULA) \\(H_a: \\mu \\ne 110\\) mm de chuva (HIPÓTESE ALTERNATIVA) O segundo passo é decidir sobre o limite de rejeição, isto é, um limite a partir do qual iremos conlcuir que \\(H_0\\) é falsa. Este é um limite baseado no que denominamos de nível de significância (\\(\\alpha\\)) sobre o qual iremos falar adiante. O terceiro passo é tomar amostras independentes sobre o fenômeno em questão. Note que as hipóteses nula e alternativa se referem a predições sobre a posição da média populaçional \\(\\mu\\), que é justamente a informação que não temos, mas sobre a qual queremos conhecer. Como não temos acesso à \\(\\mu\\), nossa opção é tomar amostras do fenômeno, medindo a quantidade de chuva em diferentes localidades e calcular a média amostral \\(\\overline{X}\\). 17.1 Probabilidade e teste de hipóteses A média \\(\\overline{X}\\) de uma amostra será nossa melhor evidência a respeito de \\(\\mu\\). Tendo este valor, podemos nos perguntar: O valor obtido de \\(\\overline{X}\\) é condizente com o esperado segundo \\(H_0\\)? Podemos racionalizar que se \\(\\overline{X}\\) estiver muito próximo a \\(\\mu\\), não haveria evidências para rejeitar \\(H_0\\). Por outro lado, um valor de \\(\\overline{X}\\) muito distante de \\(\\mu\\) irá colocar em dúvida a afirmação feita por \\(H_0\\). O ponto relevante aqui é: quão distante de \\(\\mu\\) deve estar \\(\\overline{X}\\) para que rejeitemos \\(H_0\\)? Esta resposta poderá ser respondida somente com o auxílio de um modelo probabilístico aplicado ao experimento em questão. Seja \\(H_0\\) verdadeira, é esperado que a probabilidade de \\(\\overline{X}\\) estar próximo a \\(\\mu\\) é alta. Portanto, uma pergunta melhor formulada seria: Sendo \\(H_0\\) verdadeira, qual é a probabilidade de que uma determinada média amostral \\(\\overline{X}\\) esteja tão ou mais distante de \\(\\mu\\) quanto o observado em nossa amostra particular? 17.1.1 A necessidade de um modelo de distribuição das médias amostrais A pergunta feita acima é de natureza probabilística, de modo que para respondê-la iremos precisar estabelecer um modelo probabilístico para a distribuição das médias amostrais. De acordo com o que temos discutido até este ponto, Teorema Central do Limite (TCL) estabelece que a distribuição normal é um bm modelo neste situação. Desta forma, para um \\(H_0\\) verdadeiro, seria esperado que a distribuição das médias amostrais resultantes de um procedimento experimental tivesse o formato de um distribuição normal, centrada em \\(110\\) mm. Segundo o TCL, a distribuição seria centrada em \\(\\mu\\) e o desvio padrão seria definido pelo erro padrão da média (Capítulo 13), isto é, \\(\\sigma_{\\overline{X}} = \\frac{\\sigma}{\\sqrt{n}}\\). Digamos ainda que o modelo climático estabeleça que desvio padrão para a quantidade de chuva seja \\(\\sigma = 30\\). Neste caso, o erro padrão seria de \\(\\sigma_{\\mu} = \\frac{30}{\\sqrt{n}}\\). Feito isto, temos em mãos o modelo probabilístico que, aliado a uma amostra particular, nos permitirá concluir se há evidências para rejeitar \\(H_0\\) em favor de \\(H_a\\). 17.1.2 Definindo o limite de rejeição para \\(H_0\\) Segundo a distribuição normal, a probabilidade do valor observado \\(\\overline{X}\\) estar tão ou mais distante de \\(\\mu\\) na distribuição \\(Z\\) é calculando por: \\[z = \\frac{\\overline{X} - \\mu}{\\sigma_{\\overline{X}}}\\] O valor de \\(z\\) calculado é chamado de estatitica do teste. Com o uso da Tabela \\(Z\\), esta estatística será utilizada para encontrar: \\[P(Z \\ge z) = P(\\overline{X} \\ge \\mu)\\] Como nossa pergunta se refere à distância entre \\(\\overline{X}\\) e \\(\\mu\\), devemos encontar também \\(P(Z \\le -z)\\), de modo que a probabilidade que nos interessa será representada pela área destacada em vermelho na figura abaixo que nos dá \\(P(Z \\ge |z|)\\). A área destacada em vermelho será irá diminuir conforme \\(\\overline{X}\\) se distancia de \\(\\mu\\) e irá aumentar para valores de \\(\\overline{X}\\) muito próximos a \\(\\mu\\). Como estamos falando de uma distribuição de probabilidade, esta área mede a probabilidade de encontrarmos \\(\\overline{X}\\) pelo menos a uma dada distância de \\(\\mu\\), assumindo que \\(H_0\\) seja verdadeiro. Se esta área for muito pequena, a probabilidade de que \\(\\overline{X}\\) seja condizente com \\(H_0\\) diminui. Chamaremos este probabilidade de valor de p. Portanto, se \\(p\\) for muito pequeno dizemos que é improvável que \\(\\overline{X}\\) seja condizente com \\(H_0\\), nos levando a rejeitar \\(H_O\\) em favor de \\(H_a\\). A decisão do que é uma probabilidade foi muito pequena é feita com base no limite de rejeição \\(\\alpha\\), tambṕem chamado de nivel crítico ou nível de significância. Deste modo, a conclusão de um teste estatístico se dá por: Se \\(p &gt; \\alpha\\) –&gt; ACEITAMOS \\(H_0\\) Se \\(p \\le \\alpha\\) –&gt; REJEITAMOS \\(H_0\\) (e assumimos \\(H_a\\) como verdadeira) 17.2 Exemplificando um teste de hipóteses: o teste z Digamos que o número de batimentos cardíacos por minuto de um adulto em repouso tenha média \\(\\mu = 65\\) e desvio padrão \\(\\sigma = 9\\). Você imagina que o sedentarismo altera o batimento médio de um adulto. Para testar esta suposição você deve inicialmente determinar as hipóteses nula e alternativa: \\(H_0: \\mu = 65\\) batimentos por minuto \\(H_a: \\mu \\ne 65\\) batimentos por minuto Em seguida você determina o nível de significância (\\(\\alpha\\)) do teste. Vamos determinar que queremos fazer o teste ao nível de significância \\(\\alpha = 0,05\\). IMPORTANTE: O nível de significância \\(\\alpha\\) deve ser determinado antes da tomada de dados. Finalmente, você seleciona ao acaso \\(n = 15\\) pessoas de hábito sedentário e mede seus batimentos cardíacos. Os resultados obtidos desta amotra aleatória são: Amostra: 65, 73, 56, 71, 69, 69, 68, 59, 73, 68, 69, 64, 67, 64, 66 que nos dá uma média amostral de: \\(\\overline{X} = \\frac{\\sum{X_i}}{n} = \\frac{65+73+56+71+69+69+68+59+73+68+69+64+67+64+66}{15} = 66.73\\) batimentos por minuto; e um erro padrão de: \\(\\sigma_{\\mu} = \\frac{\\sigma}{\\sqrt{n}} = \\frac{9}{3.87} = 2.32\\) Com estes resultados encontramos o valor correspondente de Z. \\(z = \\frac{\\overline{X} - \\mu}{\\sigma_{\\mu}} = \\frac{66.73 - 65}{2.32} = 0.75\\) Utilizando a Tabela Z, encontramos a probabilidade de obtermos valores tão ou mais extremos que \\(-0.75\\) e \\(+0.75\\). Com isto, a probabilidade de encontarmos valores tão ou mais extermos que \\(\\overline{X} = 66.73\\) foi calculada em \\(0.227 + 0.227 =\\) 0.453. Neste exemplo, a estatística do teste foi \\(z = 0.75\\) o a probabilidade associada \\(p = 0.453\\). No R fazemos: X &lt;- c(65, 73, 56, 71, 69, 69, 68, 59, 73, 68, 69, 64, 67, 64, 66) Xm &lt;- mean(X) pnorm(q = Xm, mean = 65, sd = 9/sqrt(15), lower.tail = FALSE) * 2 ## [1] 0.4557231 17.2.1 Tomada de decisão sobre \\(H_0\\): nível de significância No exemplo acima, obtivemos \\(p =\\) 0.453. Isto significa que: sendo \\(H_0\\) verdadeira, existe uma probabilidade igual a \\(0.453\\) de que a média de uma amostra com \\(n = 15\\) esteja tão ou mais distante de \\(\\mu = 65\\) como observado neste experimento. Se aceitarmos que esta probabilidade é alta, então não há motivo para buscar por outras explicações. Por outro lado, se concluirmo que esta probabilidade é baixa, estamos dizendo que resultado obtido é improvável segundo a hipótese nula. Neste caso, temos espaço para buscar por hipóteses alternativas que possam explicar o fenômeno. Para decidir se a probabilidade obtida é alta ou baixa, devemos compará-la ao nível de significância \\(\\alpha\\) pré-estabelecido. \\(H_0\\) será aceita somente se a probabilidade encontrada for maior que \\(\\alpha\\). Por outro lado, se nossa probabilidade for menor ou igual a \\(\\alpha\\), considerarmos os resultados improváveis segundo a hipótese nula e rejeitamos \\(H_0\\) em favor de \\(H_a\\). Um nível crítico comumente utilizado é \\(\\alpha = 0.05\\). No exemplo acima a probabilidade foi de 0.453, um valor muito acima de \\(0.05\\). Dizemos portanto, que a média amostral \\(\\overline{X}\\) não está tão distante do \\(\\mu\\) a ponto de rejeitarmos \\(H_0\\). Concluimos que, neste exemplo, \\(\\overline{X} = 66.73\\) não nos fornece evidência suficiente para rejeitar \\(H_0\\). 17.3 Erros de decisão em um teste de hipóteses A interpretação da probabilidade final esta associada à situação em que \\(H_0\\) seja verdadeira. Isto nos leva perguntar: o que esperar caso \\(H_0\\) seja falsa? Como não sabemos de fato, de \\(H_0\\) é verdadeira ou não, a tomada de decisão sobre um resultado de um teste estatístico pode nos levar às seguintes situações: \\(H_0\\) Verdadeira \\(H_0\\) Falsa \\(H_0\\) é rejeitada \\(\\alpha\\) (\\(\\textbf{Erro Tipo I}\\)) Decisão correta (\\(1-\\beta\\)) \\(H_0\\) é aceita Decisão correta (\\(1-\\alpha\\)) \\(\\beta\\) (\\(\\textbf{Erro Tipo II}\\)) A tabela acima nos mostra os tipos de erros aos quais estamos sujeitos ao realizar um teste de hipótese. Eventualmente, podemos rejeitar \\(H_0\\), ainda que ela seja verdadeira. O nivel de significância adotado, estabele que a probabilidade disto acontecer é \\(\\alpha\\). Se rejeitarmos \\(H_0\\) quando ela é verdadeira, estaremos incorrendo em um erro de decisão que denominamos de Erro Tipo I. Consequentemente, temos uma probabilidade de \\(1 - \\alpha\\) de aceitar corretamente \\(H_0\\) quando ela é verdadeira. Estabelecer um \\(\\alpha = 0,05\\) nos garante que iremos incorrer no erro do tipo I em somente \\(5\\%\\) das vezes que o experimento for realizado. Um outra situação ocorre quando aceitamos erroneamente a hipótese nula que é falsa, incorrendo no Erro Tipo II. O erro do tipo II tem probabilidade \\(\\beta\\) de acontecer. O complementar desta probabilidade (\\(1-\\beta\\)) é denominado de Poder do Teste. Um teste poderoso é portanto, aquele que tem elevada probabilidade de rejeitar \\(H_0\\) quando ela é falsa. As figuras abaixo representam as distribuições das médias amostrais e os erros do tipos I e II quando o \\(H_0\\) é verdadeira (\\(\\mu_a = \\mu\\)) e quando \\(H_0\\) é falsa (\\(\\mu_a &gt; \\mu\\)). Idealmente em um teste estatístico, seria interessante reduzir ao máximo os erros do tipo I e II. Ao reduzirmos o erro do tipo I, diminuindo \\(\\alpha\\) teremos um teste mais rigoroso que raramente iria errar ao rejeitar um \\(H_0\\) verdadeiro (Figura A). Entretanto, este teste também raramente iria rejeitar \\(H_0\\) ainda que ele seja falso (Figura B). Consequentemente, ao diminuir o valor de \\(\\alpha\\) ficamos menos propensos a cometer o erro do tipo I, porém mais propensos a incorrer no erro tipo II, isto é, não rejeitar uma \\(H_0\\) falsa. Dadas estas características, o único modo que reduzir os dois tipos de erros simultaneamente é aumentando o tamanho amostral \\(n\\) pois, neste caso, reduzimos o erro padrão (\\(\\sigma_{\\overline{X}}\\)) e consequentemente a sobreposição entre as duas curvas acima. 17.4 Estabelecendo a hipótese alternativa: testes Bicaudais e Unicaudais "],["testet.html", "Capítulo 18 Teste t de Student 18.1 Teste t para uma média populacional 18.2 Graus de liberdade 18.3 Probabilidades no teste \\(t\\) de Student: a tabela \\(t\\) 18.4 Teste t para comparação de duas médias independentes", " Capítulo 18 Teste t de Student De acordo com o que discutimos no capítulo 16, o modelo normal de probabilidades não é a melhor aproximação para a distribuição das madias amostrais quando não conhecemos \\(\\mu\\) e \\(\\sigma\\) e/ou quando o tamanho amostral \\(n\\) é pequeno. Nesta situação, apresentamos a distribuição t de Student como uma opção mais apropriada para o cálculo de um intervalo de confiança. Da mesma forma, o teste \\(Z\\) que introduzimos no capítulo 17 assume que a distribuição das médias amostrais é normalmente distribuída e que a variância populacional \\(\\sigma\\) seja conhecida, uma informação que não temos na prática científica. Seguindo este raciocínio portanto, iremos apresentar o teste t para verificar uma hipótese sobre uma média populacional. Este teste é utilizado em substituição ao teste \\(Z\\) quando \\(\\sigma\\) é desconhecido e/ou o tamanho amostral é pequeno. O teste segue também a mesma lógica discutida no teste \\(Z\\), porém estabelece que a distribuição das médias amostrais é melhor descrita pela distribuição \\(t\\) e não pela distribuição normal. Figure 18.1: Teste Z versus teste t. Estatística do teste e distribuição de probabilidades 18.1 Teste t para uma média populacional Considere um exemplo simples. Dados do Banco Central do Brasil dizem que moedas de \\(R\\$ 0,10\\) da segunda geração pesam 4.8 gramas. Você tem \\(8\\) moedas no bolso e resolve testar essa afirmação pesando cada moeda. Os pesos obtidos são: \\(X = 5.1, 5, 4.8, 5, 5, 4.9, 4.9, 4.7\\). Inicialmente, devemos estabelecer nossa hipótese nula (\\(H_0\\)), nossa hipótese alternativa (\\(H_a\\)) e o nível se significância \\(\\alpha\\). Iremos estabelecer \\(\\alpha = 0,05\\) e as hipóteses como: \\(H_0: \\mu = 4.8\\) gramas \\(H_a: \\mu \\ne 4.8\\) gramas Como não conhecemos \\(\\sigma\\) e temos uma amostra pequena, a posição das médias amostrais seguirá uma distribuição \\(t\\) de Student e a estatística do teste será: \\[t = \\frac{\\overline{X} - \\mu}{s_{\\overline{X}}}\\] sendo o erro padrão amostral obtido por: \\[s_{\\overline{X}} = \\frac{s}{\\sqrt{n}}\\] O cálculo de \\(t\\) é muito similar ao escore \\(Z\\). No entanto, substituímos \\(\\sigma\\) por \\(s\\). Como visto no capítulo 16, as distribuições de \\(t\\) e de \\(Z\\) são muito similares. Entretanto, para amostras pequenas e quando \\(\\sigma\\) é desconhecido, a curva de \\(t\\) nos fornece uma melhor estimativa das probabilidades associadas a distribuição das médias amostrais. Para este exemplo, temos uma amostra de tamanho \\(n = 8\\) com média \\(\\overline{X} = 4.925\\)g e desvio padrão \\(s = 0.13\\)g. O valor de \\(t\\) pode ser calculado por: \\[t_{c} = \\frac{\\overline{X} - \\mu}{s_{\\overline{X}}} = \\frac{\\overline{X} - \\mu}{\\frac{s}{\\sqrt{n}}} = \\frac{4.925 - 4.8}{\\frac{0.13}{\\sqrt{8}}} = 2.76\\] Assim como fizemos para a distribuição \\(Z\\), devemos encontrar a probabilidade de obtermos um valor tão ou maior que o módulo de \\(t_c\\). Na figura abaixo, nosso resultado fica: A probabilidade de encontrarmos um valor de \\(t_c\\) tão ou mais extremo segundo a hipótese nula foi de \\(p = 0.028\\). Uma vez que este valor é menor que o nível crítico \\(\\alpha = 0,05\\), concluímos que existe evidência suficiente para rejeitar \\(H_0\\) e aceitar a hipótese alternativa de que as moedas de \\(10\\) centavos não provém de uma população estatística com \\(\\mu = 4,8\\) gramas. Nossa conclusão é portanto, que as moedas de \\(R\\$ 0,10\\) são mais pesadas que \\(4,8\\) gramas. No R, o teste \\(t\\) discutido acima pode ser realizado por: t.test(X, mu = 4.8) ## ## One Sample t-test ## ## data: X ## t = 2.7584, df = 7, p-value = 0.02816 ## alternative hypothesis: true mean is not equal to 4.8 ## 95 percent confidence interval: ## 4.817844 5.032156 ## sample estimates: ## mean of x ## 4.925 Nos comandos acima, \\(X\\) é a amostra e o argumento mu representa a expectativa sobre a média populacional segundo \\(H_0\\). Como resultados temos: a indicação de que fizemos um teste \\(t\\) para uma amostra: One Sample t-test; o valor de \\(t\\) calculado: t = 2.7584; os graus de liberdade: df = 8 - 1 = 7; e o valor de p = 0.028. Podemos ver ainda o valor da média amostral (\\(\\overline{X} = 4.925\\)) e o intervalo de confiança a \\(95\\%\\) (4.817844 - 5.032156). 18.2 Graus de liberdade A distribuição \\(t\\) como várias outras distribuições amostrais utilizadas em inferência estatística, muda seu formato em função do que chamamos de graus de liberdade (\\(gl\\)). Os graus de liberdade têm relação com o tamanho amostral. No caso do teste \\(t\\) para \\(1\\) amostra, esta relação é simplesmente: \\(gl = n-1\\). À medida que os graus de liberdade aumentam, o formato da distribuição \\(t\\) se assemelha ao formato da distribuição Normal padronizada. De fato, para graus de liberdades altos (ex. \\(n \\ge 30\\)), os formatos das distribuições \\(Z\\) e \\(t\\) são praticamente indistinguíveis. Na prática, isto faz que as distribuição \\(Z\\) raramente seja utilizada. 18.3 Probabilidades no teste \\(t\\) de Student: a tabela \\(t\\) A rejeição uo aceitação da hipótese nula em um teste \\(t\\) pode ser feita por meio da obtenção do valor de p ou pela comparação do \\(t\\) calculado com valores críticos de referência para determinado nível de significância. O primeiro caso foi o que apresentamos acima e depende de um software estatístico para obtermos valores exatos de \\(p\\). O segundo caso, pode ser feito com auxílio da Tabela \\(t\\), em que limites críticos de \\(t\\) são disponibilizados para diferentes níveis de significância e graus de liberdade. Atualmente, o uso da tabela \\(t\\) têm finalidade em grande parte didática e, por este motivo, vamos apresentá-lo aqui rapidamente. No entanto, fora da sala de aula, o teste \\(t\\) será invariavelmente conduzido por meio de um software estatístico este método que permitirá a obtenção do valor exato de \\(p\\). Na tabela \\(t\\) (acesse aqui), a primeira coluna mostra os graus de liberdade de \\(1\\) a \\(120\\). O cabeçalho da tabela de \\(90\\%\\) a \\(0,1\\%\\) mostra a área na distribuição de \\(t\\) nas caldas inferior e superior. Vamos retornar ao exemplo da moedas de \\(R\\$0,10\\) para exemplificar sua utilização. Neste exemplo a tinhamos \\(8\\) (\\(gl = 7\\) graus de liberdade) e o teste foi feito com \\(\\alpha =0,05\\). Se buscarmos na linha \\(gl = 7\\) e a coluna \\(5\\%\\) (\\(\\alpha = 0,05\\)), encontraremos o valor \\(t = 2,3646\\). Este é o chamado \\(t\\) crítico (\\(t_{crítico}\\)). Acima deste valor e abaixo de sua contraparte negativa temos exatamente \\(5\\%\\) da área na disribuição \\(t\\). Deste modo, qualquer valor calulado maior que \\(t_{crítico}\\) estará mais para a extremidade da distribuição e consequentemente estará associado a menores valores de probabilidade. Neste sentido: \\(t_{calculado} \\ge t_{crítico}\\) leva a rejeição de \\(H_0\\) \\(t_{calculado} &lt; t_{crítico}\\) leva a aceitação de \\(H_0\\) O resultado do teste estatístico em nosso exemplo foi \\(t_c = 2.76\\) que é maior que \\(2,3646\\). Isto nos leva à mesma decisão anterior (rejeitar \\(H_0\\)), ainda que por meio da tabela \\(t\\) não tenhamos o valor exato de probabilidade. 18.4 Teste t para comparação de duas médias independentes O que vimos no teste \\(t\\) para uma amostra pode ser facilmente extendido para testarmos a diferenças entre duas amostras. Os dados abaixo mostram o tempo de coagulação sanguínea (em minutos) em ratos machos adultos tratados com dois tipos de drogas, retirado do livro Biostatistical Analysis (Zar 2010), pp. 130-134. Droga Tempo Droga A 8.8 Droga A 8.4 Droga A 7.9 Droga A 8.7 Droga A 9.1 Droga A 9.6 Droga B 9.9 Droga B 9.0 Droga B 11.1 Droga B 9.6 Droga B 8.7 Droga B 10.4 Droga B 9.5 Nosso objetivo é testar se as duas drogas resultam, em média, no mesmo tempo de coagulação. Inicialmente, vamos fazer um gráfico de dispersão para verificar a distribuição do tempo de coagulação para cada droga. As médias, desvios padrões e tamanhos amostrais de cada grupo são: Droga Tempo médio Desvio n Droga A 8.75 0.58 6 Droga B 9.74 0.82 7 Para testarmos se as médias dos grupos provém de populações estatísticas com diferentes \\(\\mu&#39;s\\) devemos estabelecer nosso nível de significância (por exemplo \\(\\alpha = 0.05\\)) as hipoteses estatísticas: \\(H_0: \\mu_A = \\mu_B\\) gramas \\(H_a: \\mu_A \\ne \\mu_B\\) gramas O teste t para duas amostras é calculado por: \\[t = \\frac{(\\overline{X_A} - \\mu_A) - (\\overline{X_B} - \\mu_B)}{s_{\\overline{X_A}-\\overline{X_B}}}\\] Assumindo a hipotese nula em que \\(\\mu_A = \\mu_B\\) a expressão fica \\[t = \\frac{\\overline{X_A} - \\overline{X_B}}{s_{\\overline{X_A}-\\overline{X_B}}}\\] em que a quantia \\(s_{\\overline{X_A}-\\overline{X_B}}\\) é calculada por: \\[s_{\\overline{X_A}-\\overline{X_B}} = \\sqrt{\\frac{s^2_{p}}{n_1} + \\frac{s^2_{p}}{n_2}}\\] \\(s_p\\) é denominada de variância conjunta calculada por \\[s^2_p = \\frac{(n_1 - 1) \\times s^2_1 + (n_2 - 1) \\times s^2_2}{(n_1 - 1) + (n_2 - 1)}\\] Para este exemplo, \\(s_p = 0.52\\) e \\(s_{\\overline{X_A}-\\overline{X_B}} = 0.4\\) O valor de t calculado é: \\(t_c = -2.476\\) Na distribuição t, a probabilidade de encontrar valores tão ou mais extremos que 2.476 é de \\(p = 0.031\\). Portanto: \\(P(|t| \\ge 2.476) \\le 0.05\\) Uma vez que a probabilidade associada ao valor de \\(t\\) é menor que o nível de significância, rejeitamos \\(H_0\\) e assumimos que os tempos médios de coagulação são diferentes. Ao avaliar as média amostrais \\(\\overline{X}_A\\) e \\(\\overline{X}_B\\), concluímos que a droga \\(A\\) resulta, em média, em tempos menores de coagulação. References "],["modlinear.html", "Capítulo 19 Modelos lineares 19.1 Simulando um modelo linear no R", " Capítulo 19 Modelos lineares Até agora estávamos nos referindo à nossa variável de interesse principal como variável \\(X\\). Nesta seção (Capítulos 19 a 25) iremos nos referir a uma variável resposta (ou variável dependente, \\(Y\\)), que será nossa variável de interesse principal e que será descrita como função de uma ou mais variáveis preditoras (ou variáveis independentes, \\(X\\)). Abordaremos novamente os conceitos de partição da soma dos quadrados, covariância e coeficiente de determinação no contexto do teste de hipóteses e ajuste de modelos. Neste sentido, esta seção tem relação direta com o que foi apresentado nos capítulos 10 e 11 e é interessante revê-los antes de continuar. Modelos lineares compõem os modelos estatísticos clássicos para descrevermos o comportamento de uma variável resposta \\(Y\\) como função de uma ou mais variáveis preditoras \\(X\\)´s quando esta relação é linear. O formato básico de um modelo linear é: \\[Y = \\beta_0 + \\beta_1X + \\epsilon\\] em que \\(\\beta_0\\) e \\(\\beta_1\\) são os parâmetros do modelo e \\(\\epsilon\\) é denominado de resíduo do modelo. Nos modelos que veremos nesta seção, o resíduo tem distribuição normal com média \\(0\\) e variância constante \\(\\sigma^2\\), ou seja: \\[\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\] A parte inicial do modelo (\\(\\beta_0 + \\beta_1X\\)) forma a parte sistemática do modelo, enquanto \\(\\epsilon\\) descreve a parte estocástica. Quando existe mais de uma variável preditora, o modelo pode ser descrito como: \\[Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_pX_p + \\epsilon\\] em que temos portanto \\(p\\) variáveis preditoras envolvidas. \\(Y\\) consiste de uma variável contínua, enquanto as variáveis \\(X\\) em um modelo linear podem ser todas quantitativas (contínuas ou discretas), todas qualitativas (categóricas ou ordinais) ou conter uma combinação destes tipos. Os modelos que veremos nos capítulos desta seção são casos particulares de modelos lineares que variam em função do número de variáveis preditoras envolvidas e da natureza destas variáveis (quantitativas ou qualitativas). Deste modo temos por exemplo: Número de variáveis preditoras (\\(X\\)) Quantitativa Qualitativa Modelo Uma X ANOVA Uma X Regressão linear simples Duas ou mais X Regressão linear múltipla Duas ou mais X ANOVA fatorial Duas ou mais X X ANCOVA 19.1 Simulando um modelo linear no R Vamos simular dois tipos de modelos lineares (o modelo de Regressão Linear e o modelo de Análise de Variância - ANOVA) utilizando uma notação comum. Nos capítulos seguintes cada um destes modelos será tratado de forma particular. 19.1.1 O Modelo de Regressão Envolve uma única variável preditora contínua e três parâmetros desconhecidos, \\(\\beta_0\\) e \\(\\beta_1\\) e \\(\\sigma^2\\). Neste modelo, os pontos mostram o resultado do modelo estatístico e a linha descreve a parte sistemática. O parâmetro \\(\\beta_0\\) descreve a posição média em \\(Y\\) para um valor de \\(X = 0\\) e \\(\\beta_1\\) a taxa de incremento médio em \\(Y\\) para a deferença de uma unidade em \\(X\\). O parâmetro \\(\\sigma\\) se refere ao desvio padrão do resíduo e define o grau de espalhamento dos pontos ao redor da reta média. Experimente diminuir e aumentar o valor de \\(\\sigma\\) e verifique o que ocorre com a dispersão dos pontos. 19.1.2 Modelo de ANOVA Neste caso a variável preditora \\(X\\) é categórica. Vamos criar uma variável \\(X\\) com 4 níveis (A, B, C e D) em um experimento com \\(8\\) repetições (réplicas) por nível, totalizando \\(n = 8 \\times 4 = 40\\) observações. Quando modelos de ANOVA são representados como modelos lineares, a variável categórica deve ser transformada em uma (no caso de dois níveis) ou mais variáveis indicadoras (ou variáveis dummy). Esta transformação é consiste basicamente em criar variáveis do tipo \\(0/1\\) que indiquem todas as combinações de níveis do experimento. Para entender melhor, acesse o link: Dummy variable (statistic). Os comandos neste caso são mais complexos que na regressão simples, mas seguem a mesma lógica de criarmos as variáveis preditoras e somá-las ao ressíduo do modelo. Nestes comandos criamos uma variável preditora \\(X\\) com \\(4\\) níveis, as transformamos em uma variável indicadora e criamos os valores de \\(\\mu_A\\), \\(\\mu_B\\), \\(\\mu_C\\) e \\(\\mu_D\\), de modo que: \\(\\mu = \\beta_0 = 50\\): média geral \\(\\mu_A = \\beta_0 + \\beta_1 = 50 + 10\\): média do tratamento A \\(\\mu_B = \\beta_0 + \\beta_2 = 50 + -20\\): média do tratamento B \\(\\mu_C = \\beta_0 + \\beta_3 = 50 + 30\\): média do tratamento C \\(\\mu_D = \\beta_0 + \\beta_4 = 50 + 15\\): média do tratamento D As notações \\(\\mu_{A \\cdots D}\\) são respectivamente as médias populacionais dos tratamentos, conforme iremos definir no capítulo sobre Análise de Variância (ANOVA - capítulo 20). Deste modo, o gráfico de dispersão resultante deste modelo contém variáveis preditoras categóricas em que os pontos estão espalhados ao redor da media \\(\\mu\\) (em azul) do respectivo tratamento de acordo com a magnitude de \\(\\sigma\\) . "],["anova.html", "Capítulo 20 Análise de variância de um fator 20.1 O modelo da ANOVA e as hipóteses estatísticas 20.2 Partição da soma dos quadrados 20.3 Quadrados médios e graus de liberdade 20.4 Estatística \\(F\\) e teste de hipóteses 20.5 Um exemplo de ANOVA 20.6 Testes a posteriori de comparação de médias 20.7 Ajustando a ANOVA no R 20.8 Pressupostos da ANOVA", " Capítulo 20 Análise de variância de um fator A Análise de Variância (ANOVA) desenvolvida por R. A. Fisher aplica-se à uma classe de desenho experimental em que a variável resposta \\(Y\\) é contínua e a variável explanatória \\(X\\) é categórica com \\(2\\) ou mais níveis. A ANOVA permite testarmos a hipótese de que duas ou mais médias amostrais (\\(\\overline{Y}_i\\)) possam ter sido obtidas de uma mesma população estatística com média \\(\\mu\\). Alternativamente, podemos concluir que as médias amostrais diferem umas das outras, de tal forma que devemos assumir que foram amostradas a partir de diferentes populações estatísticas, nas quais ao menos um \\(\\mu_i\\) seja diferente dos demais. Iremos denominar estas duas possibilidades de hipótese estatísticas sobre a relação entre as médias populacionais. 20.1 O modelo da ANOVA e as hipóteses estatísticas O modelo pode ser representado por: \\[Y_{ij} = \\mu + A_i + \\epsilon_{ij}\\] onde \\(Y_{ij}\\) é a variável resposta associada à observação \\(i\\) do tratamento \\(j\\), \\(\\mu\\) representa a média geral e \\(A_i\\) o efeito do tratamento \\(i\\). O termo \\(\\epsilon_{ij}\\) é denominado de resíduo (ou erro) associado a cada observação, que assumimos ter distribuição normal com média zero e variância constante. \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\) As hipóteses estatísticas no modelo de ANOVA são: \\(H_0: \\mu_1 = \\mu_2 = \\mu_3 =.... = \\mu_k\\) (HIPÓTESE NULA) \\(H_a\\): ao menos um par de médias é diferente (HIPÓTESE ALTERNATIVA) A hipótese nula (\\(H_0\\)) define a ausência de diferenças entre as médias populacionais enquanto a hipótese alternativa (\\(H_a\\)) refere-se a qualquer possibilidade diferente de \\(H_0\\). Se temos exatamente dois níveis em \\(X\\), a comparação de médias pode ser feita por meio de um teste \\(t\\). A ANOVA deve ser utilizada quando temos mais de dois níveis em \\(X\\). Neste sentido, o teste \\(t\\) é um caso particular da ANOVA. 20.2 Partição da soma dos quadrados A ANOVA consiste basicamente da partição da soma dos quadrados (Capítulo 11) seguida da construção de um teste estatístico apropriado (o teste \\(F\\)) para verificar as plasibilidade de \\(H_0\\). Apresentaremos aqui um exemplo simples descrito na figura e na tabela abaixo para descrever o ajuste da ANOVA. Para deixarmos claro as notações que iremos adotar adiante neste capítulo vamos definir que: Temos \\(k = 3\\) grupos (A, B ou C) e para cada grupo \\(n = 5\\) observações. Denotamos por \\(n_{ij}\\) o número de observações dentro de cada grupo, em que \\(i\\) é a i-ésima observação (\\(i = 1\\) a \\(5\\)) do j-ésimo grupo (\\(j = 1\\) a \\(3\\) - grupos A ao C). Neste exemplo, o número de observações em cada grupo é o mesmo (\\(n_1 = n_2 = n_3 = n\\)), de modo que o total de observações é dado por: \\(N = k \\times n = n_1 + n_2 + n_3 = 15\\) A média de cada grupo será denotada por \\(\\overline{Y}_j\\), que neste exemplo são: \\(\\overline{Y}_1 = 20.64\\) (grupo A), \\(\\overline{Y}_2 = 28.68\\) (grupo B) e \\(\\overline{Y}_3 = 12.18\\) (grupo C). Estas médias estimam as quantias \\(\\mu_1\\), \\(\\mu_2\\) e \\(\\mu_3\\) sobre as quais versam as hipóteses do modelo. Vamos denotar por \\(\\overline{\\overline{Y}}\\) a Grande Média, isto é, a média geral de todas as observações independente do grupo de origem e que é utilizada para estimar \\(\\mu\\). \\[\\overline{\\overline{Y}} = \\sum_{j = 1}^{k}\\sum_{i = 1}^{n}\\frac{Y_{ij}}{N} = \\frac{\\overline{Y_1} + \\overline{Y_2} + \\overline{Y_3}}{3} = 20.5\\] Podemos agora observar estes elementos no gráfico de dispersão. Para ajustar o modelo de ANOVA a estes dados precisamos calcular \\(3\\) quantias: i - a Soma dos Quadrados Totais (\\(SQ_{Total}\\)), ii - a Soma dos Quadrados dos Tratamentos \\(SQ_{Trat}\\) e iii - a Soma dos Quadrados dos Resíduos \\(SQ_{Res}\\). Soma dos Quadrados Totais \\(SQ_{Total}\\): mede as diferenças entre \\(Y_{ij}\\) e \\(\\overline{\\overline{Y}}\\). Temos nesta expressão o somatório dos desvios ao quadrado de todas as observações com relação à grande média independente do grupo de origem de cada observação. \\[SQ_{Total} = \\sum_{j = 1}^{k}\\sum_{i = 1}^{n}(Y_{ij} - \\overline{\\overline{Y}})^2\\] Soma dos Quadrados dos Tratamentos \\(SQ_{Trat}\\): mede as diferenças entre as médias dos tratamentos \\(\\overline{Y}_j\\) e \\(\\overline{\\overline{Y}}\\), sendo portanto os desvios ao quadrado da média de cada tratamento subtraída da grande média. \\(SQ_{Trat}\\) também é chamada de soma dos quadrados entre grupos ou entre tratamentos \\[SQ_{Trat} = \\sum_{j = 1}^{k}\\sum_{i = 1}^{n_{j}}(\\overline{Y}_{j} - \\overline{\\overline{Y}})^2 = \\sum_{j = 1}^{k}n_{j}(\\overline{Y}_{j} - \\overline{\\overline{Y}})^2\\] Soma dos Quadrados dos Resíduos \\(SQ_{Res}\\): mede as diferenças entre cada observação \\(Y_{ij}\\) e a média de seu próprio grupo \\(\\overline{Y}_{j}\\). \\(SQ_{Res}\\) também é chamada de soma dos quadrados dentro dos grupos ou dentro dos tratamentos \\[SQ_{Res} = \\sum_{j = 1}^{k}\\sum_{i = 1}^{n_{j}}(Y_{ij} - \\overline{Y}_{j})^2\\] 20.2.1 A característica aditiva das somas dos quadrados A partição da soma dos quadrados consiste em decompor a variação total do experimento em uma parcela atribuída à variação entre tratamentos e outra parcela da variação dentro dos tratamentos. Isto é possível pois as somas dos quadrados definidas acima podem ser expressas de forma aditiva como: \\[SQ_{Total} = SQ_{Trat} + SQ_{Res}\\] Deste modo, é possível demostrar que: \\(\\sum_{j = 1}^{k}\\sum_{i = 1}^{n}(Y_{ij} - \\overline{\\overline{Y}})^2 = \\sum_{j = 1}^{k}n_{j}(Y_{j} - \\overline{\\overline{Y}})^2 + \\sum_{j = 1}^{k}\\sum_{i = 1}^{n}(Y_{ij} - \\overline{Y}_{j})^2\\) 20.2.2 Medindo a associação entre \\(Y\\) e \\(X\\) A característica aditiva das somas dos quadrados pode ser utilizada para mensurar o grau de dependência de \\(Y_{ij}\\) com respeito aos diferentes tratamentos. Compare as duas figuras abaixo: A soma dos quadrados dentro dos grupos é a mesma nas duas figuras (\\(SQ_{Res} = 362.6\\)). No entanto, na figura da esquerda, em que as médias dos tratamentos são similares (e consequentemente próximas à grande média), a soma dos quadrados entre os tratamentos é muito menor (\\(SQ_{Trat}^{esquerda} = 15.8\\)) que na figura da direita, em que as médias dos tratamentos estão distantes entre si (\\(SQ_{Trat}^{direita} = 680.8\\)). É desta forma que a partição das somas dos quadrados nos permite diferenciar situações em que: i - a média dos grupos depende dos níveis do tratamento (figura da direita); de situações em que ii - a média não depende dos níveis do tratamento (figura da esquerda). 20.3 Quadrados médios e graus de liberdade Para que os somatórios dos quadrados expressem uma medida de variação é necessário corriglos em função dos graus de liberdade (\\(gl\\)), obtendo assim Quadrados médios dados abaixo: Quadrado médio total (\\(QM_{Total}\\)) \\[QM_{Total} = \\frac{SQ_{Total}}{gl_{Total}}\\] em que \\(gl_{Total} = N - 1\\) Quadrado médio entre tratamentos (\\(QM_{Trat}\\)) \\[QM_{Trat} = \\frac{SQ_{Trat}}{gl_{Trat}}\\] em que \\(gl_{Trat} = k - 1\\) Quadrado médio dentro dos tratamentos (\\(QM_{Res}\\)) \\[QM_{Res} = \\frac{SQ_{Res}}{gl_{Res}}\\] em que \\(gl_{Res} = N-k\\) Assim como a soma dos quadrados, os graus de liberdade também têm característica aditiva. \\[gl_{Total} = gl_{Trat} + gl_{Res} = (k - 1) + (N - K) = N - 1\\] Os quadrados médios que são estimativas de variâncias. Compare por exemplo a expressão do \\(QM_{Total}\\) com a fórmula da variância amostral (\\(s^2\\)) (Capítulo 6) e verá que excetuando mudanças de notação, as expressões são essencialmente as mesmas. 20.4 Estatística \\(F\\) e teste de hipóteses Uma vez que os quadrados médios são estimativas de variância, uma estatística de teste apropriada é: \\[F_{calculado} = \\frac{QM_{Trat}}{QM_{Res}}\\] A estatística \\(F\\) (ou razão-\\(F\\)) está associada à distribuição de probabilidades \\(F\\) e nos permite comparar a variância associada ao tratamento com a variância associada aos resíduos. Em mãos do valor de \\(F_{calculado}\\), o teste de hipóteses é possível após a definição do nível de significância \\(\\alpha\\). 20.4.1 Nível de significância Assim como discutimos nos testes \\(Z\\) e \\(t\\), o valor de \\(\\alpha\\) estabelece um limite de aceitação para \\(H_0\\), isto é, um limite a partir do qual a estatística do teste se torna tão extrema que nos leva a assumir que \\(H_0\\) é improvável, devendo portanto ser rejeitada em favor de \\(H_a\\). Este passo é possível pois o valor de \\(F_{calculado}\\) pode ser associado à distribuição \\(F\\) de probabilidades, o que nos permite calcular a probabilidade: \\[P(F_{calculado}) \\le \\alpha\\] Para facilitar a notação denominaremos \\(P(F_{calculado})\\) simplesmente de valor de \\(p\\) expresso em vermelho na figura abaixo: Se \\(p &gt; \\alpha\\) –&gt; ACEITAMOS \\(H_0\\) Se \\(p \\le \\alpha\\) –&gt; REJEITAMOS \\(H_0\\) (e assumimos \\(H_a\\) como verdadeira) Tradicionalmente utiliza-se \\(\\alpha = 0.05\\). Neste caso, \\(H_0\\) seria rejeitada somente de \\(p \\le 0.05\\). Algumas área da medicina por eoutro lado, são tradicionais por utilizar valores de \\(\\alpha = 0.01\\), o que torna o experimento menos sujeito ao erro do tipo I (Capítulo 17). Portanto, outros valores de \\(\\alpha\\) diferentes de \\(0.05\\) podem ser escolhidos. O fundamental é que esta decisão, isto é, sobre o nível de significância \\(\\alpha\\) a ser adotado, seja feita previamente à obtenção dos dados. 20.5 Um exemplo de ANOVA Vamos exemplificar o passo-a-passo de uma ANOVA utilizando nosso exemplo fictício. /o primeiro passo é definir as hipóteses estatpisticas e o nível de significância: \\(H_0: \\mu_1 = \\mu_2 = \\mu_3\\) \\(H_a\\): ao menos um \\(\\mu\\) é diferente \\(\\alpha = 0.05\\) Utilizando os tados da tabela abaixo podemos obetr todas as quantias necessárias para o cálculo da ANOVA, isto é, os somatórios dos quadrados, os graus de liberdade,os quadraos médios e finalmente o valor de \\(F_{calculado}\\) X Y A 16.9 A 20.9 A 15.8 A 28.0 A 21.6 B 23.9 B 30.4 B 31.7 B 30.9 B 26.5 C 19.6 C 13.9 C 8.9 C 0.9 C 17.6 1. Somatórios dos quadrados \\(SQ_{Trat} = \\sum_{j = 1}^{k}\\sum_{i = 1}^{n_{j}}(\\overline{Y}_{j} - \\overline{\\overline{Y}})^2 = 680.772\\) \\(SQ_{Res} = \\sum_{j = 1}^{k}\\sum_{i = 1}^{n_{j}}(Y_{ij} - \\overline{Y}_{j})^2 = 362.568\\) 2. Graus de liberdade \\(gl_{Trat} = k - 1 = 2\\) \\(gl_{Res} = N-k = 12\\) 3. Quadrados médios \\(QM_{Trat} = \\frac{SQ_{Trat}}{gl_{Trat}} = 340.386\\) \\(QM_{Res} = \\frac{SQ_{Res}}{gl_{Res}} = 30.214\\) 4. Estatística \\(F\\) \\(F_{calculado} = \\frac{QM_{Trat}}{QM_{Res}} = 11.266\\) 5. Tabela da ANOVA Estas quantias são tradicionalmente expressas em uma Tabela de ANOVA conforme abaixo: Df Sum Sq Mean Sq F value Pr(&gt;F) X 2 680.772 340.386 11.26584 0.0017611 Residuals 12 362.568 30.214 NA NA em que: Df: graus de liberdade Sum Sq: soma dos quadrados Mean Sq: quadrados médios F value: valor de \\(F_{calculado}\\) Pr(&gt;F): valor de p A primeira linha refere-se aos valores associados aos tratamentos e a segunda linha aos resíduos. Note que o cômputo de \\(SQ_{Total}\\), \\(gl_{Total}\\) e \\(QM_{Total}\\) não é realmente necessário. O valor de \\(p = 11.265837\\) mostrado na tabela acima é aquele referência à área na distribuição \\(F\\) que fica acima de \\(F_{calculado}\\). Poderíamos tentar observar representar este valor visualmente na distribuição \\(F\\), mas ele é tão pequeno, que a área em vermelho sequer aparece na figura. Como conclusão temos que \\(p \\le \\alpha\\) nos leva a REJEITAR \\(H_0\\), pois \\(F_{calculado}\\) é muito extremo para ser resultante da hipótese nula. Neste caso, assumimos que a \\(H_a\\) é mais condizente com a estrutura dos dados, de modo que os tratamentos devem ser provenientes de populações estatísticas com diferentes médias \\(\\mu\\). 20.6 Testes a posteriori de comparação de médias Tendo rejeitado \\(H_0\\) concluímos que ao menos 1 par médias é diferente entre si, saber qual(is). Isto nos leva a buscar por um teste que permita fazer comparações par-a-par. Os testes a posteriori são uma alternativa. Entre os diferentes testes a posteriori na literatura discutiremos o teste de Tukey, em que o objetivo é estabelecer uma Diferença Honesta Significativa (DHS) entre um dado par de médias. Esta diferença pode ser calculada por: \\[DHS_{12} = q\\sqrt{\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)QM_{Res}}\\] onde: \\(q\\): é um valor retirado de uma tabela estatística da distribuição de amplitude normalizada (studentized range q table). Para um dado \\(\\alpha\\), o valor desejado de \\(q\\) é encontrado cruzando a linha contento o número de grupos \\(k\\) tratamentos do experimento com a linha contendo os graus de liberdade do resíduo (\\(gl_{Res}\\)). Veja um exemplo desta tabela no link: Studentized Range q Table; \\(QM_{Res}\\): é quadrado médio do resíduo obtido na ANOVA, e; \\(n_1\\), \\(n_2\\): os tamanhos amostrais de cada grupo envolvido na comparação. Para um dado nível de significância \\(\\alpha\\), a \\(DHS\\) irá depender basicamente da variação residual do modelo de ANOVA e dos tamanhos amostrais de cada grupo. Em um experimento balanceado, isto é, onde \\(n_1 = n_2 = \\cdots = n_k = n\\), a diferença mínima para que um par de médias seja cosiderado diferente é sempre a mesma. Em nosso exemplo, a \\(DHS\\) com \\(\\alpha = 0.05\\) será: \\(DHS = 3.773\\sqrt{\\left(\\frac{1}{5} + \\frac{1}{5}\\right)30.214} = 13.116\\) Assim, qualquer diferença entre pares de médias maior ou igual a \\(13.116\\) será considerada estatisticamente significativa, nos levando a concluir que aqueles grupos têm médias populacionais distintas. Em nosso exemplo, as médias dos grupos foram: Grupo A: \\(\\overline{Y}_A = 20.64\\) Grupo B: \\(\\overline{Y}_B = 28.68\\) Grupo C: \\(\\overline{Y}_C = 12.18\\) E as diferenças (\\(\\overline{Y}_{maior} - \\overline{Y}_{menor}\\)) entre elas: A B C A 0.00 NA NA B 8.04 0.0 NA C 8.46 16.5 0 Das três comparações possíveis, somente a comparação entre os grupos B e C (\\(\\overline{Y}_B - \\overline{Y}_C = 16.5\\)) foi maior que o limite estabelecido pelo teste de Tukey. Desta forma, concluímos que a ANOVA foi significativa e que somente os grupos B e C diferem entre si. 20.7 Ajustando a ANOVA no R Considere que a tabela em nosso exemplo está no objeto Tab. A ANOVA no R é feita com o comando aov. ajuste = aov(Y ~ X, data = Tab) ajuste ## Call: ## aov(formula = Y ~ X, data = Tab) ## ## Terms: ## X Residuals ## Sum of Squares 680.772 362.568 ## Deg. of Freedom 2 12 ## ## Residual standard error: 5.496726 ## Estimated effects may be unbalanced A notação Y ~ X será muito utilizada nesta seção sobre modelos lineares e lê-se como \\(Y\\) é função de \\(X\\). O comando acima fez os cálculos da ANOVA, isto é, computou as somas dos quadrados, os graus de liberdade, os quadrados médios, o \\(F_{calculado}\\) e o valor de \\(p\\). Para visualizarmos a tabela da ANOVA fazemos: anova(ajuste) ## Analysis of Variance Table ## ## Response: Y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## X 2 680.77 340.39 11.266 0.001761 ** ## Residuals 12 362.57 30.21 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note que os resultados coincidem com o que apresentamos anteriormente. Como o valor de \\(p\\) foi menor que \\(\\alpha = 0.05\\), concluimos que a ANOVA foi significativa, isto é, indicou que ao menos um par de médias difere ente si. Podemos fazer o teste a posteriori de Tukey com o comando: alfa = 0.05 TukeyHSD(ajuste, conf.level = 1-alfa) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = Y ~ X, data = Tab) ## ## $X ## diff lwr upr p adj ## B-A 8.04 -1.234654 17.3146545 0.0923564 ## C-A -8.46 -17.734654 0.8146545 0.0751622 ## C-B -16.50 -25.774654 -7.2253455 0.0012751 O resultado apresenta todas as comparações possíveis entre os grupos, mostrando as diferenças de médias, seus intervalos de confiança a \\(95\\%\\) e os valores de \\(p\\), indicando quais destas diferenças são significativas, isto é, \\(p \\le \\alpha\\). Estes resultados nos permitem concluir novamente que somente o par C-B difere entre si, pois p adj &lt; 0.05. Um gráfico facilita a visualização das comparações, sobretudo em situações com muitos pares de médias envolvidos: plot(TukeyHSD(ajuste)) Neste gráfico, as comparações em que o intervalo de confiança não inclui o zero, são consideradas significativas. Novamente, vemos que somente o grupo C-B tem médias estatisticamente diferentes. 20.8 Pressupostos da ANOVA Os pressupostos da ANOVA são: Os observação são independentes e; A variância dos resíduos é homogênea e; Os resíduos têm distribuição normal com média \\(0\\) e variância \\(\\sigma^2\\). Vamos inicialmente testar o pressuposto de homogeneidade de variâncias com um teste \\(F\\). Tab %&gt;% group_by(X) %&gt;% summarise(Var = var(Y)) Note que a maior variância é \\(56.347\\) e a menor \\(11.152\\). O teste \\(F\\) consiste em dividir a maior variância pela menor: ## ## F test to compare two variances ## ## data: Tab$Y[Tab$X == &quot;C&quot;] and Tab$Y[Tab$X == &quot;B&quot;] ## F = 5.0526, num df = 4, denom df = 4, p-value = 0.1457 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.526068 48.528196 ## sample estimates: ## ratio of variances ## 5.052636 A maior variância foi 5.05 vezes maior que a menor variância e o test F sugere que esta diferença é não-significativa a \\(5\\%\\) (\\(p &lt; 0.05\\)). Isto indica que as variâncias são homogêneas. A verificação visual de que as variâncias são homogêneas pode também ser inspecionada pelo gráfico de resíduos: Em seguida avaliamos o histograma dos resíduos e aplicamos um teste de normalidade (ex. teste de Shapiro-Wilk) para verificar se o pressuposto de normalidade pode ser aceito. ## ## Shapiro-Wilk normality test ## ## data: rstudent(ajuste) ## W = 0.93171, p-value = 0.2894 Neste caso, o valor de \\(p &gt; 0.05\\) indica não haver desvio da normalidade. "],["anovafatorial.html", "Capítulo 21 Análise de variância fatorial", " Capítulo 21 Análise de variância fatorial "],["regressao.html", "Capítulo 22 Regressão linear e correlação 22.1 Modelo geral de regressão 22.2 Ajuste dos dados ao modelo de regressão 22.3 Testes de hipóteses na regressão linear simples 22.4 Coeficiente de determinação \\(R^2\\) 22.5 Intervalo de confiança de \\(Y\\) 22.6 Pressupostos da regressão linear simples 22.7 Diagnósticos da regressão 22.8 Coeficiente de correlação de Pearson \\(r\\)", " Capítulo 22 Regressão linear e correlação Um modelo de regressão linear nos permite verificar se há uma relação funcional entre variáveis quantitativas. Nesta relação, uma variável é denominada dependente (ou variável resposta - \\(Y\\)) e as demais independentes (ou variáveis preditoras - \\(X\\)). Portanto, ao ajustar um modelo de regressão linear, estamos assumindo que existe uma relação estatística de dependencia de \\(Y\\) como função das variáveis preditoras em \\(X\\). No modelo de regressão linear simples temos somente uma variável preditora e sua relação funcional com \\(Y\\) é dada por: \\[Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\] Considere novamente os dados sobre pluviosidade anual e vazão em uma bacia hidrográfica americada, medidos entre os anos de 1958 e 1987 (disponível em: tiee.esa.org). Vamos avaliar a relação entre a vazão na bacia e os volumes de chuva. Year FlowR RainR Year FlowR RainR 1958 567.36 1161.0 1973 1396.43 1792.8 1959 918.23 1479.1 1974 890.45 1408.9 1960 752.06 1325.3 1975 939.52 1448.6 1961 436.25 978.9 1976 1022.06 1516.0 1962 699.29 1230.6 1977 843.75 1388.2 1963 662.58 1151.7 1978 613.79 1085.7 1964 630.45 1175.2 1979 1036.93 1432.7 1965 546.69 1120.6 1980 548.28 1101.1 1966 726.73 1223.2 1981 1093.91 1664.9 1967 780.76 1296.8 1982 756.12 1114.4 1968 762.84 1285.2 1983 889.35 1451.8 1969 998.68 1403.5 1984 970.65 1403.5 1970 697.53 1201.5 1985 627.84 1137.2 1971 676.19 1173.4 1986 960.94 1372.3 1972 885.91 1424.0 1987 797.09 1234.6 É razoável supor que em anos de mais chuva, seriam esperadas maiores vazões e que anos mais secos resultassem menores volumes de vazão. Para verificar esta suposição vamos fazer um gráfico de dispersão entre vazão e chuva. O gráfico sugere que a suposição faz sentido. Volumes baixos de chuva estão associados a volumes baixos de vazão e vice versa. O gráfico sugrere ainda que a relação funcional é linear. Nestas condições, faz sentido tentar modelar a relação entre estas variáveis por meio de um modelo de regressão linear simples. Ao ajustar um modelo de regressão, vemos que a linha em azul é a que melhor descreve a relação linear entre as variáveis. Esta linha nos permite obter uma estimativa sobre a vazão esperada (\\(Y\\)) para qualquer dado volume de chuva (\\(X\\)). Neste exemplo, a equação que melhor associa vazão e chuva é: \\[Y_i = -571.98 + 1.05 X_i\\] O valor de \\(\\beta_1 = 1.05\\) nos diz que para um aumento de 1 mm/area/ano de chuva, a vazão aumentará 1.05 mm/area/ano. \\(\\beta_1\\) é conhecido como coeficiente de inclinação da reta e nos fornece magnitude da variação em \\(Y\\) para um aumento de 1 unidade em \\(X\\). Esta equação prevê por exemplo, que para um volume de chuva igual a 1400 mm/area/ano a vazão na bacia será de 898 mm/area/ano. Faça as contas para conferir. \\[898.02 = -571.98 + 1.05 \\times 1400\\] A reta descreve portanto os valores preditos de vazão para cada nível de chuva. 22.1 Modelo geral de regressão A estrutura de um modelo de regressão é dada por: \\[Y_i = f(X_i, \\beta) + \\epsilon_i\\] onde \\(f(X_i, \\beta)\\) representa a parte determinística e \\(\\epsilon\\) a parte estocástica. O sulfixo i nos diz que esta expressão é dada para cada par de observação \\((Y,X)\\). 22.1.1 Porção determinística A porção determinística é um modelo matemático que descreve a relação funcional entre \\(X\\) e \\(Y\\). Os parâmetros \\(\\beta\\)’s determinam a intensidade do efeito de \\(X\\) sobre \\(Y\\). Na regressão linear simples temos somente uma variável \\(X\\), e a relação funcional é dada pela equação da reta. No modelo de regressão linear múltipla existe mais de uma variável \\(X\\). Finalmente, nos modelos de regressão não-lineares a relação funcional pode ser representada por outros modelos matemáticos (ex. função potência \\(Y = \\beta_0X^{\\beta_1}\\)). Na regressão linear simples, o parâmetro \\(\\beta_1\\) é geralmente o de maior interesse. Este parâmetro nos dirá se a relação será crescente (\\(\\beta_1 &gt; 0\\)), decrescente (\\(\\beta_1 &lt; 0\\)) ou nula (\\(\\beta_1 = 0\\)). \\(\\beta_0\\) é o \\(\\textbf{intercepto}\\) e expressa o ponto em \\(Y\\) em que a reta cruza o eixo das ordenadas. 22.1.2 Porção estocástica A porção estocástica, é representada pelo resíduo ou erro. A cada observação \\(Y_i\\) está associado um valor de resíduo correspondente (\\(\\epsilon_i\\)), dado pela distância vertical entre \\(Y_i\\) e o valor predito \\(\\hat{Y_i}\\) sobre a reta de regressão. No modelo de regressão linear que veremos aqui, os resídos são uma variável aleatória prevenientes de uma distribuição normal de probabilidades com média \\(\\mu = 0\\) e variância \\(\\sigma^2\\) constante ao longo da reta de regressão, \\(N(0, \\sigma^2)\\). 22.2 Ajuste dos dados ao modelo de regressão O ajuste de dados observados a um modelo de regressão requer a obtenção de estimativas para \\(\\beta_0\\), \\(\\beta_1\\) e \\(\\sigma^2\\), denotadas respectivamente por \\(\\hat{\\beta_0}\\), \\(\\hat{\\beta_1}\\) e \\(\\hat{\\sigma}^2\\). Note que o símbolo \\(\\hat{}\\) significa que estamos falando de estimativas obtidas a partir de dados amostrais e não dos parâmetros populacionais. Ao obter estas estimativas, podemos encontrar valores ajustados de \\(Y\\) para um dados valor de \\(X\\). Os valores ajustados de \\(Y\\) são denotados por \\(\\hat{Y}\\). \\[\\hat{Y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}X_i\\] 22.2.1 Método dos mínimos quadrados O Método dos Mínimos Quadrados (\\(MMQ\\)) é uma das formas disponíveis para calcularmos \\(\\hat{\\beta_0}\\), \\(\\hat{\\beta_1}\\) e \\(\\hat{\\sigma}^2\\). O \\(MMQ\\) envolve encontrar a combinação de \\(\\hat{\\beta_0}\\) e \\(\\hat{\\beta_1}\\) que minimiza a Soma dos Quadrados dos Resíduos (\\(SQ_{Resíduo}\\)), ou seja, que minimizam a quantia: \\[SQ_{Resíduo} = \\sum{(Y_i-\\hat{Y_ i})^2} = \\sum{(Y_i-(\\hat{\\beta_0} + \\hat{\\beta_1}X_i))^2}\\] Nas figuras acima, a linha da esquerda (\\(SQ_{Resíduo} = \\sum{\\epsilon_i^2} = 37\\)) está claramente melhor ajustada à nuvem de pontos, o que se expressa em um menor somatório dos quadrados dos resíduos (\\(SQ_{Resíduo} = \\sum{\\epsilon_i^2} = 37\\)) quando comparado com o ajuste da figura à direita (\\(SQ_{Resíduo} = \\sum{\\epsilon_i^2} = 145\\)). 22.2.2 Variâncias, covariâncias e coeficientes da regressão Para estimarmos os coeficientes da regressão \\(\\beta_0\\) e \\(\\beta_1\\) devemos retomar o conceito de variância amostral e introduzir o conceito de covariância amostral. A variância amostral de \\(Y\\) por exemplo, pode ser obtida subtraindo cada observação em \\(Y\\) de sua média (\\(\\overline{Y}\\)) e elevando esta subtração ao quadrado \\((Y_i - \\overline{Y})^2\\). Ao somar para todos os valores de \\(Y_i\\) teremos o somatório dos quadrados de \\(Y\\) (\\(SQ_Y\\)). \\[SQ_Y = \\sum_{i-1}^{n} (Y_i - \\overline{Y})^2 = \\sum_{i-1}^{n}(Y_i - \\overline{Y}) (Y_i - \\overline{Y})\\] Dividindo \\(SQ_Y\\) por \\(n-1\\) teremos a variância amostral de \\(Y\\) (\\(\\hat{\\sigma^2_Y}\\)). \\[\\hat{\\sigma^2_Y} = \\frac{\\sum_{i-1}^{n} (Y_i - \\overline{Y})^2}{n-1}\\] No capítulo ?? denominamos esta quantia simplesmente por \\(s^2\\). Aqui vamos usar uma notação diferente, pois no ajuste de um modelo de regressão haverá outros estimadores de variância envolvidos, de modo que deveremos ser mais claros a respeito de qual estimador estaremos nos referindo. Adotando o mesmo procedimento para \\(X\\), podemos calcular o somatório dos quadrados de \\(X\\) (\\(SQ_X\\)). \\[SQ_X = \\sum_{i-1}^{n} (X_i - \\overline{X})^2 = \\sum_{i-1}^{n}(X_i - \\overline{X}) (X_i - \\overline{X})\\] e a variância amostral de \\(X\\) (\\(\\hat{\\sigma^2_X}\\)). \\[\\hat{\\sigma^2_X} = \\frac{\\sum_{i-1}^{n} (X_i - \\overline{X})^2}{n-1}\\] Combinando as duas ideias, teremos o produto cruzado de \\(Y\\) e \\(X\\) (\\(SQ_{YX}\\)) \\[SQ_{YX} = \\sum_{i-1}^{n}(Y_i - \\overline{Y}) (X_i - \\overline{X})\\] e a covariância amostral entre \\(Y\\) e \\(X\\) (\\(\\hat{\\sigma}_{YX}\\)). \\[\\hat{\\sigma}_{YX} = \\frac{\\sum_{i-1}^{n}(Y_i - \\overline{Y}) (X_i - \\overline{X})}{n-1}\\] O estimador \\(\\hat{\\beta_1}\\) nada mais é que a covariância entre \\(Y\\) e \\(X\\) padronizada pela variância de \\(X\\). \\[\\hat{\\beta_1} = \\frac{\\hat{\\sigma}_{YX}}{\\hat{\\sigma^2_X}} = \\frac{\\frac{SQ_{XY}}{n-1}}{\\frac{SQ_X}{n-1}} = \\frac{SQ_{XY}}{SQ_X} = \\frac{\\sum{(Y_i - \\overline{Y})(X_i - \\overline{X})}}{\\sum{(X_i - \\overline{X})^2}}\\] \\[\\hat{\\beta_1} = \\frac{\\sum{(Y_i - \\overline{Y})(X_i - \\overline{X})}}{\\sum{(X_i - \\overline{X})^2}}\\] Após encontrar \\(\\hat{\\beta_1}\\), podemos calcular \\(\\hat{\\beta_0}\\) sabendo que a melhor reta de regressão passará necessariamente pelo ponto médio de \\(X\\) e de \\(Y\\). Deste modo temos: \\[\\hat{\\beta_0} = \\overline{Y} - \\hat{\\beta_1}\\overline{X}\\] Calculados \\(\\hat{\\beta_1}\\) e \\(\\hat{\\beta_0}\\), podemos encontrar os valores ajustados de \\(Y\\) para cada valor de \\(X\\) que serão utilizados para construir a reta de regressão. \\(\\hat{Y_i}\\) será dado por: \\[\\hat{Y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}X_i\\] Por fim, a variância residual \\(\\hat{\\sigma}^2\\) é dada por: \\[\\hat{\\sigma}^2 = QM_{Resíduo} = \\frac{SQ_{Resíduo}}{n-2} = \\frac{\\sum{(Y_i-\\hat{Y_ i})^2}}{n-2}\\] 22.2.3 Exemplo de ajuste ao modelo de regressão Considere a tabela abaixo com os dados de riqueza da macro-fauna praial (número de espécies) e de um índice de exposição às ondas (NAP). Os dados foram obtidos em 2002 na costa da Holanda em nove praias (Zuur et al. 2009). Valores negativos de NAP se referem a locais mais expostos e valores positivos a locais menos expostos à ação das ondas. Richness NAP 13 -1.336 8 0.635 4 -0.201 3 0.460 6 0.729 1 2.222 1 1.375 7 -1.005 3 -0.002 O gráfico de dispersão sugere uma relação negativa e possivelmente linear, em que a riqueza de espécies diminui com o aumento no grau de exposição. Vamos ajustar um modelo de regressão a estes pontos calculando \\(\\hat{\\beta_0}\\), \\(\\hat{\\beta_1}\\) e \\(\\hat{\\sigma}^2\\). Os passos intermediários envolvem o cálculo do somatórios dos quadrados de X: \\[SQ_X = \\sum{(X_i - \\overline{X})^2}\\] de Y: \\[SQ_Y = \\sum{(Y_i - \\overline{Y})^2}\\] e do somatório dos produtos cruzados de X e Y: \\[SQ_{XY} = \\sum{(X_i - \\overline{X}) (Y_i - \\overline{Y})}\\] Estes passos são descritos na tabela a seguir. Richness NAP \\((X_i - \\overline{X})\\) \\((Y_i - \\overline{Y})\\) \\((X_i - \\overline{X})^2\\) \\((Y_i - \\overline{Y})^2\\) \\((X_i - \\overline{X})(Y_i - \\overline{Y})\\) 13 -1.34 -1.66 7.89 2.74 62.23 -13.06 8 0.64 0.32 2.89 0.10 8.35 0.91 4 -0.20 -0.52 -1.11 0.27 1.23 0.58 3 0.46 0.14 -2.11 0.02 4.46 -0.30 6 0.73 0.41 0.89 0.17 0.79 0.36 1 2.22 1.90 -4.11 3.62 16.90 -7.82 1 1.38 1.06 -4.11 1.11 16.90 -4.34 7 -1.00 -1.32 1.89 1.75 3.57 -2.50 3 0.00 -0.32 -2.11 0.10 4.46 0.68 Após os cálculos, os valores estimados são: \\[\\hat{\\beta_1} = \\frac{\\sum{(X_i - \\overline{X})(Y_i - \\overline{Y})}}{\\sum{(X_i - \\overline{X})^2}} = \\frac{-25.49}{9.88} = -2.58\\] \\[\\hat{\\beta_0} = \\overline{Y} - \\hat{\\beta_1}\\overline{X} = 5.11 -2.58 \\times 0.32 = 5.94\\] \\[\\hat{\\sigma}^2 = QM_{Resíduo} = \\frac{SQ_{Resíduo}}{n-2} = \\frac{\\sum{(Y_i-\\hat{Y_ i})^2}}{n-2} = \\frac{53.11}{7} = 7.59\\] De modo que a melhor reta de regressão é dada por: \\[Richness = 5.94 -2.58 \\times NAP\\] 22.3 Testes de hipóteses na regressão linear simples Até o momento, apresentamos uma discussão sobre o método para calcular os estimadores \\(\\hat{\\beta_0}\\), \\(\\hat{\\beta_1}\\) e \\(\\hat{\\sigma}\\). Entretanto, como nossas observações provêm de amostras, estas estimativas estão sujeitas à variação inerente às observações de que dispomos e certamente não serão iguais ao valor da população estatística. Devemos portanto, entender quais evidências estes estimadores nos fornecem para a existência de um efeito de \\(X\\) sobre \\(Y\\), ou seja, para rejeitarmos a hipótese nula em favor de \\(H_A\\). 22.3.1 Teste sobre \\(\\beta_1\\) Na regressão linear simples, o efeito de \\(X\\) sobre \\(Y\\) depende do valor de \\(\\beta_1\\) \\(Y_i = \\beta + \\beta_1X_i + \\epsilon_i\\) A não existência de um efeito implica em \\(\\beta_1 = 0\\) e consequentemente: \\(Y_i = \\beta_0 + 0 \\times X_i + \\epsilon_i\\) \\(\\rightarrow\\) \\(Y = \\beta_0 + \\epsilon_i\\) Portanto, as hipóteses nula e alternativa seriam: \\(H_0: \\beta_1 = 0\\) \\(H_A: \\beta_1 \\ne 0\\) Segundo \\(H_0\\), a inclinação da reta \\(populacional\\) não é diferente de zero e o valor estimado \\(\\hat{\\beta_1}\\) ocorreu puramente ao acaso, como efeito da variação amostral. Para testar esta hipótese, utilizamos a distribuição de t de modo que: \\[t = \\frac{\\hat{\\beta_1} - \\beta_1}{s_{\\hat{\\beta_1}}}\\] Como segundo \\(H_0\\), \\(\\beta_1 = 0\\) a expressão fica: \\[t = \\frac{\\hat{\\beta_1} - 0}{s_{\\hat{\\beta_1}}} = \\frac{\\hat{\\beta_1}}{s_{\\hat{\\beta_1}}}\\] \\(s_{\\hat{\\beta_1}}\\) é o erro padrão de \\(\\beta_1\\) calculado por: \\[s_{\\hat{\\beta_1}} = \\sqrt{\\frac{\\hat{\\sigma}^2}{\\sum{(X_i-\\overline{X})^2}}}\\] No exemplo sobre a fauna praial estamos interessados em testar a hipótese de que a riqueza de espécies esteja associada ao grau de exposição às ondas. Em regressão linear, esta hipótese pode ser expressa por: \\[t = \\frac{\\hat{\\beta_1}}{s_{\\hat{\\beta_1}}} = \\frac{-2.58}{0.88} = -2.944\\] Que na distribuição de t fica: Se nosso nível de significancia \\(\\alpha = 0.05\\), então a probabilidade \\(p = 0.011 + 0.011 = 0.022\\) indica que devemos rejeitar \\(H_0\\) e aceitar que existe uma relação entre Riqueza de espécies e NAP. 22.3.2 Análise de variância da regressão Como já dizemos, a estrutura de um modelo de regressão é dada por um componente sistemático expresso como função de \\(X\\) (\\(\\beta_0 + \\beta_1X_i\\)) e um componente aleatório expresso pelos resíduos do modelo (\\(\\epsilon_i\\)). A variação total em \\(Y\\) no modelo de regressão portanto, pode ser atribuída a ambos os efeitos de \\(X\\) e do resíduo. Estas quantias de variação podem mensuradas pelos somatório dos quadrados abaixo. Soma dos quadrados totais: \\(SQ_Y = \\sum{(Y_i - \\overline{Y})^2}\\) Soma dos quadrados da regressão: \\(SQ_{Regressão}= \\sum{(\\hat{Y_i} - \\overline{Y})^2}\\) E soma dos quadrados do resíduo: \\(SQ_{Resíduo}= \\sum{(Y_i - \\hat{Y_i})^2}\\) Pode-se mostrar ainda que vale a expressão: \\[SQ_Y = SQ_{Regressão} + SQ_{Resíduo}\\] A decomposição destas quantias é conhecida partição das somas dos quadrados e nos permitem comparar a influência de \\(X\\) com a influência do puro acaso sobre a variabilidade em \\(Y\\). Se todos os pontos estiverem perfeitamente sobre a reta, então toda a variação em \\(Y\\) seria atribuída à influência de \\(X\\). Por outro lado, à medida que aumenta a distância média dos pontos acima e abaixo da curva, aumenta a parcela atribuída ao acaso. Estes componentes de variação podem ser organizados em uma Tabela de Análise de Variância (ANOVA). \\(n\\) se refere ao número de amostras. Fonte de variação SQ gl QM F p Regressão \\(SQ_{Regressão}\\) \\(gl_{Regressão}\\) \\(QM_{Regressão} = \\frac{SQ_{Regressão}}{gl_{Regressão}}\\) \\(\\frac{QM_{Regressão}}{QM_{Resíduo}}\\) Probabilidade associada à cauda da distribuição F Resíduo \\(SQ_{Resíduo}\\) \\(gl_{Resíduo}\\) \\(QM_{Resíduo} = \\frac{SQ_{Resíduo}}{gl_{Resíduo}}\\) Total \\(SQ_{Y}\\) \\(gl_{Y}\\) \\(QM_{Y} = \\frac{SQ_{Y}}{gl_{Y}}\\) As coluna \\(gl\\) se refer aos graus de liberdade nos modelo de regressão, a semelhança do que discutimos para o teste t de Student. A coluna QM (Quadrado médio) apresenta os estimadores de variância da regressão (\\(QM_{Regressão}\\)), do resíduo (\\(QM_{Resíduo}\\)) e total (\\(QM_{Y}\\)). 22.3.2.1 A distribuição F O valor de \\(F\\) na tabela se refere a distribuição de probabilidade F. Esta distribuição de probabilidades é esperada para a razão entre duas variâncias amostrais. No caso da regressão linear, estas são a variância da regressão (\\(QM_{Regressão}\\) no numerador) e a variância residual (\\(QM_{Resíduo}\\) no denominador). Diferente da distribuiçao t, a distribuição F tem um formato assimétrico, sendo que o grau de assimetria depende dos graus de liberdade do numerador e do denominador. O valor de \\(p\\) na tabela se refere à área sob a distribuição F, acima do valor de \\(F\\) calculado. Na ANOVA da regressão, um valor de \\(p &lt; \\alpha\\) nos leva a rejeitar a hipótese nula e assumir que a variável \\(X\\) exerce algum efeito sobre \\(Y\\). O símbolo \\(F\\) foi dado em homenagem a Ronald Aylmer Fisher o estatístico e geneticista Britânico do início do séc. XX, que entre inúmeras outras contribuições, desenvolveu a Análise de Variância. Fisher é descrito como “a genius who almost single-handedly created the foundations for modern statistical science” (Halt 1998) e como “the single most important figure in 20th century statistics” (Efron 1998). Ver Ronald Aylmer Fisher. Os resultados da ANOVA para os dados da fauna praial nos dá os seguintes valores. Confira os cálculos. Fonte de variação SQ gl QM F p Regressão 65.68 1 65.68 8.64 0.022 Resíduo 53.21 7 7.60 NA NA Total 118.89 8 14.86 NA NA O valor de \\(p = 0.022\\) abaixo do nível de significância \\(\\alpha = 0.05\\), nos leva a rejeitar a hipótese nula em favor da alternativa, concluindo que o índice de exposição às ondas interfere sobre a riqueza da macro-fauna. O valor de \\(p\\) foi identico ao obtido no teste de hipóteses de \\(\\beta_1\\). No modelo de regressão linear simples isto é necessariamente verdadeiro, pois toda a variação associada à regressão é devida ao efeito do coeficiente \\(\\beta_1\\). Por outro lado, nos modelos de regressão múltipla, em que temos: \\[Y_i = \\beta_0 + \\beta_1X_{i1} + \\beta_1X_{i2} + \\cdots + \\beta_mX_{im} + \\epsilon_i\\] esta relação não é mais observada, pois existem múltiplos coeficientes agindo sobre a variação em \\(Y\\). 22.4 Coeficiente de determinação \\(R^2\\) Uma vez que toda a variação observada em Y pode ser alocada aos efeitos da reta de regressão e e do resíduo podemos fazer a seguinte questão: Qual parcela da variação na Riqueza é explicada exclusivamente pelo modelo de regressão? Esta pergunta pode ser respondida calculando o que denominamos de coeficiente de determinção ou simplesmente \\(R^2\\): \\[R^2 = \\frac{SQ_{Regressão}}{SQ_Y} = 1 - \\frac{SQ_{Resíduo}}{SQ_Y}\\] O valor de \\(SQ_{Regressão}\\) mede a variação explicada exclusivamente pela regressão, \\(SQ_{Resíduo}\\) a variação residual e \\(SQ_Y\\) mede a variação total em \\(Y\\). Ao dividir \\(SQ_{Resíduo}\\) por \\(SQ_Y\\), o \\(r^2\\) nos informa sobre qual a fração da variação total é explicada somente pela reta de regressão. Npo exemplo da fauna praial: \\[R^2 = 1 - \\frac{53.11}{118.89} = 0.5533\\] O que significa que aproximadamente 55.33% da variação na riqueza é explicada pela variação no grau de exposição às ondas (NAP). Não sabemos a que se deve o restante da variação e, no contexto do modelo de regressão, assumimos ser uma variação aleatória inerente a cada observação (\\(\\epsilon_i\\)). Esta variação aleatória, como dito, segue uma distribuição normal com ponto central sobre a reta e variância data por \\(\\sigma^2\\). Este pressuposto é fundamental para a discussão do próximo ponto a respeito do intervalo de confiança de \\(Y\\) 22.5 Intervalo de confiança de \\(Y\\) Como nem todos os pontos caem perfeitamente sobre a reta, seria interessante que pudéssemos obter um intervalo de confiança de \\(Y\\) para um dado valor de \\(X\\). A amplitude deste intervalo irá depender da variância dos valores ajustados (\\(\\hat{\\sigma}^2_{Y|X}\\)) de \\(Y\\), calculada por: \\[\\hat{\\sigma}^2_{Y|X} = \\hat{\\sigma}^2(\\frac{1}{n} + \\frac{(X_i-\\overline{X})^2}{SQ_X})\\] do modo que: \\[\\hat{\\sigma}_{Y|X} = \\sqrt{\\hat{\\sigma}^2(\\frac{1}{n} + \\frac{(X_i-\\overline{X})^2}{SQ_X})}\\] Note pela expressão acima que o \\(\\hat{\\sigma}_{Y|X}\\) diminui quanto: a variância residual \\(\\hat{\\sigma}^2\\) diminui; o tamanho amostral \\(n\\) aumenta. o dado valor de \\(X_i\\) está próximo à média, pois neste caso \\((X_i-\\overline{X})\\) diminui. Encontrado \\(\\hat{\\sigma}_{Y|X}\\), o intervalo de confiança de \\(Y\\) é dado por: \\[IC_{Y} = \\hat{Y}\\pm t_{(\\alpha, n-2)} \\times \\hat{\\sigma}_{Y|X}\\] Para os dados da macrofauna, vamos exemplificar o cálculo de \\(IC_{95\\%}\\) para a \\(4^a\\) observação da tabela, em que Richness = 3 e NAP = 0.46. Lembre-se que já estimamos anteriormente a variância residual destes dados (\\(\\hat{\\sigma}^2 = 7.59\\)). Como temos 9 observações, o valor de \\(t_{(\\alpha, n-2)} = 2.36\\), portanto: \\(\\hat{\\sigma}_{Y|X} = \\sqrt{\\hat{\\sigma}^2(\\frac{1}{n} + \\frac{(X_i-\\overline{X})^2}{SQ_X})} = 0.93\\) O valor estimado de riqueza neste ponto é 4.75, portanto: \\(IC_{Y} = \\hat{Y} \\pm t_{(\\alpha, n-2)} \\times \\hat{\\sigma}_{Y|X} = 4.75 \\pm 2.36 \\times 0.93\\) \\(IC_{Y} = 4.75 \\pm 2.19\\) \\(IC_{Y_{limite superior}} = 6.94\\) \\(IC_{Y_{limite inferior}} = 2.56\\) Podemos calcular intervalos destes para todos os pontos observados como expresso na tabela abaixo. Richness NAP \\(\\hat{Y}\\) \\(\\hat{\\sigma}_{Y \\mid X}\\) \\(IC_{inferior}\\) \\(IC_{superior}\\) 13 -1.34 9.40 1.72 5.34 13.46 8 0.64 4.29 0.96 2.02 6.56 4 -0.20 6.46 1.03 4.04 8.88 3 0.46 4.75 0.93 2.56 6.94 6 0.73 4.06 0.99 1.73 6.39 1 2.22 0.21 1.90 -4.29 4.71 1 1.38 2.38 1.30 -0.70 5.46 7 -1.00 8.52 1.48 5.02 12.02 3 0.00 5.94 0.96 3.67 8.21 E representá-los graficamente, juntamente com os valores ajustados de Y. Note que na figura acima, estão representados os valores observados de riqueza de espécies (em preto), os valores ajustados (azul) e os intervalos a 95% (vermelho). Os valores ajustados são aqueles utilizados para construir a reta de regressão. O intervalo não costuma ser representados por pontos individuais, mas por uma banda que delimita a área que restringe o intervalo de confiança ao nível \\(1 - \\alpha\\) como na figura abaixo. A banda mais estreita próxima ao ponto médio de \\(X\\), reflete o ponto comentado anteriormente, de que quanto mais próximo ao centro da distibuição de pontos, mais confiança temos sobre os limites máximos e mínimos que um valor de \\(Y\\) pode assumir. Do mesmo modo, esta confiança diminui à medida que nos aproximamos dos extremos dos valores observados em \\(X\\). 22.6 Pressupostos da regressão linear simples Ao realizar uma regressão linear simples, devemos assumir como verdadeiros alguns pressupostos. O modelo linear descreve adequadamente a relação funcional entre \\(X\\) e \\(Y\\); Cada par de observação \\((X,Y)\\) é independente dos demais; A variável \\(X\\) é medida sem erros; Os resíduos têm distribuição normal, e; A variância residual \\(\\sigma^2\\) é constante ao longo dos valores de \\(X\\). 22.6.1 Relação funcional linear Caso a relação funcional entre \\(X\\) e \\(Y\\) assuma uma forma diferente de \\(Y_i = \\beta_0 + \\beta_1X_i\\), o modelo de regressão não é mais válido, pois a estimativa de erro irá conter, além do componente aleatório residual, um componente sistemático. Este componente terá efeito sobre influência sobre a predição do modelo, sobretudo nos extremos das observações. Por modelo linear, entendemos aqueles em que os coeficientes \\(\\beta\\) aparecem de forma aditiva. Modelos em que os componentes aparecem de outro modo na equação como potência ou no denominador de uma equação são exemplos de modelos não-lineares. Abaixo estão dois exemplos de relações não-lineares comumente observadas em fenômenos ambientais: Equação potência: \\(Y_i = \\beta_0 X_i^{\\beta_1}\\) Modelo de Michaelis-Menten: \\(Y_i = \\frac{\\beta_0 X_i}{\\beta_1 + X_i}\\) 22.6.2 Independência A falta de independência pode ocorrer como resultado do delineamento amostral inapropriado para a questão em teste. A falta de independência torna crítico o uso de uma distribuição de probabilidade para o cálculo do intervalo de confiança (distribuição \\(t\\)) e para o teste de hipóteses (distribuições \\(t\\) e \\(F\\) ). Casos clássicos de falta de independência são aqueles em que as observações são denominadas como pseudoréplicas (Hurlbert 1984). Após a publicação clássica de Hurlbert, muito tem sido dito sobre pseudoreplicação. Em experimentos de campo, a falta de independência ocorre geralmente como resultados da proximidade espacial entre as réplicas ou sobre séries temporais. 22.6.3 Variável \\(X\\) é medida sem erros Veja que a parcela residual do modelo de regressão se refere à distância vertical de \\(Y_i\\), para um dados valor de \\(X\\). Isto implica que os níveis de \\(X\\) são previamente definidos. Quando existe variabilidade aleatória tanto em \\(Y\\) quanto em \\(X\\), o modelo correto para a estimativa dos parâmetros da regressão é conhecido como Modelo II de regressão. Este pressuposto é frequêntemente ignorado em delineamentos de regressão, sobretudo em estudos observacionais, o que não parece ser particularmente problemático. 22.6.4 Distribuição normal dos resíduos Assim como no pressuposto de independência, assumir que os resíduos têm uma distribuição normal permite o uso da distribuição \\(F\\) pra o teste de hipótese e da distribuiçãio \\(t\\) para o cálculo do intervalo de confiança. Uma distribuição de erros diferente da distribuição normal terá influência sobre o cálculo da amplitude do intervalo de confiança. 22.6.5 Variância residual constante Caso, a variância \\(\\sigma\\) não seja constante ao longo da reta de regressão, o cálculo do intervalo de confiança e o resultado do teste de hipóteses são afetados. Uma vez diagnosticada uma variância não-constante existem modelos de regressão que podem ser apicados para incorporar este efeito em suas estimativas (Zuur et al. 2009). 22.7 Diagnósticos da regressão O diagnóstivo da regressão é composto por observações e testes que ajudam a decidirmos se a regressão linear foi um bom modelo para ajustar a um conjunto de dados particular. Um bom modelo neste contexto significa um modelo que atendeu aos pressuostos descritos acima. Esta verificação passa pela observação de padrões nos resíduos da regressão, ou seja, pela observação da parcela estocástica do modelo. 22.7.1 Gráfico de resíduos O primeiro diagnóstico da regressão é conhecido como gráfico de resíduos, que consiste em um gráfico de dispersão entre os resíduos e o valor ajustado \\(\\hat{Y}\\). Abaixo estão os gráficos de resíduos que surge quando ajustamos uma reta a dados que apresentam uma relação linear, uma função potência, uma função assintótica e uma relação linear porém comm variância heterogênea. Nas primeiras duas figuras, em que a relação é linear, vemos um padrão crescente de \\(Y\\) como função de \\(X\\) (figura da esquerda), em que os pontos estão aleatóriamente acima e abaixo da reta de regressão. Este padrão se reflete em um gráfico de resíduos (figura da direita) em que os pontos ficam aleatóriamente acima e abaixo de zero expressando resíduos positivos e negativos respectivamente. Em uma situação em que os pontos estivessem perfeitamente sobre a reta, os resíduos seriam todos iguais a zero e o gráfico de resíduos mostraria todos os pontos alinhados horizontalmente em zero. Quando a relação é potência e tentamos ajustar uma reta sobre, vemos que inicialmente os resíduos sao positivos, o seja, estão acima da reta. Os resíduos se tornam negativos no centro da nuvem de pontos e novamente positivos ao final do gráfico. Este padrão é mais evidente no gráfico de resíduos, que mostra um componente sistemático dos resíduos como fução do valor ajustado. Ao usar uma regressão linear neste caso, iríamos subestimar consistentemente os valores de Y nos extremos da figura e superestimlá-los no trecho central. Portanto, uma reta de regressão, quando ajustada a um conjunto de dados que expressa um padrão não-linear, não é capaz de isolar adequadamente as parcelas aleatórias e sistemáticas da relação entre \\(Y\\) e \\(X\\). Isto pode ser corrigido aplicando-se uma regressão não-linear aos dados. Quando a relação é assintótica, o resultado do ajuste foi inverso ao anterior. De fato, resultados análogos serão observados senpre que tentarmos ajustra uma regressão linear a dados que expressam padrões não-lineares. No último exemplo (variância heterogênea) os pontos tendem a se afastar consistentemente da reta de regressão conforme aumentam os valores de \\(X\\). Isto denota que o pressuposto de variância \\(\\sigma^2\\) constante não é válido nesta relação. Isto pode ser corrigido aplicando-se um modelo de regressão linear com variância heterogênea. 22.7.2 Histograma dos resíduos Outro diagnóstico da regressão consiste em fazer um histograma dos gráficos de resíduos. Um histograma, aproximadamente simétrico ao redor de zero o que sugere que o pressuposto de normalidade dos resíduos é válido neste caso. Existem testes formais de normalidade cmo o teste de Kolmogorov Smirnov ou o teste de Shapiro-Wilk. 22.8 Coeficiente de correlação de Pearson \\(r\\) Como dito no início do capítulo, ao ajustar uma regressão linear a um conjunto e dados, assumimos haver uma relação de dependência estatística de \\(Y\\) como função de \\(X\\). No entanto, por vezes estamos interessados em verificar a associação entre duas variáveis quantitativas sem assumir uma relação de dependência. Veja o exemplo apresentado em (Haddon 2010) sobre a pesca do camarão tigre e do camarão rei entre nos anos de 1976 a 1987. O camarão tigre constitui a espécie alvo da pesca, enquanto o camarão rei aparece como uma espécie acidental. Ano Camarão tigre Camarão rei 1976 566 10 1977 1437 22 1978 1646 42 1979 2056 33 1980 3171 64 1981 2743 34 1982 2838 59 1983 4434 146 1984 4149 78 1985 3480 75 1986 2375 81 1987 3355 52 Veja nas figuras acima que a captura em toneladas do camarão tigre é sempre mais elevada. Entretanto, a figura da direita sugere haver uma relação linear entre as capturas. Nos anos em que houve maiores capturas do camarão tigre parece ter havido também um aumento nas capturas do camarão rei. Dizemos que existe uma correlação positiva entre a captura das duas espécies. Em nenhum momento estamos dizendo que a abundância de uma espécie é a causa do aumento na captura da outra. Muito provavelmente, as abundâncias das duas espécies estão relacionadas a um terceiro fator que gera um comportamento similar na variação das capturas ano a ano. Portanto, aqui não há como se falar em variável dependente e independente. Podemos no referir apenas a uma correlação aparentemente linear entre duas variáveis. Quando estamos interessados em medir o grau de correlação entre duas variáveis utilizamos o coeficiente de correlação de Pearson (\\(r\\)) (Capítulo 10). O coeficiente \\(r\\) mede a intensidade da correlação linear entre \\(Y\\) e \\(X\\). Como não há variável dependente e independente, pouco importa no exemplo acima, a quem chamaremos de \\(Y\\) ou de \\(X\\). De fato, seria mais correto denominarmos simplesmente as variáveis de \\(X_1\\) e \\(X_2\\), porém vamos seguir utilizando a nomenclatura que vimos ao longo do capítulo. Vimos que a covariância amostral (\\(\\sigma_{YX}\\)) mede a intensidade de uma associação linear entre \\(Y\\) e \\(X\\). A covariância entretanto, não tem limite superior ou inferior. Vimos também que valor de \\(\\beta_1\\) é dado pela covariância entre \\(Y\\) e \\(X\\) padronizada pela variância de \\(X\\) o que expressa a noções de dependência de \\(Y\\) como função de \\(X\\). A obtenção do coeficiente de correlação \\(r\\) também parte do cálculo da covariância entre \\(Y\\) e \\(X\\) porém padronizada pelo produto dos desvios padrões de \\(Y\\) e de \\(X\\). \\[r = \\frac{\\hat{\\sigma}_{YX}}{\\hat{\\sigma}_Y \\hat{\\sigma}_X} = \\frac{\\frac{\\sum{(Y_i - \\overline{Y})(X_i - \\overline{X})}}{n-1}} {\\sqrt{\\frac{\\sum{(Y_i - \\overline{Y})^2}}{n-1}} \\times \\sqrt{\\frac{\\sum{(X_i - \\overline{X})^2}}{n-1}}}\\] \\[r = \\frac{\\sum{(Y_i - \\overline{Y})(X_i - \\overline{X})}}{\\sqrt{\\sum{(Y_i - \\overline{Y})^2 \\sum{(X_i - \\overline{X})^2}}}}\\] A expressão acima, nos dá um índice que pode variar entre +1 (correlação perfeitamente linear e positiva) e -1 (correlação perfeitamente linear e negativa), e que se aproxima de zero quando não existe correlação. Em nosso exemplo a intensidade da correlação linear entre as abudâncias dos camarões tigre e rei é \\(r = 0.82\\) (confira os cálculos). 22.8.1 Teste de hipóteses para \\(r\\) O coeficiente de correlação que calculamos provém de dados amostrais das capturas em anos específicos, e portanto é um estimador do coeficiente de correlação da população estatística. Enquanto denotamos por \\(r\\) o coeficiente de correlação amostral, o coeficiente de correlação populacional é denominado de \\(\\rho\\). Como \\(r\\) provém de dados amostrais, seria interessante testarmos a hipótese de que o valor estimado implica em evidência de que \\(\\rho \\ne 0\\). Assim, as hipóteses em teste sobre a correlação linear entre duas variáveis são: \\(H_0: \\rho = 0\\) \\(H_a: \\rho \\ne 0\\) Estas hipóteses são testadas pelo cálculo da estatística \\(t\\) de Student que já nos é familiar, sendo o valor calculado \\(t_c\\) comparado à distribuição \\(t\\) de Student com nível de significância \\(\\alpha\\) e \\(n-2\\) graus de liberdade. Neste caso temos: \\[t_c = \\frac{r}{\\hat{\\sigma_r}} = \\frac{r}{\\sqrt{\\frac{1-r^2}{n-2}}}\\] Que para nosso exemplo será: \\[t = \\frac{0.82}{\\sqrt{\\frac{1-0.82^2}{12-2}}} = 4.53\\] Quando comparado à distribuição t, a área da curva aos extremos de 4.53 e -4.53 nos dá uma probabilidade de \\(p = 0.0011\\), o que está muito abaixo do nível de significância \\(\\alpha = 0.05\\). Portanto, aceitamos que um valor de \\(r = 0.82\\) é evidência suficiente para rejeitarmos \\(H_0\\) e concluirmos que há uma associação positiva para as capturas das duas espécies. References "],["regresmultipla.html", "Capítulo 23 Regressão linear múltipla", " Capítulo 23 Regressão linear múltipla "],["ancova.html", "Capítulo 24 Análise de covariância", " Capítulo 24 Análise de covariância "],["repanova.html", "Capítulo 25 Análise de variância de medidas repetidas", " Capítulo 25 Análise de variância de medidas repetidas "],["espacoamostral.html", "Capítulo 26 O espaço de possibilidades de um experimento 26.1 Probabilidades de um evento", " Capítulo 26 O espaço de possibilidades de um experimento O peixe acará, Geophagus brasiliensis é uma espécie de peixe comumente encontrada em rios e riachos de mata atlântica no siudeste do Brasil. Uma das táticas de forrageamento e alimentação recentemente descrita para a espécie é chamada de virar para pegar (Souza, Sabino, and Garrone-Neto 2019) que envolve virar objetos presentes no substrato (folhas, galhos e cascas de árvores) para encontrar e capturar macroinvertebrados ocultos sob estes objetos. Veja o vídeo. Ao realizar o ato de virar uma estrutura o indivíduo não sabe se irá encontrar e capturar um alimento. Em probabilidade, podemos chamar esta ação de um experimento aleatório. Um experimento aleatório é aquele que produz um resultado que iremos conhecer somente após sua realização. No entanto, ainda que não saibamos o resultado que uma realização específica do experimento, sabemos quais são os possíveis resultados. Neste exemplo vamos assumir que existem unicamente dois resultados possíveis para o ato de virar para pegar: \\(\\{(captura), (não-captura)\\}\\) Denominamos de evento, cada um destes resultados pssíveis e, ao conjunto de todas as possibilidades denominamos de espaço amostral do experimento aleatório. Geralmente utilizamos o símbolo \\(\\Omega\\) para nos referir ao espaço amostral. Neste caso: \\(\\Omega = \\{(captura), (não-captura)\\}\\) Experimento aleatório: aquele que possui resultados possíveis mas que são observados somente APÓS a realização do experimento. Espaco amostral: conjunto de TODAS as possibilidades de um experimento aleatório. Evento: cada resultado de um experimento aleatório. 26.1 Probabilidades de um evento Embora não saibamos qual será o resultado de um experimento particular, podemos nos perguntar sobre qual a chance de ocorrência de cada evento, neste caso, qual a chance de um alimento ser capturado. Em probabilidade perguntamos sobre qual a probabilidade de captura \\(P(captura)\\). Uma probabilidade \\(P(captura) = 0\\) significa que o animal jamais irá capturar um alimento ao virar um objeto. Por outro lado, \\(P(captura) = 1\\) significa que irá capturar um alimento sempre que virar um objeto. No mundo real, a probabilidade será alguma coisa entre estes dois extremos, ou seja, \\(0 \\le P(captura) \\le 1\\). Como podemos estimar esta probabilidade? A resposta é: observando! ou experimentando! Delineamos um estudo em que um observador irá encontar um animal e seguí-lo, contando o número de objetos virados (\\(\\#viradas\\)) e o número de capturas (\\(\\#capturas\\)). Digamos que em um dia de observação o animal vire 10 objetos e capture 4 itens. Nossa estimativa da probabilidade de captura será: \\[P(captura) = \\frac{\\#capturas}{\\#viradas} = \\frac{4}{10} = 0.4\\] Naturalmente, como os dois únicos eventos do espaço amostral são \\((captura)\\) e \\((não-captura)\\), a probabilidade de não-captura é: \\[P(não-captura) = \\frac{\\#não-capturas}{\\#viradas} = \\frac{6}{10} = 0.6\\] e como estes eventos, além de serem únicos no espeço amostral são também mutuamente exclusivos, temos que: \\[P(captura) + P(não-captura) = 1 = P(\\Omega)\\] Por evento mutuamente exclusivos entendemos que ambos não podem ocorrer ao mesmo tempo, ou seja, a ocorrência de um exclui a possibilidade da ocorrência do outro. A probabilidade de \\((não-captura)\\) é conhecida como o complemento da probabilidade de \\((captura)\\). Utilizando uma notação comum à operações com conjuntos, também poderíamos escrevê-la como: \\[P(não-captura) = P(\\overline{captura})\\] 26.1.1 Estimando as probabilidades de um evento por amostragem O valor acima descreve o resultado para a observação de um único indivíduo em um único dia. No entanto, esperamos que existam dias melhores e dias piores para os peixes e uma série de fatotres pode interferir nestes resultados. Por exemplo, como Geophagus brasiliensis é uma espécie visualmente orientada, poderíamos supor que em dias de água turva (como após uma chuva intensa) um indivíduo tenha menor sucesso de captura. Poderíamos supor ainda que o sucesso de captura pode depender da habilidade individual, tamanho do predador, do tamanho da presa, de locais com maior densidade de presas, ou de predadores, etc. Todas esta possíveis causas, além do simples acaso, podeam fazer o número relativos de capturas varie dia-a-dia. Para estimarmos a chance de captura devemos portanto repetir este experimento. Suponha então que o experimento seja repetido em 30 dias diferentes de observação e que os resultados fossem: Com base nestes resultados temos estimativas que variam entre 0 e 0.23. Veja que em 6 dias não foi observada nenhuma captura, \\(\\hat{P}(captura) = 0\\). Isto não significa que a captura seja impossível, mas que neste dia em particular, o sucesso foi muito baixo. Tivemos também dias com elevada captura, por exemplo \\(\\hat{P}(captura) = 0.23\\). Isto não significa que o sucesso é sempre desta forma. Na realidade tamanho sucesso parece ser improvável. Diante destes resultados, poderíamos estimar que em média a probabilidade de captura esteja em \\(\\overline{P}(captura) = 0.09\\) A estimativa de \\(\\overline{P}(captura) = 0.09\\) resulta de um número finito de repetições do experimento, é portanto uma estimativa da verdadeira probabilidade de captura. Lembre-se que: Ao jogar uma moeda para o alto exatamente 10 vezes, provavelmente não teremos exatamente 5 caras e 5 coroas. Esta proporção esperada pode ser somente aproximada à medida que aumentemos indefinidamente o número de jogadas. O mesmo vale para o experimento de captura que descrevemos acima. O valor exato de \\(P(captura)\\) só será observado após infinitas observações deste experimento. Obs.: Note que inseri o símbolo \\((\\hat{})\\) para identificar que esta probabilidade não se refere ào valor verdadeiro, mas a uma estimativa deste valor, obtiva por meio de amostragem. Da mesma forma, o símbolo \\((\\overline{})\\) identifica o valor da média amostral dos resultados obtidos após a realização dos 30 do experimento. Aqui valem os conceitos de população estatística e amostra (Capítulo ??). O valor verdadeiro da probabilidade de captura é o valor esperado na população estatística deste tipo de experimento, enquanto os valores observados são resultados obtidos a partir de amostras particulares. Nos referimos a cada uma destas amostras como réplicas do experimento. A partir deste conjunto de réplicas, poderíamos calcular uma média aritmética e um intervalo de confiânça para \\(P(captura)\\), o que seria nossa melhor estimativa para o fenômeno. Os conceitos de probabilidade são essenciais ao delineamento experimental, pois modelos probabilísticos nos dizem sobre quais são os resultados possíveis deste experimento e sobre quais as chances de observação de cada resultado. Assim, temos uma expectativa sobre o que vamos encontrar no futuro. Após a realização do experimento, utilizamos métodos e técnicas estatísticas para analizar os resultados e tirar concluções sobre a ocorrência destes rsultados a luz de um ou mais modelos probabilísticos. Neste caso, podemos pensar nos modelos probabilísticos como as hipóteses que serão contrastadas com os resultados obtidos após a realização do experimento. Iremos explorar a fundo estas questões nos capítulos ?? a 38. References "],["probregras.html", "Capítulo 27 Combinando as probabilidades de eventos 27.1 Eventos complexos", " Capítulo 27 Combinando as probabilidades de eventos No capítulo anterior, falamos sobre espaço amostral e a definição de probabilidade. Aqui vamos combinar eventos de um espaço amostral e calcular as probabilidades de eventos complexos. 27.1 Eventos complexos Vamos assumir que o sucesso na capacidade de forrageamento de um organismo depende de uma série de fatores relacionados à densidade da presa, à capacidade sensorial do predador, de sua eficiência em capturar/manipular o alimento, entre outros fatores. Para exemplo do Acará (Capítulo 26), o sucesso depende de virar a estrutura certa, a que tenha maior número de presas disponíveis. Para simplificar consideremos a seguintes condições. Existem dois tipos de estruturas principais sobre as quais o acará forrageia: folhas e galhos. Folhas podem ter entre 0 e 6 itens enquanto galhos podem ter entre 0 e 4 itens. Uma vez que o predador vira uma estrutura ele se alimenta de todos os itens presentes. Certamente, esta é uma situação altamente hipotética, mas que facilita nossa discussão. A pergunta aqui é: ao virar uma estrutura, de quantos itens um predador poderá se alimentar? Vamos as possibilidades. Denominemos por \\(F\\) ou \\(G\\) o encontro de uma folha ou galho respectivamente e de \\(0\\) a \\(n\\), o número de itens encontrados. O espaço amostral do experimento virar uma estrutura e contar o número de itens consiste de: \\(\\Omega = \\{(F0), (F1), (F2), (F3), (F4), (F5), (F6), (G0), (G1), (G2), (G3), (G4)\\}\\) Neste experimento temos 12 eventos simples e mutuamente exclusivos Considere agora o evento \\(A\\) “virar uma folha”. que pode ocorrer pela observação de F0 ou F1 ou F2 ou F3 ou F4 ou F5 ou F6. \\(A = \\{(F0), (F1), (F2), (F3), (F4), (F5), (F6)\\}\\) 27.1.1 Representação de eventos: diagrama de Venn Uma forma de visualizar o espaço amostral e eventos deste espaço é pela construção de diagramas de Venn. Para isto, vamos considerar também o evento \\(B\\) “encontrar mais de 3 itens” que consiste de: \\(B = \\{(F3), (F4), (F5), (F6), (G3), (G4)\\}\\) Observando os eventos \\(A\\) e \\(B\\) em um Diagrama de Venn temos: Cada um destes eventos é denominado de evento complexo, pois pode ser obtido pela combinação de eventos simples. O evento \\(B\\) por exemplo, pode ocorrer quando é encontrado uma folha OU um galho com mais de 3 itens, enquanto o evento \\(A\\) ocorre quando é encontrada uma folha com \\(0\\) OU \\(1\\) OU \\(2\\) OU \\(3\\) OU \\(4\\) OU \\(5\\) OU \\(6\\) itens. Estes eventos complexos foram obtidos por meio da união de eventos simples. A declaração OU significa que qualquer uma destas observações é suficiente para dizer que o evento ocorreu. Ou seja, \\(A\\) pode ocorrer por 7 formas e \\(B\\) por 6 formas distintas. Note que as observações \\((G0)\\), \\((G1)\\) e \\((G2)\\) não pertencem aos eventos \\(A\\) nem \\(B\\), ainda que estejam no espaço amostral \\(\\Omega\\) de possibilidades. Considere agora o evento \\(C\\) virar uma folha mais de 3 itens. \\(C\\) pode ocorrer a partir de F3 ou F4 ou F5 ou F6. No diagrama de Venn, vemos que estas opções consistem da intersecção de \\(A\\) e \\(B\\). Ou seja, as observações que satisfazem \\(C\\) em ambos os eventos anteriores. Você verá em livros de probabilidade que o termo de união é representado pelo símbolo \\(\\cup\\), enquanto o termo de intersecção é representado por \\(\\cap\\). Assim, temos que \\(C\\) pode ser excrito como \\(A \\cap B\\). 27.1.2 Probabilidade de eventos simples Considerando o experimento virar uma estrutura e contar o número de itens, qual seria a probabilidade da ocorrência de cada observação? Para isto, devemos inicialmente lembrar que: O espaço amostral consiste de \\(N = 12\\) observações e; Definir um modelo de probabilidade para cada uma destas observações. Neste tópico vamos assumir um modelo de probabilidade uniforme em que cada observação tem a mesma probabilidade. Iremos assumir este modelo somente para iniciar a discussão. Outros tipos de modelos com suas respectivas aplicações serão considerados nos capítulo 30. Portanto, assumindo este modelo uniforme, cada observação individual tem probabilidade \\(\\frac{1}{N}\\). Diante disto, a probabilidade da ocorrência do evento \\(A\\) será o número de ocorrências favoráveis a \\(A\\) dividido pelo número de resultados do espaço amostral. Como \\(A\\) consiste de 7 observações, então: \\[P(A) = \\frac{7}{12} = 0.58\\] Naturalmente, a probabilidade de \\(A\\) não ocorrer será: \\[P(\\overline{A}) = 1 - \\frac{7}{12} = 1 - 0.58 = 0.42\\] Obs. O símbolo \\(\\overline{A}\\) significa todas as observações que não pertencem a \\(A\\). Considere também a probabilidade do evento \\(B\\): \\[P(B) = \\frac{6}{12} = 0.5\\] e do evento \\(C = A \\cap B\\) \\[P(C) = P(A \\cap B) = \\frac{4}{12} = 0.33\\] 27.1.3 Probabilidade da união de eventos Vamos ao evento \\(A \\cup B\\) que consiste de todas a observações que estejam em \\(A\\) ou \\(B\\): \\(A \\cup B = \\{(F0),(F1),(F2),(F3),(F4),(F5),(F6),(G3),(G4) \\}\\) Temos então que esta união consiste de 9, de modo que: \\[P(A \\cup B) = \\frac{9}{12} = 0.75\\] Veja que \\(P(A \\cup B)\\) pode ser obtido por: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) 27.1.4 Representação de eventos: diagrama de árvore Considere agora o experimento “virar duas estruturas e verificar o tipo de estrutura”. O espaço amostral deste experimento consiste de 4 observações: \\(\\Omega = \\{(galho, galho), (galho, folha), (folha, galho), (folha, folha)\\}\\) E pode ser representado visualmente por um diagrama de árvore. REFAZER DIAGRAMA DE ÁRVORE. Função ‘bayes_probability_tree()’ desatualizada O diagrama de árvore é útil quando desejamos representar os resultados possíveis de um experimento que consiste de múltiplas etapas consecutivas, possivelmente composto por mais de uma caminho possível. Utilizaremos novamente este tipo de diagrama para falarmos em probabilidade condicional (Capítulo 28) e do Teorema de Bayes (Capítulo 29). "],["probcondind.html", "Capítulo 28 Probabilidade condicional e independência 28.1 Eventos independentes 28.2 Eventos independentes vs mutuamente exclusivos", " Capítulo 28 Probabilidade condicional e independência Voltemos ao experimento “virar uma estrutura e contar o número de itens” com o espaço amostral: \\(\\Omega = \\{(F0), (F1), (F2), (F3), (F4), (F5), (F6), (G0), (G1), (G2), (G3), (G4)\\}\\) Os eventos \\(A\\) “virar uma folha”: \\(A = \\{(F0), (F1), (F2), (F3), (F4), (F5), (F6)\\}\\) E \\(B\\) “encontar mais de 3 itens”: \\(B = \\{(F3), (F4), (F5), (F6), (G3), (G4)\\}\\) Representados no diagrama de Venn abaixo. Nos capítulos anteriores obtivemos as probabilidades \\(P(A)\\), \\(P(B)\\), \\(P(A \\cup B)\\) e \\(P(A \\cap B)\\). Digamos agora que, ao virar uma estrutura, sabemos que a estrutura era uma folha. A pergunta é: Qual a probabilidade de que tenham sido obtidos mais de 3 itens? Ao informarmos que a estrutura era uma folha, sabemos que nem todos os eventos de \\(\\Omega\\) podem ter ocorrido. Neste exemplo, somente as 7 observações do evento e \\(A\\) consistem de uma folha: Destas, apenas 4 possuem mais de 3 itens, de modo a resposta à pergunta seria \\(\\frac{4}{7}\\). Este resultado é conhecido como probabilidade condicional, denotada pelo símbolo (\\(|\\)). Neste exemplo específico estamos perguntando: Dado que \\(A\\) OCORREU, qual a probabilidade de que \\(B\\) tenha ocorrido? Simbolicamente, esta questão é escrita como \\(P(B|A)\\). \\[P(B|A) = \\frac{4}{7} = 0.57\\] Esta probabilidade condicional foi calculada pelo número de observações favoráveis à intersecção de \\(A\\) e \\(B\\) (\\(\\#A \\cap B\\)) relativa ao número de observações do evento \\(A\\) (\\(\\#A\\)). Isto significa ao sabermos parte dos resultados, o espaço amostral inicial inicial foi reduzido, neste caso, ao espaço coincidente com \\(A\\). Portanto, temos que: \\[P(B|A) = \\frac{\\#A \\cap B}{\\#A}\\] Se dividirmos ambos o numerador e o denominador da expressão acima pelo tamanho do espaço amostral (\\(\\#\\Omega\\)) teremos: \\[P(B|A) = \\frac{\\frac{\\#A \\cap B}{\\#\\Omega}}{\\frac{\\#A}{\\#\\Omega}}\\] Como \\(\\frac{\\#A \\cap B}{\\#\\Omega} = P(A \\cap B)\\) e \\(\\frac{\\#A}{\\#\\Omega} = P(A)\\), uma expressão simples para a probabilidade condicional será: \\[P(B|A) = \\frac{P(A \\cap B)}{P(A)}\\] Esta expressão nos dá também uma forma de calcularmos a probabilidade da intersecção de \\(A\\) com \\(B\\). No capítulo 27 esta probabilidade foi obtida contando o número de eventos na intersecção dividido pelo tamanho do espaço amostral. No entanto, podemos calculá-la diretamente por: \\[P(B \\cap A) = P(A) \\times P(B|A)\\] Obs: faça os cálculos para \\(P(A \\cap B)\\) utilizando a expressão acima e veja se coincide com o obtido no capítulo 27. Uma vez que probabilidade condicional se refere ao cálculo de probabilidades em eventos complexos sequenciais, podemos fazer a mesma representação de um experimento complexo utilizando um diagrama de árvore onde estão indicadas as probabilidades de ocorrência de cada evento, bem como as probabilidades condicionais. Neste esquema, podemos ler as probabilidades de obtenção de cada resultado da seguinte forma. A primeira etapa do experimento, pode resultar na ocorrencia do evento \\(A\\) com probabilidade \\(P(A)\\), ou de seu complemento, com probabilidade \\(P(\\overline{A})\\). Na segunda etapa, os resultados podem ser dar da seguinte forma: Dado que \\(A\\) ocorreu: \\(B\\) pode ocorrer com probabilidade \\(P(B|A)\\). Neste caso, a ocorrência de \\(A\\) e \\(B\\) será dada por \\(P(A \\cap B) = P(A) \\times P(B|A)\\); o complemento de \\(B\\) pode ocorrer com probabilidade \\(P(\\overline{B}|A)\\). Neste caso, a ocorrência de \\(A\\) e \\(\\overline{B}\\) será dada por \\(P(A \\cap \\overline{B}) = P(A) \\times P(\\overline{B}|A)\\); Dado que \\(\\overline{A}\\) ocorreu: \\(B\\) pode ocorrer com probabilidade \\(P(B|\\overline{A})\\). Neste caso, a ocorrência de \\(\\overline{A}\\) e \\(B\\) será dada por \\(P(\\overline{A} \\cap B) = P(\\overline{A}) \\times P(B|\\overline{A})\\); o complemento de \\(B\\) pode ocorrer com probabilidade \\(P(\\overline{B}|\\overline{A})\\). Neste caso, a ocorrência de \\(\\overline{A}\\) e \\(\\overline{B}\\) será dada por \\(P(\\overline{A} \\cap \\overline{B}) = P(\\overline{A}) \\times P(\\overline{B}|\\overline{A})\\); Estas ocorrências denotam as probabilidades de todos os eventos possíveis para este experimento. Vamos expressar numericamente todas as probabilidades representadas no diagrama de árvore acima: REFAZER DIAGRAMA DE ÁRVORE. Função ‘bayes_probability_tree()’ desatualizada Obs.: Refaça os cálculos e confira os resultados 28.1 Eventos independentes Vamos retomar todas as probabilidades do experimento deste capítulo: “virar uma estrutura e contar o número de itens”. As probabilidades de cada evento \\(A\\), \\(B\\) e de seus complementos são: \\(P(A) = 0.58\\) \\(P(\\overline{A}) = 0.42\\) \\(P(B) = 0.5\\) \\(P(\\overline{B}) = 0.5\\) Embora a probabilidade de ocorrencia de \\(B\\) seja 0.5, a discussão sobre probabilidade condicional nos informa que ao sabermos que \\(A\\) ocorreu, o conhecimento sobre \\(B\\) deve ser atualizado para \\(P(B|A)\\), que em nosso exemplo será 0.57. Note que a nova estimativa da probabilidade de \\(B\\) está condicionada ao conhecimento prévio sobre a ocorrência de \\(A\\). Portanto dizemos que \\(A\\) e \\(B\\) são eventos dependentes, de modo que \\(P(B) \\neq P(B|A)\\). 28.1.1 Um exemplo de eventos independentes Dois eventos são ditos independentes se a informação sobre a ocorrência de um não altera a probabilidade condicional da ocorrência do outro, de modo que \\(P(B) = P(B|A)\\). Suponha em um estudo sobre o perfil de visitação em uma área de preservação ambiental, tenham sido avaliados a idade (até 20 ou acima de 20 anos) e a região de origem do visitante (da própria cidade ou de outra cidade). Foram investigadas ao todo 600 pessoas com os seguintes perfis: Da cidade De fora da cidade Até 20 30 170 Mais de 20 60 340 Vamos denominar de: \\(A\\): ter até 20 anos e \\(\\overline{A}\\): ter mais de 20 anos. \\(B\\): ser da cidade e \\(\\overline{B}\\): ser de fora da cidade. Neste caso a tabela fica: \\(P(A) = \\frac{30 + 170}{600} = \\frac{200}{600} \\approx 0.33\\) \\(P(\\overline{A}) = \\frac{60 + 340}{600} = \\frac{400}{600} \\approx 0.67\\) \\(P(B) = \\frac{30 + 60}{600} = \\frac{90}{600} = 0.15\\) \\(P(\\overline{B}) = \\frac{30 + 340}{600} = \\frac{510}{600} = 0.85\\) Se soubermos por exemplo que a pessoa tem mais de 20 anos, a probabilidade condicional de ser da cidade é de: \\(P(B|A) = \\frac{60}{60 + 340} = \\frac{60}{400} = 0.15\\) Vemos que \\(P(B) = P(B|A) = 0.85\\) de modo que informar se uma pessoa tem ou não mais de 20 anos não nos diz nada sobre se a pessoa é da cidade ou não. Portanto, a classe de idade neste caso é independente da origem do visitante. 28.2 Eventos independentes vs mutuamente exclusivos Uma questão comum em tópicos de probabilidade é a confusão entre os conceitos de eventos mutuamente exclusivos e de eventos independentes. Inicialmente vamos às definições: A união de eventos é dada por: \\(P(A \\cup B) = P(A) \\times P(B) \\times P(A \\cap B)\\) Quando dois eventos são mutuamente exclusivos não há intersecção e consequentemente: \\(P(A \\cap B) = 0\\) de modo que, \\(P(A \\cup B) = P(A) \\times P(B)\\) Eventos mutuamente exclusivos são representados no diagrama de Venn abaixo. Quando dois eventos são independentes: \\(P(A \\cap B) = P(A) \\times P(B)\\) Se ambos \\(P(A)\\) e \\(P(B)\\) são diferentes de zero, esta definição não permite que dois eventos sejam simultaneamente independentes E mutuamente exclusivos, pois para eventos independentes \\(P(A \\cap B)\\) será zero somente se \\(P(A)\\) ou \\(P(B)\\) forem zero. Vamos agora à ideia da probabilidade condicional quando dos eventos são dependentes. Neste caso temos que: \\(P(A \\cap B) = P(A) \\times P(B|A)\\) Esta relação expressa a ideia que ao informar sobre a ocorrência de \\(A\\), a probabilidade sobre a ocorrência de \\(B\\) deve ser atualizada de \\(P(B)\\) para \\(P(B|A)\\). Deste modo, para dois eventos dependentes, \\(P(B) \\ne P(B|A)\\). Finalmente, vamos associar a ideia de dependência com a ideia de eventos mutuamente exclusivos. Se sabemos que dois eventos são mutuamente exclusivos, então sabemos que ao ocorrer um deles, o outro não poderá ocorrer, ou seja, dado que \\(A\\) ocorreu temos certeza de que \\(B\\) não poderá ocorrer, de modo que \\(P(B|A) = 0\\). Representando eventos mutuamente exclusivos em um diagrama de árvore teremos: Ao saber que \\(A\\) ocorreu, veja que \\(P(B|A)\\) tem probabilidade igual a zero, de modo que eventos mutuamente exclusivos são necessariamente dependentes. O que ocorre se os eventos não são mutuamente exclusivos, ou seja, se \\(P(A \\cap B) \\ne 0\\)? Neste caso \\(A\\) e \\(B\\) podem ou não ser independentes e a resposta dependerá se: \\(P(B) = P(B|A)\\) (eventos independentes) ou; \\(P(B) \\ne P(B|A)\\) (eventos dependentes). "],["tbayes.html", "Capítulo 29 Teorema de Bayes: atualizando o conhecimento 29.1 Teorema de Bayes 29.2 Teorema da probabilidade total 29.3 O problema da detecção de espécies", " Capítulo 29 Teorema de Bayes: atualizando o conhecimento 29.1 Teorema de Bayes O teorema de Bayes decorre de nossa definição de probabilidade condicional em que: \\[P(B|A) = \\frac{P(A \\cap B)}{P(A)}\\] o que implica em: \\[P(A \\cap B) = P(A) \\times P(B|A)\\] Das expressões acima, poderíamos escrever também que: \\[P(A|B) = \\frac{P(B \\cap A)}{P(B)}\\] e \\[P(B \\cap A) = P(B) \\times P(A|B)\\] Uma que \\(P(A \\cap B) = P(B \\cap A)\\), temos que: \\[P(A) \\times P(B|A) = P(B) \\times P(A|B)\\] o que nos leva à expressão que define o Teorema de Bayes e nos fornece a probabilidade condicional de \\(P(B|A)\\) uma vez que conhecemos \\(P(A|B)\\). \\[P(B|A) = \\frac{P(B) \\times P(A|B)}{P(A)}\\] 29.2 Teorema da probabilidade total Veja novamente a esquema concentual de um diagrama de árvore: Podemos perceber que dois caminhos nos levam à ocorrência de \\(A\\). Um caminho em que \\(B\\) também ocorre - com probabilidade \\(P(A \\cap B)\\) - ou um caminho em que \\(B\\) não ocorre - com probabilidade \\(P(A \\cap \\overline{B})\\). Uma vez que estas rotas são mutuamente exclusivas, \\(P(A)\\) pode ser calculado por: \\[P(A) = P(A \\cap B) + P(A \\cap \\overline{B})\\] que podemos re-escrever como: \\[P(A) = P(B) \\times P(A|B) + P(\\overline{B}) \\times P(A|\\overline{B})\\] Esta última expressão é conhecida como Teorema da probabilidade total. Ao utilizá-la, o Teorema de Bayes pode ser re-escrito como: \\[P(B|A) = \\frac{P(B) \\times P(A|B)}{P(B) \\times P(A|B) + P(\\overline{B}) \\times P(A|\\overline{B})}\\] 29.3 O problema da detecção de espécies Suponha um estudo sobre a distribuição de uma espécie de peixe em uma área de restinga da Mata Atlântica. A espécie ocorre em poças formadas pela água da chuva e está presente em 5% das poças na região. Isto denota uma espécie rara, pois ao investigarmos uma poça em particular, sua probabilidade de ocorrência será de \\(P(O) = 0,05\\). A detecção desta espécie é feita pela captura e identificação taxonômica. Dado que esteja presente em uma poça, sua probabilidade de ser capturada é de \\(0,99\\). Consequentemente, existe uma probabilidade de \\(0,01\\) de que, mesmo estando presente, a espécie não seja capturada, o que é conhecido como falso negativo. Um falso negativo pode acontecer por exemplo se os indivíduos permanecem escondidos sob o substrato e não são capturados. Sabe-se também que existe na região uma espécie muito similar e, por vezes, ocorre um erro de identificação. Desta forma, mesmo nas poças em que a espécie não está presente, existe uma possibilidade de que seja registrada erroneamente, o que pode ocorrer com probabilidade de \\(0,10\\). Chamamos esta possibilidade de falso positivo. O problema da não-detecção ou da falsa-detecção, longe de ser somente um exemplo, é comum em levantamentos de fauna e flora ainda que raramente seja considerado. As probabilidades descritas neste problema podem ser organizadas em um diagrama de árvore. Figure 29.1: Diagrama de árvore representando as probabilidades de ocorrência P(O) e detecção P(D) de uma espécie. No diagrama, cada bifurcação representa as possíveis saídas de um estágio do experimento. Por exemplo, em uma determinada poça a espécie está ou não está presente, com probabilidades \\(0,05\\) e \\(0,95\\) respectivamente. O segundo estágio do experimento consiste na tentativa de detecção da espécie. Neste caso, as possibilidades são as seguintes: Dado que a espécie esteja presente, os resultados possíveis são: i) ocorre a detecção com probabilidade \\(0,99\\) ou ii) não ocorre a detecção com probabilidade \\(0,01\\). Dado que a espécie não esteja presente, os resultados possíveis são: i) a ocorrência é registrada erroneamente com probabilidade \\(0,10\\) ou ii) a ocorrência não é registrada com probabilidade \\(0,90\\). Diante deste cenário, temos como possíveis resultados: A espécie Ocorre e é Detectada com probabilidade \\(P(O \\cap D) = 0,0495\\) A espécie Ocorre porém permanece Não-Detectada com probabilidade \\(P(O \\cap \\bar{D}) = 0,0005\\) A espécie Não Ocorre e é Detectada erroneamente com probabilidade \\(P(\\bar{O} \\cap D) = 0,095\\) A espécie Não Ocorre e permanece Não-Detectada com probabilidade \\(P(\\bar{O} \\cap\\overline{D}) = 0,855\\) De acordo com o Teorema da probabilidade total, \\(P(D)\\) pode ser calculada: \\[P(D) = P(O \\cap D) + P(\\bar{O} \\cap D) = 0,0495 + 0,095 = 0,1445\\] pois o registro de detecção pode ocorrer quando a espécie está ou não está presente na poça. 29.3.1 Razão de verossimilhança, inferência bayesiana e teste de hipóteses Um questão relevante neste problema é: Ao recebermos a notícia de uma possível detecção, deveríamos ficar completamente convencidos sobre a ocorrência desta espécie em uma determinada poça? Esta questão é importante pois nos permite tomar uma decisão sobre um fenômeno que não conhemos (a ocorrência da espécie na poça), com base na informação sobre uma possível detecção. A tomar esta decisão, entramos no campo da inferência estatística. Nos capítulos 16 e 17 discutimos a ideia da inferência estatística clássica, onde introduzomos conceitos como Erro do Tipo I, do Tipo II e poder do teste. Ńesta seção iremos introduzir outras duas abordagens, a inferência por verossimilhança e a inferência bayesiana. Ambas serão tratadas mais detalhadamente nos capítulos 34 a 36. Verossimilhança: uma medida indireta para \\(P(O|D)\\) Veja que, ao existir a possibilidade de um falso negativo, a ocorrência da espécie deve ser tomada como uma hipótese a ser confirmada. Caso \\(P(D|O)\\) seja elevado, deveríamos estar bastante confiantes sobre a ocorrência da espécie, pois a probabilidade de detectá-la seria alta quando ela, de fato, ocorre. Por outro lado, caso \\(P(D|\\overline{O})\\) seja elevado, deveríamos estar mais confiantes da não-ocorrência da espécie, pois sua probabilidade de detecção seria alta mesmo nas situações em que não esteja presente (falso positivo). Em inferência estatística, \\(P(D|O)\\) pode ser tomada como uma medida de verossimilhança para a hipótese de ocorrência da espécie condicional ao evento de detecção. Como discutimos ao longo deste capítulo, \\(P(D|O)\\) não é igual a \\(P(O|D)\\), porém as duas medidas são positivamente relacionadas, uma vez que \\(P(O|D)\\) é uma função positiva crescente de \\(P(D|O)\\). Estritamente falando, a probabilidade condicional \\(P(D|O)\\) não é realmente sinônimo de verossimilhança. A função de verossimilhança seria expressa por \\(\\mathcal{L}(O|D)\\) (note a inversão nas posições \\(O\\) e \\(D\\)). Dizemos portanto que \\(P(D|O) \\propto \\mathcal{L}(O|D)\\) (é proporcional à) e isto ficará mais claro nos capítulos 30 a 36. Por hora, vamos tratar os dois conceitos como análogos. Assim como \\(P(D|O)\\) pode ser utilizada para medir o grau de confiabilidade na ocorrência da espécie, \\(P(D|\\overline{O})\\) pode ser utilizada para medir o grau de confiabilidade da não-ocorrência. Têm-se então duas hipóteses a serem contrastadas: Hipótese 1: a espécie ocorre e foi detectada; Hipótese 2: a espécie não ocorre porém foi detectada. As medidas que nos permitem contrastar as hipóteses 1 e 2 são \\(P(D|O)\\) e \\(P(D|\\overline{O})\\). Utilizando-as, podemos calcular o que denominamos de razão de verossimilhanças (\\(RV\\)), tomando a razão entre a maior e a menor medida, de modo que: \\[RV = \\frac{P(D|O)}{P(D|\\overline{O})} = \\frac{0,99}{0,10} = 9,9\\] Interpretamos o resultado acima dizendo que a hipótese da ocorrência (hipótese 1) é cerca de 10 vezes mais verossímil que a hipótese da não-ocorrência. \\(RV\\) nos fornece portanto, uma medida indireta sobre a expectativa de ocorrência da espécie, uma vez registrada sua detecção. Neste caso, o resultado foi favorável à Hipótese 1. Inferência bayesiana: o conhecimento a priori é importante? A ideia da verossimilhança parte unicamente do resultado obtido no experimento e de nosso pressoposto sobre a probabilidade do evento \\(D\\) condicional aos eventos \\(O\\) e \\(\\overline{O}\\). Não utilizamos no entanto, a informação sobre \\(P(O)\\) que nos é fornecida no início do problema. Ao utilizarmos \\(P(O)\\), entramos no campo da inferência bayesiana. Note que \\(P(O)\\) foi uma informação fornecida no início do experimento, antes da informação sobre uma possível detecção. Em inferência bayesiana, esta medida é conhecida como a probabilidade a priori para a ocorrência da espécie. Esta probabilidade foi obtida antes do evento de detecção, e geralmente parte de um conhecimento prévio que pode ser obtido a partir da literatura, de experimentos piloto ou do conhecimento empírico de especialistas. Ao obtermos uma informação sobre uma possível detecção, devemos atualizar nossa hipótese sobre a ocorrência, que agora será condicional à observação do evento \\(D\\). Deste modo, em inferência bayesiana temos como informações de entrada: As probabilidades a priori \\(P(O)\\) e \\(P(\\overline{O})\\); e, A informação sobre \\(P(D|O)\\) e \\(P(D|\\overline{O})\\) que utilizamos diretamente para calcular \\(P(O|D)\\) e \\(P(\\overline{O}|D)\\). Isto é feito utilizando o Teorema de Bayes que, neste exemplo, pode ser re-escrito como: \\[P(O|D) = \\frac{P(O) \\times P(D|O)}{P(O) \\times P(D|O) + P(\\overline{O}) \\times P(D|\\overline{O})}\\] Fazendo cálculos com as informações fornecidas no início do problema, a probabilidade de ocorrência dado o evento de detecção será: \\[P(O|D) = \\frac{0,02 \\times 0,99}{0,02 \\times 0,99 + 0,98 \\times 0,10} \\approx 0.17\\] Enquanto a probabilidade de não-ocorrência será de: \\[P(\\bar{O}|D) = \\frac{0,98 \\times 0,10}{0,02 \\times 0,99 + 0,98 \\times 0,10} \\approx 0.83\\] Estes resultados nos dizem que, mesmo ao ser informados sobre uma possível detecção, a chance de não-ocorrência da espécie em uma determinada poça ainda é cerca de 5 vezes maior que a chance de ocorrência, o que é favorável à Hipótese 2. Entendendo as diferênças entre as duas abordagens As conclusões das duas abordagens foram distintas. Desta forma, é necessário entendermos melhor em quais causas destas diferenças, ou seja, em quais informações cada abordagem está baseada. Enquanto a abordagem por verossimilhança utiliza \\(P(D|O)\\) para nos auxiliar indiretamente no julgamento das hipóteses 1 e 2, a inferência bayesiana utiliza o Teorema de Bayes para calcular diretamente \\(P(O|D)\\). Nesta última, \\(P(O|D)\\) pode ser entendida como uma média ponderada de \\(P(D|O)\\), em que os fatores de ponderação são as probabilidades a priori \\(P(O)\\) e \\(P(\\overline{O})\\). Temos portanto, que a ideia da probabilidade a priori é central e inferência bayesiana, porém irrelevante na inferência por verossimilhança. Vejam por exemplo o que ocorreria se as probabilidades a priori fossem \\(P(O) = P(\\overline{O}) = 0,5\\). Neste caso: \\[P(O|D) = \\frac{0,5 \\times 0,99}{0,5 \\times 0,99 + 0,5 \\times 0,10} = \\frac{0,5}{0,5} \\times \\frac{0,99}{0,99 + 0,10} = \\frac{0,99}{0,99 + 0,10} \\approx 0,91 \\] e \\[P(\\overline{O}|D) = \\frac{0,5 \\times 0,10}{0,5 \\times 0,99 + 0,5 \\times 0,10} = \\frac{0,5}{0,5} \\times \\frac{0,10}{0,99 + 0,10} = \\frac{0,10}{0,99 + 0,10} \\approx 0,09 \\] Ao dividirmos \\(P(O|D)\\) por \\(P(\\overline{O}|D)\\) terímos: \\[\\frac{P(O|D)}{P(\\overline{O}|D)} = \\frac{0,91}{0,09} = 9,9\\] O que é exatamente igual à razão de verossimilhança \\(RV\\) obtida anteriormente. Ao dizer que \\(P(O) = P(\\overline{O}) = 0,5\\), estamos dizendo que, a priori do experimento, sabemos unicamente que um espécie tem changes iguais de estar ou não estar presente. Em inferência bayesiana esta informação é conhecida como priori não-informativa, pois dá pesos iguais a todas as possibilidades. Por outro lado, quando informamos em nosso exemplo que a espécie é muito rara, os fatores de ponderação (\\(P(\\overline{O}) = 0,95\\) versus \\(P(O) = 0,05\\)) tornaram-se muito influentes no cálculo de \\(P(O|D)\\), independente do resultado de uma possível detecção da espécie. Portanto, as duas abordagens serão equivalentes somente quando utilizamos uma priori não informativa. Isto equivale a dizer que, por não termos informações melhores, estamos dando chances iguais para os eventos \\(O\\) e \\(\\overline{O}\\). "],["va.html", "Capítulo 30 As variáveis são aleatórias, não imprevisíveis! - Modelos discretos 30.1 Experimento 1: Sucessos e fracassos 30.2 Experimento 2: O custo de 1 sucesso 30.3 Experimento 3: Uma sequência de sucessos 30.4 Experimento 4: Quantas marcas na amostra! 30.5 Experimento 5: Contagem por unidade de área, tempo, …..", " Capítulo 30 As variáveis são aleatórias, não imprevisíveis! - Modelos discretos Nos capítulos 26 a 29 apresentamos conceitos básicos que regem as leis de probabilidade aplicadas à ocorrência de eventos simples e complexos. No entanto, ao conduzir um experimento, muitas vezes não estamos interessados nos eventos em si, mas em quantias numéricas que expressam a ocorrência destes eventos. Definiremos estas quantias aqui como variáveis aleatórias. Variáveis aleatórias são funções matemáticas que associam cada evento do espaço amostral a uma quantia numérica. Embora expressem resultados probabilísticos, as variáveis aleatórias como funções matemáticas, têm estruturas previssíveis como média, variância, grau de assimetria, etc. Estas estruturas são as utilizadas para fazermos predições sobre eventos futuros. Neste sentido, as variáveis aleatórias compõem os modelos probabilísticos sobre os quais nos baseamos para fazer predições sobre fenômenos naturais. Estas variáveis são subdivididas em variáveis aleatórias discretas e contínuas, a depender da natureza da informação que está sendo mensurada. As diferências entre dados discretos e contínuos é discutida no capítulo ??. A seguir serão exemplificados diferentes modelos de variáveis aleatórias associados a diferentes tipos de experimentos aleatórios. 30.1 Experimento 1: Sucessos e fracassos Rhamdioglanis transfasciatus é um peixe endêmico de riachos costeiros de Mata Atlântica do sudeste do Brasil, particularmente abundante em pequenos riachos, rasos, com boa cobertura vegetal e de fundo predregoso. A captura é feita buscando os indivíduos ocultos sob rochas e troncos, que são o micro-habitats que a espécie ocupa. Geralmente esta busca é feita percorrendo um trecho de algumas dezenas de metros de extensão em um dado riacho. Suponha que a probabilidade de ocorrência da espécie em riachos da região seja de \\(P(O) = 0.4\\). Suponha também que não existam falsos positivos ou falsos negativos. Isto é, se a espécie estiver presente no trecho ela certamente será capturada e, se não estiver, não há a possibilidade de falso registro. Neste caso, se fizermos uma busca, é esperado que a captura seja feita em 40% dos riachos selecionados. Ao selecionar um dado riacho, os resultados possíveis são: \\[\\Omega = \\{(ocorre), (nao-ocorre)\\}\\] Em que \\(P(ocorre) = 0,4\\) e \\(P(não-ocorre) = 1-0,4 = 0,6\\) Vamos denominar de sucesso a escolha de um riacho em a espécie esteja presente e de fracasso a escolha de um riacho que em esteja ausente. Se selecionarmos um único riacho, a probabilidade de captura da espécie poderia ser modelada por um variável aleatória denominada de Modelo de Bernoulli. Aqui, vamos generalizar este modelo e supor que estejamos interessados em selecionar aleatoriamente \\(n = 4\\) riachos. Este experimento consiste portanto de 4 etapas, isto é, a amostragem do \\(1^o\\), \\(2^o\\), \\(3^o\\) e \\(4^o\\) riacho, verificando em cada um a ocorrẽncia da espécie. Se representarmos por \\(O\\) o evento de ocorrência e por \\(\\overline{O}\\) o evento não-ocorrência, os possíveis resultados do experimento seriam: A tabela acima apresenta os 16 eventos possíveis deste experimento, em que cada riacho pode ou não conter a espécie. A pirmeira linha por exemplo, descreve a situação em que a espécie não-ocorre em nenhum riacho, enquanto a última descreve a situação em que a espécie ocorre em todos os riachos. Estamos interessados aqui em uma representação numérica destes eventos (nossa variável aleatória \\(Y\\)) que descreve no número de ocorrências após a seleção de \\(4\\) riachos. A última coluna da tabela nos mostra todos os valores possíveis para \\(Y\\). Veja que, não encontrar a espécie em nenhum riacho equivale a dizer que \\(Y = 0\\), enquanto encontrá-la em todos os riachos resulta em \\(Y = 0\\). Para outros resultados temos mais de uma possibilidade. Por exemplo, \\(Y = 2\\) ocorre em 6 situações diferentes, dependendo das combinações entre ocorrência e não-ocorrência para cada riacho. Assim, ao utilizar a variável aleatória \\(Y\\) nosso novo espaço amostral consiste de 5 possibilidades numéricas distintas e mutuamente exclusivas, em que \\(Y\\) pode ser \\(0\\), \\(1\\), \\(2\\), \\(3\\), ou \\(4\\). Escrevemos que nosso espaço amostral é: \\[\\Omega = \\left\\{ y \\in \\mathbb{N}: 0 \\le y \\le 4 \\right\\}\\] A distribuição de probabilidade de \\(Y\\) Qual a probabilidade de ocorrência de cada um dos 16 eventos possíveis? Se assumirmos que os resultados obtidos em cada riacho são independentes, a probabilidade do evento descrito na primeira linha por exemplo seria: \\(P(1^a linha) = P(Y = 0) = P(\\overline{O}) \\times P(\\overline{O}) \\times P(\\overline{O}) \\times P(\\overline{O})\\) \\(P(Y = 0) = 0.6 \\times 0.6 \\times 0.6 \\times 0.6 = 0.1296\\) No caso da segunda linha temos: \\(P(2^a linha) = P(\\overline{O}) \\times P(\\overline{O}) \\times P(\\overline{O}) \\times P(O)\\) \\(P(2^a linha) = 0.6 \\times 0.6 \\times 0.6 \\times 0.4 = 0.0864\\) Esta linha equivale a \\(Y = 1\\). No entanto, veja que existem outras 3 linhas em que \\(Y = 1\\). Deste modo: \\(P(Y = 1) = 4 \\times (0.6 \\times 0.6 \\times 0.6 \\times 0.4) = 0.3456\\) Seguindo o mesmo raciocínio para os demais valores de \\(Y\\) temos: \\(P(Y = 2) = 6 \\times (0.6 \\times 0.6 \\times 0.4 \\times 0.4) = 0.3456\\) \\(P(Y = 3) = 4 \\times (0.6 \\times 0.4 \\times 0.4 \\times 0.4) = 0.1536\\) \\(P(Y = 4) = 1 \\times (0.4 \\times 0.4 \\times 0.4 \\times 0.4) = 0.0256\\) A soma de \\(P(Y = 0) + \\cdots + P(Y = 4) = 1\\), de modo que temos \\(0 \\le Y \\le 4\\) como todos os valores possíveis desta variável aleatória. Uma expressão geral para \\(P(Y = y)\\) Vamos definir \\(P(Y = y)\\) como a probabilidade da variável aleatória \\(Y\\) (MAIÚSCULO) assumir um valor particular \\(y\\) (minúsculo). Mais a frente, para simplificar as notações, iremos utilizar simplesmente \\(p_{Y}(y)\\) ou \\(p(y)\\) no lugar de \\(P(Y = y)\\). Vamos definir também \\(P(O) = p\\), \\(P(\\overline{O}) = 1 - p\\) e \\(n\\) como o número de tentativas do experimentos (= número de riachos selecionados). Utilizando as regras de probabilidade para eventos independentes e regras de contagem, podemos encontrar a expressão que generaliza o cálculo de cada uma das probabilidades acima por: \\[P(Y = y|n,p) = \\left (\\begin{array}{c} n \\\\ y \\end{array}\\right) \\times p^y \\times (1-p)^{(n-y)}\\] Este modelo para é conhecido como Modelo Binomial, em que \\(n\\) e \\(p\\) são denominados de parâmetros. A expressão é conhecida como a função de massa de probabilidade de \\(Y\\) ou simplesmente como distribuição de probabilidade de \\(Y\\) para o modelo binomial. Entendendo a expressão do modelo binomial A porção \\(p^y \\times (1-p)^{(n-y)}\\) indica que o sucesso ocorreu \\(y\\), cada uma com probabilidade \\(p\\), enquanto o fracasso ocorreu \\((n - y)\\) vezes, cada uma com probabilidade \\((1-p)\\). A expressão \\(\\left (\\begin{array}{c} n \\\\ y \\end{array}\\right)\\) vem da teoria combinatória e nos diz quantas vezes podemos obter \\(y\\) ocorrências dentro de \\(n\\) possibilidades. Lembre-se que: \\[\\left (\\begin{array}{c} n \\\\ y \\end{array}\\right) = \\frac{n!}{(n-y)! \\times y!}\\] \\(P(Y = y|n,p)\\) é lido como a probabilidade de \\(Y\\) assumir o valor \\(y\\) dados os valores dos parâmetros \\(n\\) e \\(p\\), ou simplesmente, a probabilidade de obtermos \\(y\\) sucessos em \\(n\\) tentativas. 30.1.0.1 Exemplo de cálculo Vamos exemplificar o cálculo para \\(Y = 3\\), lembrando que em nosso exemplo \\(n = 4\\) e \\(p = 0,4\\). \\(P(Y = 3|4,0.4) = \\left (\\begin{array}{c} 4 \\\\ 3 \\end{array}\\right) \\times 0.4^3 \\times (1-0.4)^{(4-3)}\\) \\(P(Y = 3|4,0.4) = \\frac{4!}{(4-3)!3!} \\times 0.4^3 \\times (0.6)^{1}\\) \\(P(Y = 3|4,0.4) = \\frac{24}{6} \\times 0.064 \\times (0.6) = 0.1536\\) o que é condizente com o resultado obtido anteriormente. Se fizermos os cálculos para todos os valores de \\(Y\\) e os expressarmos em um gráfico de barras (Capítulo ??) veremos a figura abaixo: Vemos que as probabilidades de selecionarmos 1 ou 2 riachos contendo a espécie são maiores que as probabilidades de que nenhum ou de que todos os riachos contenham a espécie. Função de distribuição acumulada Obtida \\(p_Y\\) para cada valor particular, podemos obter as probabilidades acumuladas para \\(Y\\). Vamos denotar esta função de \\(F(y)\\), que nos fornece a probabilidade de obtermos valores de \\(Y\\) menores ou iguais a um limite particular \\(y\\). Deste modo temos que: \\[F(y) = P(Y \\le y)\\] Se comparamos \\(F(y)\\) e \\(p(y)\\) graficamente teremos: A barra laranja em 1 por exemplo, é o somatório das duas primeiras barras em azul da distribuição de \\(p(y)\\). Veja que \\(F(Y)\\) necessariamente termina em \\(1\\) expressando a soma de todos os valores possíveis de \\(p(y)\\). Estruturas previsíveis para um experimento aleatório Ainda que não saibamos o resultado final de um experimento em particular, o modelo binomial que acabamos de construir nos diz que, ao repetí-lo um grande número de vezes, é mais provável encontrarmos a espécie em 1 ou 2 riachos que em nenhum ou nos 4 riachos. Consequentemente, o modelo nos permite fazer predições sobre o comportamento futuro de um experimento deste tipo. Assim, mesmo antes da execução do experimento, sabemos que será o comportamento predito no que diz respeito aos seus possíveis resultados. Esta capacidade de predição é fundamental se queremos comparar os resultados de experimentos com diferentes modelos teóricos, como veremos no capítulos 34 e 35. Ao definirmos um modelo para nossa variável aleatória, uma forma simples de descrevê-la são por suas medidas de tendência central e de variabilidade. O valor esperado (ou esperança) de uma variável aleatória é dado por: \\[E(Y) = \\mu = \\sum_i^n y_i \\times p(y_i)\\] O valor esperado de \\(Y\\) pode ser interpretado como o centro de massa da variável aleatória, ou simplesmente como a média de \\(Y\\). Note que utilizamos o símbolo \\(\\mu\\) e não \\(\\overline{Y}\\), pois estamos nos referindo ao comportamento da média da variável aleatória na população estatística de experimentos deste tipo, e não à média de um conjunto de experimentos particulares (Capítulo ??). O valor esperado nos dá uma medida de centralidade para a distribuição de \\(Y\\). Uma outra medida importante é a variância de \\(Y\\), que nos informa sobre o grau de disperção de \\(Y\\) ao redor da média: \\[VAR(Y) = \\sigma^2 = E(Y^2) - (E(Y))^2\\] Poderíamos utilizar as expressões acima para encontrar o valor esperado e a variância de \\(Y\\) no modelo binomial. No entanto, para este modelo bem como para outros que veremos a seguir, existem expressões particulares em que: \\[E(Y) = n \\times p\\] e \\[VAR(Y) = n \\times p \\times (1-p)\\] Em nosso exemplo, temos \\(E(Y) = 4 \\times 0.4 = 0.16\\) e \\(VAR(Y) = 4 \\times 0,4 \\times 0.6 = 0.96\\). Alterando os parâmetros do modelo Em nosso exemplo, esperamos encontrar a espécie na maior parte das vezes em 1 ou 2. De fato, a esperança de \\(Y\\) será \\(E(Y) = 1.6\\). O que aconteceria para outros valores de \\(n\\) ou de \\(p\\). Veja as figuras abaixo: Na medida em que \\(p\\) é baixo, a distribuição de probabilidade é assimétrica para a esquerda, denotando que é mais provável a ocorrência de um número baixo de sucessos. Para \\(p = 0.5\\) a distribuição de probabilidade é simétrica ao redor do valor esperado, e as proporções de sucessos e fracassos são idênticas. Finalmente, para valores elevados de \\(p\\) a distribuição é assimétrica para a direita e a probabilidade de obtermos um número elevado de sucessos aumenta. 30.2 Experimento 2: O custo de 1 sucesso Suponha agora que ao chegar em uma determinada área, temos interesse em encontrar um único riacho que contenha a espécie. O experimento consistirá da amostragem de diferentes riachos até que encontremos a espécie pela primeira vez. Se soubermos que a probabilidade de ocorrência da espécie nos riachos da região é de \\(p = 0,4\\) podemos nos questionar: Quantos riachos esperamos amostrar até encontrar a espécie? Neste caso temos um experimento sequencial que só irá terminar quando encontrarmos a espécie. Em cada riacho amostrado, a espécie pode estar presente com probabilidade \\(p\\), ou não estar presente com probabilidade \\(1-p\\). Cada nova observação é independente da observação feita anteriormente, isto é, não encontrar a espécie em um riacho não nos ajuda a saber sobre sua ocorrência no próximo. A variável aleatória \\(Y\\) é definida aqui como o número de tentativas necessárias para obtenção de um único sucesso, e o experimento segue o que conhecemos por Modelo Geométrico. Se dermos sorte, poderíamos encontrar a espécie logo na primeira tentativa com probabilidade: \\(P(Y = 1) = p\\) Não encontrarmos a espécie no primero riacho, mas a encontrarmos no segundo pode ocorrer com probabilidade: \\(P(Y = 1) = (1-p) \\times p\\) Generalizando, a distribuição de probabilidade para o modelo geométrico é: \\[P(Y = y|p) = (1-p)^{y-1} \\times p\\] ?Esta expressão reflete o fato de que o primeiro 1 sucesso que irá acontecer com probabilidade \\(p\\) deve necessariamente ser precedido de \\(y-1\\) fracassos, cada um com probabilidade \\((1-p)\\). A distribuição de probabilidade e as funções de distribuição acumulada para o modelo geométrico têm os seguintes formatos: Quando \\(p = 0.05\\), por exemplo, existe uma probabilidade de cerca de 0.45 que seja necesária a busca em 10 riachos até que a espécie seja observada. Para valores altos de \\(p\\) por outro lado, a probabilidade de encontrar a espécie logo nos primeiros riachos é alta (ex. para \\(p = 0.4\\), \\(F(3) = 0.8704\\)). Nas figuras, são apresentadas probabilidades até \\(Y = 20\\), ainda que a variável aleatória no modelo geométrico não tenha limite superior. Consequentemente, o espaço amostral no modelo geométrico pode ser definido por: \\[\\Omega = \\left\\{ y \\in \\mathbb{N}: y \\ge 1 \\right\\}\\] Assim como no modelo binomial, podemos definir os valores para a esperança e variância do modelo geométrico por: \\[E(Y) = \\frac{1}{p}\\] \\[VAR(Y) = \\frac{1-p}{p^2}\\] 30.3 Experimento 3: Uma sequência de sucessos Em muitos estudos, devemos selecionar um número pré-definido de ambientes com uma determinada característica. Suponha portanto uma modificação do experimento anterior. Desejamos agora encontrar a espécie em exatamente \\(r = 3\\) riachos. Novamente, vamos supor que a probabilidade de ocorrência da espécie seja \\(p = 0.05\\). Uma pergunta importante é: Quantos riachos esperamos amostrar até que encontremos a espécies em exatamente \\(r = 3\\) riachos? Novamente, temos um experimento sequencial. Porém iremos selecionar continuamente um novo riacho até encontrarmos \\(r\\) deles contendo a espécie. Em cada riacho, a espécie pode ou não estar presente (com probabilidades \\(p\\) e \\(1-p\\) respsctivamente) e cada nova observação é independente das anteriores. Vamos assumir novamente \\(p = 0.4\\) Se dermos sorte, poderíamos encontrar a espécie logo nos primeiros três riachos, de modo que a sequencia observada seria: \\(presente\\), \\(presente\\), \\(presente\\); com probabilidade \\(p \\times p \\times p = 0,4 \\times 0,4 \\times 0,4 = 0,4^3 = 0,064\\) Outra sequencia possível seria não encontrar a espécie no primeiro porém encontrá-la nos três riachos seguintes. Neste caso: \\(ausente\\), \\(presente\\), \\(presente\\), \\(presente\\); com probabilidade \\((1 - p) \\times p \\times p \\times p = 0,6 \\times 0,4 \\times 0,4 \\times 0,4 = 0,6 \\times 0,4^3 = 0,0384\\) O parâmetro \\(r\\) refere-se ao número de sucessos e não ao número de tentativas para obtê-los. Consequentemente, não existe um número pré-definido de observações como no modelo binomial. O número mínimo de tentativas é de \\(r\\) observações, na medida em que obtenhamos logo nas primeiras tentativas. Entratento o limite superior não existe, uma vez existe, ainda que baixa, uma probabilidade de obtermos sucessivos fracassos eternamente. Neste caso, nossa sequencia de observações se extenderia indefinidamente como: \\(ausente\\), \\(ausente\\), \\(\\cdots\\), \\(ausente\\), \\(\\cdots\\), com probabilidades \\((1-p) \\times (1-p) \\times \\cdots \\times (1-p) \\times \\cdots\\) O modelo descrito aqui é conhecido como Modelo Binomial Negativo, em que a distribuição de probabilidade é dada por: \\[P(Y = y|r,p) = \\left (\\begin{array}{c} y - 1 \\\\ r - 1 \\end{array}\\right) \\times p^{r} \\times (1-p)^{(y-r)}\\] As funções de distribuição de probabilidade e a função acumulada tem os seguintes formatos: No modelo binomial negativo, a esperança matemática e a variância são definidas por: \\[E(Y) = \\frac{r}{p}\\] \\[VAR(Y) = \\frac{r \\times (1-p)}{p^2}\\] 30.4 Experimento 4: Quantas marcas na amostra! Encontrado um riacho em que a espécie ocorre, desejamos estudar o padrão de abundância de sua população. Suponha que neste riacho existam \\(N\\) indivíduos da população alvo. Esta população consiste de \\(m\\) indivíduos marcados e \\(N-m\\) não-marcados. A marcação foi feita anteriormente para diferenciarmos os dois grupos. O experimento consiste em amostrar \\(n\\) indivíduos desta população e verificar quantos (\\(y\\)) apresentam a marcação. Este experimento é comumente realizado para obtermos dados sobre manejo populacional, e descreve uma aplicação típica do Modelo Hipergeométrico em ecologia de populações. No modelo hipergeométrico a variável aleatória \\(Y\\) representa o número de indivíduos com determinada característica na segunda amostra e a variável tem a seguinte distribuição de probabilidade: \\[P(Y=y|N,m,n) = \\frac{\\left (\\begin{array}{c} m \\\\ y \\end{array}\\right) \\times \\left (\\begin{array}{c} N-m \\\\ n-y \\end{array}\\right)}{\\left (\\begin{array}{c} N \\\\ n \\end{array}\\right)}\\] Os parâmetros da distribuição hipergeométrica são: N: tamanho total de elementos; m: número total de elementos com a característica de interesse, e; n: número de elementos na amostra. Vejam que a variável aleatória \\(Y\\) tem \\(0\\) como valor mínimo, quando nenhum dos indivíduos na amostra estiver marcado. O valor máximo será o maior número entre \\(m\\) e \\(n\\). Vejamos o formato da distribuição hipergeométrica para um tamanho \\(N = 30\\) e diferentes valores de \\(m\\) e \\(n\\). No modelo hipergeométrico podemos definir o valor esperado e a variância por: \\[E(Y) = \\frac{n m}{N}\\] \\[VAR(Y) = \\frac{n m}{N}(\\frac{(n-1)(m-1)}{N-1}+1-\\frac{n m}{N})\\] 30.5 Experimento 5: Contagem por unidade de área, tempo, ….. Neste último experimento, o riacho foi subdividido em 30 unidades de \\(1m^2\\). Foram amostrados todos os indivíduos presentes nos trechos. Se o número de indivíduos em um determinado trecho for independente do número de indivíduos nos demais trechos, a contagem de organísmos por \\(m^2\\) será uma variável aleatória descrita por um Modelo de Poisson. Neste modelo, temos um único parâmetro denominado lambda (\\(\\lambda\\)) e a distribuição de probabilidade será descrita por: \\[P(Y = y|\\lambda) = \\frac{e^{-\\lambda} \\lambda^y}{y!}\\] O formato da distribuição é assimétrico para valores baixos de \\(\\lambda\\), tornado-se simétrico ao redor da média à medida que \\(\\lambda\\) aumenta. Este é um modelo particular em que o valor esperado é igual à variância, ambos dados por: \\[E(Y) = VAR(Y) = \\lambda\\] Embora tenhamos exemplificado o modelo para unidade de área, o modelo de Poisson pode ser utilizado para expressar dados de contagem para qualquer tipo de unidade (ex. indivíduos por dia), desde que a contagem em uma unidade seja independente da contagem obtida nas demais unidades. "],["vacont.html", "Capítulo 31 As variáveis são aleatórias, não imprevisíveis! - Modelos contínuos 31.1 Alguns fenômenos têm distribuição normal 31.2 Outros são altamente assimétricos", " Capítulo 31 As variáveis são aleatórias, não imprevisíveis! - Modelos contínuos No capítulo 30 exemplificamos uma série de modelos probabilísticos em que a variável aleatória tem uma natureza discreta. Aqui vamos exemplificar modelos contínuos. Uma variável aleatória contínua é representada por conjuntos numéricos não-contáveis, por exemplo a distância de deslocamento/dia de um organismo, o peso de um indivíduo, a temperatura da água, etc (Capítulo ??). Ao tratar de modelos probabilísticos para variáveis aleatórias contínuas devemos fazer uma distinção do que foi apresentado anteriormente. Nos modelos discretos, é possível por exemplo falarmos na probabilidade de um experimento resultar em um valor particular de \\(y\\), ou seja, \\(P(Y = y)\\). Neste caso: \\[P(-\\infty \\le Y \\le +\\infty) = \\sum_{-\\infty}^{+\\infty} p(y) = 1\\] Para variáveis contínuas podemos expressar somente a probabilidade de que um experimento resulte em um intervalo de valores entre \\(a\\) e \\(b\\), ou seja, \\(P(a \\le y \\le b)\\). Deste modo temos que: \\[P(-\\infty \\le Y \\le +\\infty) = \\int_{-\\infty}^{+\\infty}p(y) dy = 1\\] Portanto, para as variáveis contínuas, o conceito de somatório dos valores individuais para \\(y\\) é substituído pelo conceito de integração em um intervalo definido e a probabilidade deste intervalo equivale à área sob a curva de probabilidade. Esperança e variância em modelos contínuos Vimos que o valor esperado para uma variável aleatória discreta é dado por: \\[E(Y) = \\sum_i^n y_i \\times p(y_i)\\] Para uma variável aleatória contínua, podemos definir o valor esperado como: \\[E(Y) = \\int_{-\\infty}^{+\\infty}y \\times f(y) dy\\] Note que aqui, substituímos \\(p(y_i)\\) por \\(f(y)\\), uma vez que em um modelo contínuo, \\(f(y)\\) não nos dá a probabilidade da ocorrência de um determinado valor \\(y_i\\), mas é entendido como uma função de densidade. A variância no modelo contínuo ainda é definida como: \\[VAR(Y) = E(Y^2) - (E(Y))^2\\] 31.1 Alguns fenômenos têm distribuição normal Suponha que ao amostrar indivíduos de uma espécie de Rhamdioglanis transfasciatus em um riacho, você tenha interesse em medir o comprimento da cada indivíduo. Se assumirmos um Modelo Normal para a variável aleatória comprimento, veremos que a maioria dos indivíduos terá tamanhos próximos à média \\(\\mu\\) e poucos indivíduos serão muito grandes ou pequenos. Uma variável aleatória com distribuição normal, tem sua função de densidade de probabilidade definida por: \\[f(y|\\mu,\\sigma) = \\frac{1}{\\sqrt(2\\pi\\sigma^2)}e^{-\\frac{1}{2}(\\frac{y-\\mu}{\\sigma})^2}\\] Em que os parâmetros da função são \\(\\mu\\) e \\(\\sigma\\) representam respectivamente a média e o desvio padrão de \\(y\\). Novamente nos referimos aos símbolos da média e desvio padrão conforme a notação utilizada para descrever a população estatística de um experimento (Capítulo ??). Na função de densidade estes parâmetros determiman a prosição central e o grau de achatamento da curva. Após o término deste tópico é importante que você leia também o que foi apresentado no capítulo 14 sobre distribuição normal. Por exemplo, se o comprimento segue uma distribuição normal com \\(\\mu = 10\\) e \\(\\sigma = 2\\) veremos que a proporção de individuos iguais ou maiores que 16 cm de comprimento é muito baixa, neste caso \\(P(Y \\ge 16) = 0.0013\\). Por outro lado, para \\(\\sigma = 4\\), esperamos encontrar mais indivíduos nos extremos da distribuição em que a proporção de individuos iguais ou maiores que 16 cm seria de \\(P(Y \\ge 16) = 0.0668\\). 31.2 Outros são altamente assimétricos A distribuição normal, tem um comportamento simétrico, o seja, a proporção de valores acima e abaixo da média é idêntica. Suponha no entando que, ao invés de medir o comprimento dos animais capturados, o experimento coinsista em marcar cada animal, soltá-lo em uma posição conhecida. Futuramente os animais serão recapturados para medirmos a distância de deslocamento. Suponha ainda que a maioria dos indivíduos permaneça próxima ao local de soltura, enquanto outros se desloquem a grandes distâncias. Uma opção mais apropriada para descrever este comportamento poderia ser utilizarmos um Modelo exponencial. Uma variável aleatória que segue um modelo exponencial tem sia função de densidade de probabilidade definida por: \\[f(y|\\lambda) = \\lambda \\times e^{-\\lambda y}\\] A expressão acima vale para \\(y \\ge 0\\). Para \\(y &lt; 0\\), \\(f(y) = 0\\). O valor esperado do modelo exponencial é: \\[E(Y) = \\mu = \\frac{1}{\\lambda}\\] enquanto a variância é dada por: \\[VAR(Y) = \\sigma^2 = \\frac{1}{\\lambda^2}\\] No modelo exponencial, o único parâmetro é \\(\\lambda\\). O modelo é altamente assimétrico, com maior densidade na porção esquerda da curva. Assim, para um fenômeno que possa ser descrito por um modelo exponencial, a maioria das observações será composta de valores baixos, ainda que valores extremamente elevados possam surgir ocasionalmente. "],["detmodel.html", "Capítulo 32 Alguns modelos são determinísticos 32.1 Modelo linear 32.2 Função potência 32.3 Modelo de Michaelis-Menten (ou Resposta funcional do tipo II) 32.4 Resposta funcional do tipo III 32.5 Resposta funcional do tipo IV 32.6 Função hiperbólica 32.7 Função exponencial 32.8 Função logística 32.9 Modelo monomolecular 32.10 Modelo de Ricker 32.11 Modelo de Gompertz 32.12 Modelo de von Bertalanffy", " Capítulo 32 Alguns modelos são determinísticos Neste capítulo iremos retornar às funções no R. Para tal, carregue os pacotes ggplot2 e gridExtra. library(ggplot2) library(gridExtra) Como veremos no Capítulo 33, modelos estatísticos são compostaos por uma porção estocástica (ou aleatória) e uma porção determinística. Abaixo iremos analisar algumas funções determinísticas, a maioria delas apresentada no livro Ecological Models and Data in R (Bolker 2008). 32.1 Modelo linear É o modelo determinístico mais simples para descrever a a relação entre uma variável resposta \\(Y\\) e uma variável preditora \\(X\\) através dos parâmetros \\(\\beta_0\\) e \\(\\beta_1\\), conforme a equação. \\[Y = \\beta_0 + \\beta_1X\\] Falamos deste modelo no capítulo sobre regressão linear (Capítulo 22). Vamos a um exemplo no R. b0 &lt;- 0 b1 &lt;- 2 df &lt;-data.frame(X = seq(0, 50, by = 5)) df &lt;- df %&gt;% mutate(Y = b0 + b1 * X) ggplot(df, mapping = aes(x = X, y = Y)) + geom_point() Na figura acima, temos cada um dos pontos em \\(Y\\) como função linear de \\(X\\). Os comandos cima são entendidos como: Determinamos os parâmetros \\(\\beta_0 = 0\\) e \\(\\beta_1 = 2\\); Criamos um data.frame contendo uma sequnecia de pntos em \\(X\\) ; Atualizamos o data.frame com a variável \\(Y\\) como função linear de \\(X\\); Criamos uma camada gráfica (comando ggplot) com as variáveis \\(X\\) e \\(Y\\); Preenchemos a camada com uma geometria de pontos. Vamos substituir com uma figura em formato de linha ggplot(df, mapping = aes(x = X, y = Y)) + geom_line() No exemplo acima, temos os parâmetros \\(\\beta_0 = 0\\) e \\(\\beta_1 = 2\\). Isto significa que para \\(X = 0\\) o valor em \\(Y = 0\\). Conforme aumentamos \\(X\\) em uma unidade, \\(Y\\) aumenta em 2 unidades. No modelo linear, \\(\\beta_0\\) é interpretado portanto como a altura em \\(Y\\) em que a reta cruza o eixo das ordenadas, enquanto \\(\\beta_1\\) representa a taxa de incremento em \\(Y\\) para o aumento de uma unidade em \\(X\\). Vamos inserir duas outras curvas à figura anterior modificando os valores de \\(\\beta_0\\) e \\(\\beta_1\\). b0_i &lt;- 0 b1_i &lt;- 2 b0_ii &lt;- 20 b1_ii &lt;- 4 df &lt;-data.frame(X = seq(0, 50, by = 5)) df &lt;- df %&gt;% mutate(Y1 = b0_i + b1_i * X, Y2 = b0_i + b1_ii * X, Y3 = b0_ii + b1_i * X, Y4 = b0_ii + b1_ii * X) ggplot(df, mapping = aes(x = X)) + geom_line(mapping = aes(y = Y1), col = &#39;black&#39;) + geom_line(mapping = aes(y = Y2), col = &#39;blue&#39;) + geom_line(mapping = aes(y = Y3), col = &#39;red&#39;) + geom_line(mapping = aes(y = Y4), col = &#39;green&#39;) + annotate(geom = &quot;text&quot;, x = 24, y = mean(df$Y1) + 5, label = bquote(Y == .(b0_i) + .(b1_i) * X), col = &#39;black&#39;, angle = 25) + annotate(geom = &quot;text&quot;, x = 24, y = mean(df$Y2) + 5, label = bquote(Y == .(b0_i) + .(b1_ii) * X), col = &#39;blue&#39;, angle = 45) + annotate(geom = &quot;text&quot;, x = 24, y = mean(df$Y3) + 5, label = bquote(Y == .(b0_ii) + .(b1_i) * X), col = &#39;red&#39;, angle = 25) + annotate(geom = &quot;text&quot;, x = 24, y = mean(df$Y4) + 5, label = bquote(Y == .(b0_ii) + .(b1_ii) * X), col = &#39;green&#39;, angle = 45) + ylab(&quot;Y&quot;) Entendendo os comandos em R Nos comandos acima, criamos um data.frame com uma sequência em \\(X\\); Atualizamos o data.frame com valores de \\(Y_1\\) até \\(Y_4\\), alterando a combinação dos parâmetros \\(\\beta_0\\) e \\(\\beta_1\\); Após criar uma camada gráfica com um \\(X\\) comum (comando ggplot), adicionamos cada uma das linhas (comando geom_line) escrendo as respectivas combinações de parâmetros (comando annotate). Modelo linear segmentado Eventualmente, podemos ter um modelo linear segmentado, em que a primeira parte da reta tem uma inclinação diferente da segunda parte. Este modelo pode ser escrito como: \\[Y = \\beta_0 + \\beta_1X\\] se \\(X \\le \\alpha\\); ou \\[Y = \\beta_0 + \\beta_{1}X + \\beta_{2}(X - \\alpha)\\] se \\(X &gt; \\alpha\\) Veja que agora o modelo tem 4 parâmetros \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\) e \\(\\alpha\\). O valor de \\(\\alpha\\) representa o ponto de quebra, onde a inclinação da reta muda. Se \\(X\\) está abaixo de \\(\\alpha\\) temos o modelo linear comum. Entretanto, se estamos acima do ponto de quebra, a inclinação é dada pelos efeitos conjuntos de \\(\\beta_1\\) sobre \\(X\\) somado ao efeito de \\(\\beta_2\\) sobre a diferença \\((X - \\alpha)\\). a &lt;- 30 b0 &lt;- 10 b1 &lt;- 1.5 b2 &lt;- 2.5 df &lt;-data.frame(X = seq(0, 50, by = 5)) df &lt;- df %&gt;% mutate(Y = ifelse(test = X &lt; a, yes = b0 + b1 * X, no = b0 + b1 * X + b2 * (X - a) ) ) ggplot(df, mapping = aes(x = X, y = Y)) + geom_line() + annotate(geom = &quot;text&quot;, x = 15, y = 38, label = bquote(Y == beta[0] + beta[1]*X), angle = 33) + annotate(geom = &quot;text&quot;, x = 35, y = 90, label = bquote(Y == beta[0] + beta[1]*X + beta[2]*(X - alpha)), angle = 58) 32.2 Função potência As função potência têm um longo histórico de aplicações em ciências naturais em que diversas relações fisiológicas podem ser descritas por este tipo de relação funcional. Veja por exemplo livro o clássico The Ecological Implications of Body Size (Peters and Peters 1986). Uma relação potência é exemplo de modelo não-linear descrito por: \\[Y = \\beta_{0}X^{\\beta_{1}}\\] b0 &lt;- 10 b1 &lt;- 3 df &lt;-data.frame(X = seq(0, 10, length = 100)) df &lt;- df %&gt;% mutate(Y = b0 * X^b1) ggplot(df, mapping = aes(x = X, y = Y)) + geom_line() A representação linear da função potência Embora descreva uma relação funcional não-linear, a função potência têm sido historicamente ajustada por modelos lineares devido a seguinte relação: \\(Y = \\beta_{0}X^{\\beta_{1}}\\) \\(log(Y) = log(\\beta_{0}X^{\\beta_{1}})\\) \\(log(Y) = log(\\beta_{0}) + log(X^{\\beta_{1}})\\) \\(log(Y) = log(\\beta_{0}) + \\beta_{1}log(X)\\) Esta relação permite que os parâmetros \\(\\beta_0\\) e \\(\\beta_1\\) da relação potência possam ser ajustados por regressões lineares simples (Capítulo 22) após a aplicação da transformação logarítmica em \\(Y\\) e \\(X\\). Atualmente, diante da facilidade computacional de ajustes de modelos não-lineares, a necessidade desta transformação têm sido discutida na literatura. Para uma análise desta questão veja: Xiau et al., (2011) e Packard, (2013). Veja a relação gráfica descrita por \\(log(Y)\\) e \\(log(X)\\) após as trnasformações. Nesta figura, o intercepto da curva é dados por \\(log(\\beta_0)\\) enquanto a inclinação é dada pelo efeito de \\(\\beta_1\\) sobre \\(log(X)\\). b0 &lt;- 10 b1 &lt;- 3 df &lt;-data.frame(X = seq(0, 10, length = 100)) df &lt;- df %&gt;% mutate(Y = b0 * X^b1) ggplot(df, mapping = aes(x = log(X), y = log(Y))) + geom_line() As várias formas da função potência Na função potência temos diferentes formatos possíveis, dependendo dos valores de \\(\\beta_1\\). b0 &lt;- 10 df &lt;-data.frame(X = seq(0, 10, length = 100)) df &lt;- df %&gt;% mutate(Y1 = b0 * X^3, Y2 = b0 * X^1, Y3 = b0 * X^-0.5, Y4 = b0 * X^0.5) g1 &lt;- ggplot(df, mapping = aes(x = X, y = Y1)) + geom_line() + annotate(geom = &quot;text&quot;, x = 2.5, y = 7500, label = bquote(beta[1] &gt; 1)) g2 &lt;- ggplot(df, mapping = aes(x = X, y = Y2)) + geom_line() + annotate(geom = &quot;text&quot;, x = 2.5, y = 50, label = bquote(beta[1] == 1)) g3 &lt;- ggplot(df, mapping = aes(x = X, y = Y3)) + geom_line() + annotate(geom = &quot;text&quot;, x = 5, y = 20, label = bquote(beta[1] &lt; 0)) g4 &lt;- ggplot(df, mapping = aes(x = X, y = Y4)) + geom_line() + annotate(geom = &quot;text&quot;, x = 6.5, y = 10, label = bquote(beta[1] &gt; 0 ~ &quot;e&quot; ~ beta[1] &lt; 1)) grid.arrange(g1, g2, g3, g4, ncol = 2, nrow = 2) 32.3 Modelo de Michaelis-Menten (ou Resposta funcional do tipo II) Leva este nome desde que foi proposta como um modelo matemático para descrever a velocidade (\\(Y\\)) da reação enzimatica para um volume crescente de substrato (\\(X\\)). A velocidade da reação aumenta gradativamente até atingir uma assíntota (em \\(Y = \\beta_0\\)), quando todas as enzimas já estão ligadas ao substrado. Este modelo é conhecido também em outras áreas. Em Ecologia decreve a relação funcional entre um predador e sua presa (Resposta funcional do tipo II). Neste caso, o modelo descreve a taxa de predação em função do número de presas, ou seja, o número de presas consumidas por um predador por unidade de tempo. O modelo prevê que à medida que o número de presas disponíveis aumenta os predadores são limitados pelo tempo de manuseio da presa. Portanto que há um limite na capacidade de um predador consumir as presas disponíveis mesmo em altas densidades. Em Pesca este modelo é conhecido como Modelo de Beverton e Holt, utilizado para prever o número de recrutas (peixes que entram para a pesca) em função do tamanho do estoque pesqueiro. O modelo é dado por: \\[Y = \\frac{\\beta_0 X}{(\\beta_1 + X)}\\] O parâmetro \\(\\beta_0\\) descreve a assíntota do modelo, enquanto a taxa de meia saturação em \\(Y\\) (\\(\\frac{\\beta_0}{2}\\)) ocorre no ponto em que \\(X = \\beta_1\\). Vamos ao gráfico da função: b0 &lt;- 30 b1 &lt;- 2 df &lt;-data.frame(X = seq(0, 20, length = 100)) df &lt;- df %&gt;% mutate(Y = b0*X/(b1+X)) ggplot(df, mapping = aes(x = X, y = Y)) + geom_line() + geom_abline(intercept = b0, slope = 0, linetype = &#39;dotted&#39;, color = &#39;red&#39;) + ylim(0, 35) 32.4 Resposta funcional do tipo III Em situações de baixa densidade de presa, o aumento na taxa de predação pode ser manter reduzido devido à preferência do por outras presas mais abundante ou sua incapacidade de encontrar a vítima. Estas situações descrevem uma resposta Funcional do Tipo III. A padrão é uma curva sigmóide no início que atinge a assíntota no ponto em que \\(Y = beta_0\\) assim como a curva de Michaelis-Menten. O formato deste modelo é: \\[Y = \\frac{\\beta_0 X^2}{(\\beta_1^2 + X^2)}\\] Compare este modelo com o anterior modificando os valores para os parâmetros \\(\\beta_0\\) e \\(\\beta_1\\). b0 &lt;- 30 b1 &lt;- 2 df &lt;-data.frame(X = seq(0, 20, length = 100)) df &lt;- df %&gt;% mutate(Y = b0*X^2/(b1^2+X^2)) ggplot(df, mapping = aes(x = X, y = Y)) + geom_line() + geom_abline(intercept = b0, slope = 0, linetype = &#39;dotted&#39;, color = &#39;red&#39;) + ylim(0, 35) 32.5 Resposta funcional do tipo IV O modelo anterior ainda pode ser extendido para um Resposta Funcional do Tipo IV pela adição de um parâmetro (\\(\\beta_2\\)) ao modelo. O formato do modelo varia de um padrão assintótico (\\(\\beta_2 &gt; 0\\)) para um padrão unimodal (\\(\\beta_2 \\le 0\\)) com ponto máximo em \\(X = \\frac{-2\\beta_1}{\\beta_2}\\). O modelo é dado por: \\[Y = \\frac{\\beta_0 X^2}{(\\beta_1 + \\beta_2 X + X^2)}\\] Varie os valores de \\(\\beta_2\\) e compare os padrões com os modelos de resposta funcional do Tipo II (Michaelis-Menten) e III. b0 &lt;- 30 b1 &lt;- 20 b2 &lt;- -4 df &lt;-data.frame(X = seq(0, 50, length = 100)) df &lt;- df %&gt;% mutate(Y = (b0*X^2)/(b1 + b2*X + X^2)) ggplot(df, mapping = aes(x = X, y = Y)) + geom_line() + geom_abline(intercept = b0, slope = 0, linetype = &#39;dotted&#39;, color = &#39;red&#39;) 32.6 Função hiperbólica Em ecologia a função hiperbólica pode ser utilizada para prever o resultado de um modelo de competição, assumindo que o recurso por unidade de área é constante e que o uso do recurso por indivíduo é proporcional ao inverso da densidade de organismos (\\(X\\)) por unidade de área. O resultado é uma redução no número de descendentes por indivíduos (\\(Y\\)) em função do aumento da densidade (\\(X\\)) e o modelo é dado por: \\[ Y = \\frac{\\beta_0}{\\beta_1 + X}\\] Que tem o formato de: b0 &lt;- 10 b1 &lt;- 3 df &lt;-data.frame(X = seq(0, 20, length = 100)) df &lt;- df %&gt;% mutate(Y = b0/(b1 * X)) ggplot(df, mapping = aes(x = X, y = Y)) + geom_line() 32.7 Função exponencial O modelo exponencial á amplamente utilizado em ecologia. As aplicações mais simples são o crescimento (\\(Y = \\beta_0 e^{\\beta_1 X}\\)) e o decrescimento (\\(Y = \\beta_0 e^{-\\beta_1 X}\\)) exponencial. b0 &lt;- 2 b1 &lt;- 1 df &lt;-data.frame(X = seq(10, 20, length = 100)) df &lt;- df %&gt;% mutate(Y1 = b0 * exp(b1 * X), Y2 = b0 * exp(-b1 * X)) g1 &lt;- ggplot(df, mapping = aes(x = X)) + geom_line(mapping = aes(y = Y1), col = &#39;black&#39;) g2 &lt;- ggplot(df, mapping = aes(x = X)) + geom_line(mapping = aes(y = Y2), col = &#39;red&#39;) grid.arrange(g1, g2, ncol = 2, nrow = 1) 32.8 Função logística A função logística têm uma grande variedade de aplicações. Uma delas é o ajuste de modelo de dose-resposta para compreendermos por exemplo, o ponto em que um percentual da população sobrevive. Outra aplicação está no estudo da dinâmica de populações, onde o modelo prevê um aumento no número de indivíduos ao longo do tempo alcançando o equilíibrio. A função logística pode ser parametrizada de várias formas (Veja os exemplos). Uma forma amplamente utilizada em modelos estatísticos é: \\[Y = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\\] Neste formato, \\(\\beta_0\\) é um parâmetro de posição e \\(\\beta_1\\) controla a inclinação da curva. O ponto de meia saturação (\\(Y = 0,5\\)) ocorre quando \\(X = \\frac{-\\beta_0}{\\beta_1}\\). Varie os dois parâmetros e veja o que ocorre com a figura abaixo. b0 &lt;- -10 b1 &lt;- 3 df &lt;-data.frame(X = seq(0, 8, length = 100)) df &lt;- df %&gt;% mutate(Y = exp(b0 + b1 * X) / (1 + exp(b0 + b1 * X))) ggplot(df, mapping = aes(x = X, y = Y)) + geom_line() 32.9 Modelo monomolecular Tem um formato similar ao da função de Michaelis-Menten com assintota em \\(\\beta_0\\) e taxa de incremento dada pelo valor de \\(\\beta_1\\). É função é dada por: \\[Y = \\beta_0 (1 - e^{-\\beta_1 X})\\] b0 &lt;- 30 b1 &lt;- 1 df &lt;-data.frame(X = seq(0, 8, length = 100)) df &lt;- df %&gt;% mutate(Y = b0 * (1 - exp(-b1*X))) ggplot(df, mapping = aes(x = X, y = Y)) + geom_line() + geom_abline(intercept = b0, slope = 0, linetype = &#39;dotted&#39;, color = &#39;red&#39;) 32.10 Modelo de Ricker O modelo de Ricker é uma opção para descrever o crescimento populacional denso-dependente e portanto uma opção ao modelo Logístico. Em \\(X = 0\\), o modelo inicia com crescimento linear com inclinação dada pelo parâmetro \\(\\beta_0\\) e atinge um pico em \\(X = \\frac{1}{\\beta_1}\\). O modelo é dado por: \\[Y = \\beta_0 X e^{-\\beta_1 X}\\] b0 &lt;- 5 b1 &lt;- 1.5 df &lt;-data.frame(X = seq(0, 8, length = 100)) df &lt;- df %&gt;% mutate(Y = b0 * X * exp(-b1*X)) ggplot(df, mapping = aes(x = X, y = Y)) + geom_line() 32.11 Modelo de Gompertz O modelo de Gompertz é outra alternativa ao modelo Logístico. \\(Y\\) se aproxima de 1 à medida que \\(X\\) aumenta, entretanto, ao contrário do modelo logístico é assimétrico, e o ponto de inflecção ocorre a cerca de \\(1/3\\) do caminho até a assíntota. O modelo é dado por: \\[Y = e^{-\\beta_0 e^{-\\beta_1 X}}\\] b0 &lt;- 5 b1 &lt;- 1 df &lt;-data.frame(X = seq(0, 8, length = 100)) df &lt;- df %&gt;% mutate(Y = exp(-b0 * exp(-b1 * X))) ggplot(df, mapping = aes(x = X, y = Y)) + geom_line() 32.12 Modelo de von Bertalanffy O modelo de von Bertalanffy é uma opção para modelar o crescimento de organismos com crescimento indeterminado. A expressão abaixo utiliza os símbolos comumente empregado nestas situações, onde \\(L_{\\infty}\\) é o ponto assintótico (tamanho máximo teórico do organismo), \\(k\\) é a taxa de crescimento, \\(t_0\\) o tempo em que o tamanho (\\(Y\\)) está em zero. O modelo é dado por: \\[Y = L_{\\infty} (1 - e^{-k(X - t_0)})\\] Linf &lt;- 30 k &lt;- 1.2 t0 &lt;- -0.1 df &lt;-data.frame(X = seq(0, 5, length = 100)) df &lt;- df %&gt;% mutate(Y = Linf * (1 - exp(-k *(X - t0)))) ggplot(df, mapping = aes(x = X, y = Y)) + geom_line() References "],["modelstat.html", "Capítulo 33 As partes estocásticas e determinísticas de um modelo estatístico 33.1 A parcela aleatória 33.2 A parcela determinística 33.3 Mais um modelo Binomial: taxa de mortalidade em testes dose-resposta 33.4 O modelo estatístico da regressão linear", " Capítulo 33 As partes estocásticas e determinísticas de um modelo estatístico 33.1 A parcela aleatória Um modelo estatístico consiste de uma variável aleatória (\\(Y\\)) modelada a partir de alguns dos modelos probabilísticos vistos nos capítulos 30 e 31. Nestes, a função de probabilidade \\(f(Y|\\theta)\\) é dependente de um ou mais parâmetros. O símbolo \\(\\theta\\) é utilizado para fazer uma representação genérica dos parâmetros de um modelo estatístico. No modelo Binomial por exemplo, os parâmetros são \\(n\\) e \\(p\\), no modelo de Poisson é \\(\\lambda\\), no modelo Normal são \\(\\mu\\) e \\(\\sigma\\) e assim por diante. Definido deste forma, o resultado de um experimento aleatório pode portanto ser entendido como a realização de uma observação de \\(Y\\) de acordo com um modelo particular. Por exemplo, no experimento sobre a amostragem de 4 riachos, poderíamos ter como resultado a ocorrência de Rhamdioglanis transfasciatus em exatamente \\(3\\) riachos. Entendemos esta observação como o resultado de um experimento em que o número de ocorrências segue um modelo Binomial com parâmetros \\(n\\) e \\(p\\). O modelo Binomial é portanto o modelo estatístico que descreve a realizações deste tipo de experimento. Simulando a realização de um modelo estatístico Vamos utilizar o R para simular uma observação deste tipo de experimento. n &lt;- 4 p &lt;- 0.4 Y1 &lt;- rbinom(1, size = n, prob = p) Y1 ## [1] 2 Nos comandos acima fizemos os seguintes passos: Definimos os parâmetros \\(n\\) e \\(p\\); Utilizamos a função ´rbinom´ para simular uma observação ao acaso de acordo com um modelo Binomial com os parâmetros previamente definidos. Veja que o parâmetros \\(n\\) no R é denominado de ´size´, enquanto o parâmetro \\(p\\) é denomindado de ´prob´; Note que a cada vez que você rodar os comandos acima irá obter um valor aleatório para Y entre \\(0\\) e \\(4\\). Se repetir várias o comando vezes verá que as frequências observadas de cada resultado irão se aproximar das probabilidades preditas pelo modelo Bonimial. De fato, não é necessario repetir os comandos várias vezes. Basta indicar o número de repetições desejada do experimento. Por exemplo, se quisermos repetí-lo 10 vezes basta fazermos: n &lt;- 4 p &lt;- 0.4 Y &lt;- rbinom(10, size = n, prob = p) Y ## [1] 3 2 2 3 1 1 1 2 0 3 Aumente gradativamente o número de repetições e compare a frequência com que os valores simulados são gerados. Para um número baixo de repetições, haverá maior divergência entre os valores teóricos preditos pelo modelo Binomial e os valores simulados. A medida que o número de repetições aumenta, a distribuição de frequência dos valores simulados irá se assemelhar à distribuição teórica: n &lt;- 4 p &lt;- 0.4 repeticoes &lt;- 10 # Distribuição teórica DY &lt;- data.frame(Y = 0:n, Tipo = rep(&quot;Teórico&quot;, n + 1)) %&gt;% mutate(PB = dbinom(Y, size = n, prob = p)) # Valores simulados Y &lt;- rbinom(repeticoes, size = n, prob = p) Y_sim &lt;- data.frame(FB = prop.table(table(Y))) Y_sim$FB.Y &lt;- as.numeric(as.character(Y_sim$FB.Y)) # Unificando os data.frames DY &lt;- Y_sim %&gt;% rename(Y = FB.Y, PB = FB.Freq) %&gt;% mutate(Tipo = rep(&quot;Simulado&quot;, times = nrow(Y_sim))) %&gt;% select(Y, Tipo, PB) %&gt;% bind_rows(DY) ggplot(DY, mapping = aes(x = Y, y = PB, fill = Tipo)) + geom_bar(stat = &#39;identity&#39;) + #, width=.8, position = &quot;dodge&quot;) facet_wrap(~ Tipo) + ggtitle(&quot;Distribuição de frequência em um experimento Binomial&quot;) + theme(plot.title = element_text(hjust = 0.5)) + ylab(expression(f(y))) 33.2 A parcela determinística No exemplo anterior, temos um modelo estatístico descrito unicamente por uma variável aleatória, isto é, um modelo Binomial com \\(p = 0,4\\). Suponha agora que a ocorrência de Rhmadioglanis transfasciatus varie também como função da elevação do riacho (\\(X\\)). A espécie é pouco frequente em riachos de baixas altitudes e torna-se mais frequente à medida que a elevação aumenta. Como no exemplo anterior a espécie da espécie deverá ser modelada através de uma distribuição binomial. Entretanto, o parâmetro \\(p\\) não é mais constante e deverá ser modelado como uma função \\(g(X)\\) que deverá expressar o aumento na probabilidade de ocorrência conforme a elevação do riacho. A função \\(g(X)\\) deverá ser um dos modelos determinísticos vistos no capítulo 32, nos cabendo agora escolher um destes modelos é o mais apropriado para o experimento. Qual modelo determinístico escolher? Se assumirmos que \\(g(x)\\) deverá ser uma função crescente da elevação, poderíamos escolher por exemplo o modelo linear. Neste caso, estaríamos assumindo que o parâmetro \\(p\\) seria modelado por: \\[g(X) = p = \\beta_0 + \\beta_1 X\\] b0 &lt;- 0 b1 &lt;- 0.2 df &lt;-data.frame(X = seq(-3, 8, length = 50)) df &lt;- df %&gt;% mutate(Y = b0 + b1 * X) ggplot(df, mapping = aes(x = X, y = Y)) + geom_line() + xlab(&quot;Elevação (X)&quot;) + ylab(&quot;p&quot;) + geom_abline(intercept = c(0,1), slope = 0, linetype = &quot;dotted&quot;) Veja que este modelo tem uma limitação importante para ser utilizado neste situação. O parâmetro \\(p\\) por ser uma probabilidade, deve estar restrido ao limites entre \\(0\\) e \\(1\\). Entretanto, a função linear é sempre crescente e permite tanto valores negativos quando acima de \\(1\\) o que não faz sentido para oproblema em questão. Outros modelos descritos no captítulo 32 seriam mais interessantes, por exemplo, o modelo logístico. Se assumirmos \\(g(x)\\) como uma função logística da elevação, então \\(p\\) poderia ser modelado conforme: \\[g(X) = p = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\\] que assume o formato: b0 &lt;- -10 b1 &lt;- 3 df &lt;-data.frame(X = seq(0, 8, length = 100)) df &lt;- df %&gt;% mutate(Y = exp(b0 + b1 * X) / (1 + exp(b0 + b1 * X))) ggplot(df, mapping = aes(x = X, y = Y)) + geom_line() + xlab(&quot;Elevação (X)&quot;) + ylab(&quot;p&quot;) De fato, a equação logística seria a primeira a ser considerada em uma situação deste tipo, ainda que outros modelos possam ser aplicados. Ao assumir a equação logística, nosso modelo estatístico está completo, em que \\(Y\\) é uma variável aleatória Binomial (porção aleatória), que tem o parâmetro \\(p\\) como uma função logística da elevação \\(X\\) (porção detrminística) e \\(n\\) é o número de riachos selecionados para um dado valor de elevação. O modelo estatístico apropriado para prever a ocorrência da espécie em um dado riacho poderia ser escrito como: \\(Y ∼ Binom \\left (\\begin{array}{c} p = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}, n = 1 \\end{array}\\right)\\) A realização desde modelo nos daria como resposta \\(Y = 0\\) (a espécie não ocorre) ou \\(Y = 1\\) (a espécie ocorre). Ao nos referirmos a cada riacho, o parâmetro \\(n = 1\\) da distribuição binomial transforma este modelo em um experimento de Bernoulli em que a probabilidade de ocorrência \\(p\\) varia de acordo com a elevação. 33.3 Mais um modelo Binomial: taxa de mortalidade em testes dose-resposta O nosso próximo exemplo, fala sobre um teste de dose-resposta para o efeito de um poluente sobre um organismo modelo. O teste foi feito em 100 organismos, cada um exposto a diferentes doses do poluente. No gráfico abaixo, cada ponto é um organismo utilizado no teste. O valor em \\(X\\) nos diz a que nível do poluente o organismo foi exposto e o valor em \\(Y\\) se houve morte (\\(1\\)) ou não (\\(0\\)). INSERIR GRÁFICO Para compreendermos como se dá o efeito de \\(X\\) sobre a mortalidade podemos ajustar uma modelo estatístico. Este modelo deveria nos dizer qual a relação funcional entre a taxa de mortalidade e os níveis de poluente. Como a taxa de mortalidade está restrita entre os valores de \\(0\\) e \\(1\\) (i.e. não pode crescer ou decrescer indefinidamente), o modelo linear não seria o mais adequado. Por outro, lado uma relação funcional do tipo logśitica nos daria um modelo mais condizente com o resultado esperado do teste. Este modelo se inicia com a curva próxima a \\(0\\). A partir de determinado momento, a curva passa por uma fase de transição e atinge valores próximos a \\(1\\). Este modelo nos dá portanto a fração esperada de organismos que irão morrer após serem expostos a um determinado nível do poluente. Como esta é uma taxa esperada, o resultado exato irá variar de experimento para experimento e de organismo para organismo. Portanto, falta ainda associar uma estrutura de erro a este modelo. Neste caso, uma estrutura de erro apropriada seria uma distribuição Binomial de probabilidade com parâmetros \\(n\\) e \\(p\\), este último modeloado como uma funçao logística dos npiveis de poluência em \\(X\\). Assim como o exemplo anterior, este modelo descreve a estrutura de experimentos aleatórios que podem ter como resposta apenas dois resultados (morto/vivo, macho/fêmea, infectado/não infectado). Seguindo o exemplo anterior, o modelo estatístico apropriado para prever o estado de cada organismo (morto ou vivo) após ser exposto ao poluente poderia ser escrito como: \\(Y ∼ Binom \\left (\\begin{array}{c} p = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}, n = 1 \\end{array}\\right)\\) Dizemos então que a taxa de mortalidade de cada organismo varia de acordo com um modelo Binomial em que \\(p\\) segue uma função logística do nível de poluente. Se estivermos falando de cada organismo, o parâmetro na da distribuição binomial é \\(n = 1\\). Novamente, este seria um experimento de Bernoulli com probabilidade \\(p\\). INSERIR GRÁFICO COM AJUSTE Em experimento deste tipo, um dos principais interesse é saber o nível de poluente em que a probabilidade de sobrevivência é de 50%, ou seja, o valor de \\(X\\) em que \\(p = 0,5\\). 33.4 O modelo estatístico da regressão linear "],["emv.html", "Capítulo 34 Estimando os parâmetros: a ideia da Máxima Verossimilhança", " Capítulo 34 Estimando os parâmetros: a ideia da Máxima Verossimilhança "],["aic.html", "Capítulo 35 Comparando modelos: uma questão de parcimônia", " Capítulo 35 Comparando modelos: uma questão de parcimônia "],["statbayes.html", "Capítulo 36 Da verossimilhança à estatística Bayesiana", " Capítulo 36 Da verossimilhança à estatística Bayesiana "],["varmodels.html", "Capítulo 37 Uma variedade de modelos estatísticos: modelando a variância", " Capítulo 37 Uma variedade de modelos estatísticos: modelando a variância "],["nindep.html", "Capítulo 38 Uma variedade de modelos estatísticos: modelando os resíduos", " Capítulo 38 Uma variedade de modelos estatísticos: modelando os resíduos "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
